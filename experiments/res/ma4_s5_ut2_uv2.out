Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -1.7684271856592133, agent episode reward: [2.36, 2.36, 2.36, -8.848427185659213], time: 121.563
steps: 49975, episodes: 2000, mean episode reward: -7.065806414386487, agent episode reward: [3.43, 3.43, 3.43, -17.355806414386485], time: 161.451
steps: 74975, episodes: 3000, mean episode reward: 9.308930170478124, agent episode reward: [5.39, 5.39, 5.39, -6.8610698295218775], time: 161.433
steps: 99975, episodes: 4000, mean episode reward: 10.742795422108479, agent episode reward: [5.76, 5.76, 5.76, -6.537204577891521], time: 161.645
steps: 124975, episodes: 5000, mean episode reward: 11.472058462882664, agent episode reward: [5.97, 5.97, 5.97, -6.437941537117337], time: 162.495
steps: 149975, episodes: 6000, mean episode reward: 10.86435145205384, agent episode reward: [5.68, 5.68, 5.68, -6.175648547946159], time: 161.41
steps: 174975, episodes: 7000, mean episode reward: 15.788433107903295, agent episode reward: [8.21, 8.21, 8.21, -8.841566892096704], time: 162.3
steps: 199975, episodes: 8000, mean episode reward: 23.35489362853394, agent episode reward: [11.96, 11.96, 11.96, -12.525106371466057], time: 161.919
steps: 224975, episodes: 9000, mean episode reward: 52.572344299970915, agent episode reward: [26.77, 26.77, 26.77, -27.737655700029084], time: 162.063
steps: 249975, episodes: 10000, mean episode reward: 58.48212375187225, agent episode reward: [29.76, 29.76, 29.76, -30.797876248127757], time: 162.191
steps: 274975, episodes: 11000, mean episode reward: 39.69224594728067, agent episode reward: [22.13, 22.13, 22.13, -26.697754052719336], time: 162.227
steps: 299975, episodes: 12000, mean episode reward: 33.8463942283058, agent episode reward: [19.36, 19.36, 19.36, -24.233605771694194], time: 162.569
steps: 324975, episodes: 13000, mean episode reward: 31.813771352153275, agent episode reward: [19.17, 19.17, 19.17, -25.696228647846727], time: 161.513
steps: 349975, episodes: 14000, mean episode reward: 20.734763488388328, agent episode reward: [13.69, 13.69, 13.69, -20.335236511611676], time: 162.15
steps: 374975, episodes: 15000, mean episode reward: 11.744559523547602, agent episode reward: [9.61, 9.61, 9.61, -17.085440476452398], time: 160.663
steps: 399975, episodes: 16000, mean episode reward: 12.468707075831656, agent episode reward: [8.98, 8.98, 8.98, -14.471292924168345], time: 162.132
steps: 424975, episodes: 17000, mean episode reward: 12.88319651352975, agent episode reward: [9.21, 9.21, 9.21, -14.746803486470247], time: 161.858
steps: 449975, episodes: 18000, mean episode reward: 14.396687278286722, agent episode reward: [9.91, 9.91, 9.91, -15.333312721713279], time: 161.883
steps: 474975, episodes: 19000, mean episode reward: 16.19067120014776, agent episode reward: [10.16, 10.16, 10.16, -14.289328799852239], time: 162.362
steps: 499975, episodes: 20000, mean episode reward: 16.710791271034942, agent episode reward: [10.28, 10.28, 10.28, -14.129208728965056], time: 162.629
steps: 524975, episodes: 21000, mean episode reward: 19.019560464517127, agent episode reward: [11.25, 11.25, 11.25, -14.730439535482876], time: 162.697
steps: 549975, episodes: 22000, mean episode reward: 19.41712753403515, agent episode reward: [11.6, 11.6, 11.6, -15.382872465964846], time: 161.256
steps: 574975, episodes: 23000, mean episode reward: 19.992803851209857, agent episode reward: [11.82, 11.82, 11.82, -15.467196148790144], time: 163.874
steps: 599975, episodes: 24000, mean episode reward: 26.6301162941969, agent episode reward: [15.36, 15.36, 15.36, -19.4498837058031], time: 162.46
steps: 624975, episodes: 25000, mean episode reward: 29.158498749253503, agent episode reward: [16.28, 16.28, 16.28, -19.681501250746496], time: 162.065
steps: 649975, episodes: 26000, mean episode reward: 28.650372860742472, agent episode reward: [16.25, 16.25, 16.25, -20.099627139257528], time: 162.343
steps: 674975, episodes: 27000, mean episode reward: 37.60639591475307, agent episode reward: [20.67, 20.67, 20.67, -24.403604085246933], time: 162.594
steps: 699975, episodes: 28000, mean episode reward: 31.59760328944304, agent episode reward: [17.58, 17.58, 17.58, -21.142396710556962], time: 161.215
steps: 724975, episodes: 29000, mean episode reward: 27.03875952644647, agent episode reward: [15.4, 15.4, 15.4, -19.16124047355353], time: 164.307
steps: 749975, episodes: 30000, mean episode reward: 33.58213474797047, agent episode reward: [18.32, 18.32, 18.32, -21.37786525202953], time: 161.872
steps: 774975, episodes: 31000, mean episode reward: 31.73117163577122, agent episode reward: [17.54, 17.54, 17.54, -20.888828364228786], time: 162.993
steps: 799975, episodes: 32000, mean episode reward: 27.548149786873072, agent episode reward: [16.12, 16.12, 16.12, -20.811850213126927], time: 162.523
steps: 824975, episodes: 33000, mean episode reward: 31.34402853550387, agent episode reward: [17.95, 17.95, 17.95, -22.505971464496128], time: 163.26
steps: 849975, episodes: 34000, mean episode reward: 29.764290959424486, agent episode reward: [16.94, 16.94, 16.94, -21.055709040575515], time: 161.964
steps: 874975, episodes: 35000, mean episode reward: 25.182853368658268, agent episode reward: [15.57, 15.57, 15.57, -21.527146631341733], time: 161.771
steps: 899975, episodes: 36000, mean episode reward: 24.72258250075297, agent episode reward: [15.5, 15.5, 15.5, -21.77741749924703], time: 163.648
steps: 924975, episodes: 37000, mean episode reward: 30.870417147415107, agent episode reward: [18.01, 18.01, 18.01, -23.159582852584894], time: 162.133
steps: 949975, episodes: 38000, mean episode reward: 23.66095300902841, agent episode reward: [15.01, 15.01, 15.01, -21.36904699097159], time: 161.707
steps: 974975, episodes: 39000, mean episode reward: 31.772384797161024, agent episode reward: [18.68, 18.68, 18.68, -24.26761520283898], time: 160.971
steps: 999975, episodes: 40000, mean episode reward: 34.749692546168696, agent episode reward: [20.0, 20.0, 20.0, -25.2503074538313], time: 161.259
steps: 1024975, episodes: 41000, mean episode reward: 31.735331457714064, agent episode reward: [18.63, 18.63, 18.63, -24.154668542285936], time: 160.943
steps: 1049975, episodes: 42000, mean episode reward: 29.861370382094808, agent episode reward: [17.46, 17.46, 17.46, -22.51862961790519], time: 163.213
steps: 1074975, episodes: 43000, mean episode reward: 27.67127278195604, agent episode reward: [16.81, 16.81, 16.81, -22.758727218043962], time: 161.055
steps: 1099975, episodes: 44000, mean episode reward: 28.06276094509003, agent episode reward: [16.81, 16.81, 16.81, -22.367239054909966], time: 159.995
steps: 1124975, episodes: 45000, mean episode reward: 31.293332697443894, agent episode reward: [18.09, 18.09, 18.09, -22.976667302556105], time: 163.543
steps: 1149975, episodes: 46000, mean episode reward: 32.84348192204218, agent episode reward: [19.08, 19.08, 19.08, -24.39651807795782], time: 162.572
steps: 1174975, episodes: 47000, mean episode reward: 28.660676616740997, agent episode reward: [16.42, 16.42, 16.42, -20.599323383259005], time: 162.767
steps: 1199975, episodes: 48000, mean episode reward: 30.58040345026037, agent episode reward: [17.49, 17.49, 17.49, -21.889596549739636], time: 161.711
steps: 1224975, episodes: 49000, mean episode reward: 29.26911647799051, agent episode reward: [17.24, 17.24, 17.24, -22.45088352200949], time: 161.958
steps: 1249975, episodes: 50000, mean episode reward: 25.839561467600674, agent episode reward: [15.37, 15.37, 15.37, -20.270438532399325], time: 161.318
steps: 1274975, episodes: 51000, mean episode reward: 27.47030569417053, agent episode reward: [15.89, 15.89, 15.89, -20.19969430582947], time: 159.991
steps: 1299975, episodes: 52000, mean episode reward: 26.816630320386036, agent episode reward: [15.79, 15.79, 15.79, -20.553369679613958], time: 158.921
steps: 1324975, episodes: 53000, mean episode reward: 31.94291227641905, agent episode reward: [18.27, 18.27, 18.27, -22.867087723580944], time: 158.426
steps: 1349975, episodes: 54000, mean episode reward: 26.12378331231801, agent episode reward: [15.11, 15.11, 15.11, -19.20621668768199], time: 159.013
steps: 1374975, episodes: 55000, mean episode reward: 26.156786790282553, agent episode reward: [15.31, 15.31, 15.31, -19.773213209717447], time: 159.741
steps: 1399975, episodes: 56000, mean episode reward: 25.411179747077128, agent episode reward: [14.86, 14.86, 14.86, -19.168820252922874], time: 158.139
steps: 1424975, episodes: 57000, mean episode reward: 29.872356073741816, agent episode reward: [17.23, 17.23, 17.23, -21.817643926258185], time: 159.041
steps: 1449975, episodes: 58000, mean episode reward: 27.975063365875794, agent episode reward: [16.52, 16.52, 16.52, -21.584936634124205], time: 158.823
steps: 1474975, episodes: 59000, mean episode reward: 24.37191609839457, agent episode reward: [14.8, 14.8, 14.8, -20.02808390160543], time: 158.801
steps: 1499975, episodes: 60000, mean episode reward: 21.256347390640943, agent episode reward: [13.56, 13.56, 13.56, -19.42365260935906], time: 156.249
...Finished total of 60001 episodes.
