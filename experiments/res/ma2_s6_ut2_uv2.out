Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -24.35657237865585, agent episode reward: [-24.928537410351595, 0.285982515847868, 0.285982515847868], time: 80.127
steps: 49975, episodes: 2000, mean episode reward: -25.6387055521493, agent episode reward: [-19.86273674505946, -2.8879844035449196, -2.8879844035449196], time: 109.122
steps: 74975, episodes: 3000, mean episode reward: -5.979681879077505, agent episode reward: [-12.357218667406228, 3.1887683941643603, 3.1887683941643603], time: 109.487
steps: 99975, episodes: 4000, mean episode reward: -17.29000432691478, agent episode reward: [-20.706459731438677, 1.7082277022619508, 1.7082277022619508], time: 109.852
steps: 124975, episodes: 5000, mean episode reward: -20.46077705882339, agent episode reward: [-23.788112244440736, 1.6636675928086744, 1.6636675928086744], time: 109.922
steps: 149975, episodes: 6000, mean episode reward: -15.226656959370237, agent episode reward: [-33.70011534043037, 9.236729190530067, 9.236729190530067], time: 109.688
steps: 174975, episodes: 7000, mean episode reward: -24.703810021538594, agent episode reward: [-21.458753053161068, -1.6225284841887624, -1.6225284841887624], time: 109.267
steps: 199975, episodes: 8000, mean episode reward: -7.186874753923775, agent episode reward: [-21.250142977533127, 7.031634111804674, 7.031634111804674], time: 109.432
steps: 224975, episodes: 9000, mean episode reward: 1.2339154849070921, agent episode reward: [-8.413095791298366, 4.823505638102728, 4.823505638102728], time: 109.891
steps: 249975, episodes: 10000, mean episode reward: -15.456782783022064, agent episode reward: [-9.80062400629266, -2.8280793883647, -2.8280793883647], time: 110.073
steps: 274975, episodes: 11000, mean episode reward: -16.563423286797036, agent episode reward: [-19.633180235521333, 1.5348784743621482, 1.5348784743621482], time: 109.852
steps: 299975, episodes: 12000, mean episode reward: -12.905180736909617, agent episode reward: [-23.501623395181117, 5.298221329135748, 5.298221329135748], time: 110.047
steps: 324975, episodes: 13000, mean episode reward: -14.510235796627766, agent episode reward: [-21.8904331020967, 3.690098652734467, 3.690098652734467], time: 110.133
steps: 349975, episodes: 14000, mean episode reward: -17.433941995325135, agent episode reward: [-21.38565568320307, 1.9758568439389665, 1.9758568439389665], time: 109.777
steps: 374975, episodes: 15000, mean episode reward: -21.600168296813035, agent episode reward: [-22.173192580462548, 0.2865121418247563, 0.2865121418247563], time: 110.732
steps: 399975, episodes: 16000, mean episode reward: -20.16617026649113, agent episode reward: [-20.962497810777368, 0.39816377214311854, 0.39816377214311854], time: 109.606
steps: 424975, episodes: 17000, mean episode reward: -22.17909410191098, agent episode reward: [-21.35422081662604, -0.4124366426424709, -0.4124366426424709], time: 108.976
steps: 449975, episodes: 18000, mean episode reward: -22.128189804622984, agent episode reward: [-20.943592336732376, -0.5922987339453012, -0.5922987339453012], time: 110.837
steps: 474975, episodes: 19000, mean episode reward: -24.19463326814682, agent episode reward: [-22.8772365566086, -0.6586983557691071, -0.6586983557691071], time: 111.334
steps: 499975, episodes: 20000, mean episode reward: -23.670063378884247, agent episode reward: [-22.383560203829518, -0.6432515875273639, -0.6432515875273639], time: 110.543
steps: 524975, episodes: 21000, mean episode reward: -17.896125697600315, agent episode reward: [-23.79701231554594, 2.9504433089728126, 2.9504433089728126], time: 110.426
steps: 549975, episodes: 22000, mean episode reward: -17.292392062565327, agent episode reward: [-27.610641222912047, 5.15912458017336, 5.15912458017336], time: 110.426
steps: 574975, episodes: 23000, mean episode reward: -28.12971991378166, agent episode reward: [-30.511236673061077, 1.1907583796397094, 1.1907583796397094], time: 111.355
