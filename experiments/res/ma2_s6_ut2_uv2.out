Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -24.35657237865585, agent episode reward: [-24.928537410351595, 0.285982515847868, 0.285982515847868], time: 80.127
steps: 49975, episodes: 2000, mean episode reward: -25.6387055521493, agent episode reward: [-19.86273674505946, -2.8879844035449196, -2.8879844035449196], time: 109.122
steps: 74975, episodes: 3000, mean episode reward: -5.979681879077505, agent episode reward: [-12.357218667406228, 3.1887683941643603, 3.1887683941643603], time: 109.487
steps: 99975, episodes: 4000, mean episode reward: -17.29000432691478, agent episode reward: [-20.706459731438677, 1.7082277022619508, 1.7082277022619508], time: 109.852
steps: 124975, episodes: 5000, mean episode reward: -20.46077705882339, agent episode reward: [-23.788112244440736, 1.6636675928086744, 1.6636675928086744], time: 109.922
steps: 149975, episodes: 6000, mean episode reward: -15.226656959370237, agent episode reward: [-33.70011534043037, 9.236729190530067, 9.236729190530067], time: 109.688
steps: 174975, episodes: 7000, mean episode reward: -24.703810021538594, agent episode reward: [-21.458753053161068, -1.6225284841887624, -1.6225284841887624], time: 109.267
steps: 199975, episodes: 8000, mean episode reward: -7.186874753923775, agent episode reward: [-21.250142977533127, 7.031634111804674, 7.031634111804674], time: 109.432
steps: 224975, episodes: 9000, mean episode reward: 1.2339154849070921, agent episode reward: [-8.413095791298366, 4.823505638102728, 4.823505638102728], time: 109.891
steps: 249975, episodes: 10000, mean episode reward: -15.456782783022064, agent episode reward: [-9.80062400629266, -2.8280793883647, -2.8280793883647], time: 110.073
steps: 274975, episodes: 11000, mean episode reward: -16.563423286797036, agent episode reward: [-19.633180235521333, 1.5348784743621482, 1.5348784743621482], time: 109.852
steps: 299975, episodes: 12000, mean episode reward: -12.905180736909617, agent episode reward: [-23.501623395181117, 5.298221329135748, 5.298221329135748], time: 110.047
steps: 324975, episodes: 13000, mean episode reward: -14.510235796627766, agent episode reward: [-21.8904331020967, 3.690098652734467, 3.690098652734467], time: 110.133
steps: 349975, episodes: 14000, mean episode reward: -17.433941995325135, agent episode reward: [-21.38565568320307, 1.9758568439389665, 1.9758568439389665], time: 109.777
steps: 374975, episodes: 15000, mean episode reward: -21.600168296813035, agent episode reward: [-22.173192580462548, 0.2865121418247563, 0.2865121418247563], time: 110.732
steps: 399975, episodes: 16000, mean episode reward: -20.16617026649113, agent episode reward: [-20.962497810777368, 0.39816377214311854, 0.39816377214311854], time: 109.606
steps: 424975, episodes: 17000, mean episode reward: -22.17909410191098, agent episode reward: [-21.35422081662604, -0.4124366426424709, -0.4124366426424709], time: 108.976
steps: 449975, episodes: 18000, mean episode reward: -22.128189804622984, agent episode reward: [-20.943592336732376, -0.5922987339453012, -0.5922987339453012], time: 110.837
steps: 474975, episodes: 19000, mean episode reward: -24.19463326814682, agent episode reward: [-22.8772365566086, -0.6586983557691071, -0.6586983557691071], time: 111.334
steps: 499975, episodes: 20000, mean episode reward: -23.670063378884247, agent episode reward: [-22.383560203829518, -0.6432515875273639, -0.6432515875273639], time: 110.543
steps: 524975, episodes: 21000, mean episode reward: -17.896125697600315, agent episode reward: [-23.79701231554594, 2.9504433089728126, 2.9504433089728126], time: 110.426
steps: 549975, episodes: 22000, mean episode reward: -17.292392062565327, agent episode reward: [-27.610641222912047, 5.15912458017336, 5.15912458017336], time: 110.426
steps: 574975, episodes: 23000, mean episode reward: -28.12971991378166, agent episode reward: [-30.511236673061077, 1.1907583796397094, 1.1907583796397094], time: 111.355
steps: 599975, episodes: 24000, mean episode reward: -13.760180509731404, agent episode reward: [-34.333915915863116, 10.286867703065859, 10.286867703065859], time: 111.329
steps: 624975, episodes: 25000, mean episode reward: -8.955464289913245, agent episode reward: [-33.44122008217063, 12.242877896128691, 12.242877896128691], time: 109.9
steps: 649975, episodes: 26000, mean episode reward: 4.549267749923385, agent episode reward: [-34.77493818106975, 19.66210296549657, 19.66210296549657], time: 110.284
steps: 674975, episodes: 27000, mean episode reward: -1.230017076643931, agent episode reward: [-24.948532378026474, 11.85925765069127, 11.85925765069127], time: 110.571
steps: 699975, episodes: 28000, mean episode reward: -5.441997823311156, agent episode reward: [-18.718586930089383, 6.6382945533891125, 6.6382945533891125], time: 109.619
steps: 724975, episodes: 29000, mean episode reward: -8.802971239662973, agent episode reward: [-15.823817261266319, 3.510423010801674, 3.510423010801674], time: 109.375
steps: 749975, episodes: 30000, mean episode reward: -8.650870070255653, agent episode reward: [-5.479762575817893, -1.585553747218879, -1.585553747218879], time: 110.952
steps: 774975, episodes: 31000, mean episode reward: -14.06545695683337, agent episode reward: [-14.554611018353643, 0.2445770307601359, 0.2445770307601359], time: 110.585
steps: 799975, episodes: 32000, mean episode reward: -12.777696524009317, agent episode reward: [-14.73345279267568, 0.977878134333182, 0.977878134333182], time: 110.643
steps: 824975, episodes: 33000, mean episode reward: -17.89411340969149, agent episode reward: [-18.190573229998837, 0.14822991015367518, 0.14822991015367518], time: 110.692
steps: 849975, episodes: 34000, mean episode reward: -26.180616914163405, agent episode reward: [-20.720861322309023, -2.72987779592719, -2.72987779592719], time: 110.447
steps: 874975, episodes: 35000, mean episode reward: -21.665265322974232, agent episode reward: [-23.28595711903536, 0.8103458980305648, 0.8103458980305648], time: 110.624
steps: 899975, episodes: 36000, mean episode reward: -21.80899941277968, agent episode reward: [-23.47953089266942, 0.8352657399448689, 0.8352657399448689], time: 110.4
steps: 924975, episodes: 37000, mean episode reward: -22.23528126256033, agent episode reward: [-22.93885574590758, 0.351787241673624, 0.351787241673624], time: 110.741
steps: 949975, episodes: 38000, mean episode reward: -21.858931293972834, agent episode reward: [-22.613740293333738, 0.37740449968045187, 0.37740449968045187], time: 111.153
steps: 974975, episodes: 39000, mean episode reward: -23.621152112513755, agent episode reward: [-22.578760745640203, -0.5211956834367746, -0.5211956834367746], time: 110.062
steps: 999975, episodes: 40000, mean episode reward: -24.68431700108511, agent episode reward: [-23.478340592837046, -0.6029882041240339, -0.6029882041240339], time: 110.437
steps: 1024975, episodes: 41000, mean episode reward: -23.780070401125776, agent episode reward: [-23.695447060422573, -0.04231167035160263, -0.04231167035160263], time: 111.228
steps: 1049975, episodes: 42000, mean episode reward: -24.806549200108943, agent episode reward: [-24.32204847489606, -0.2422503626064415, -0.2422503626064415], time: 110.848
steps: 1074975, episodes: 43000, mean episode reward: -25.029955975469875, agent episode reward: [-20.961769341623974, -2.0340933169229483, -2.0340933169229483], time: 110.563
steps: 1099975, episodes: 44000, mean episode reward: -21.93387185343248, agent episode reward: [-20.60712041940523, -0.6633757170136241, -0.6633757170136241], time: 110.425
steps: 1124975, episodes: 45000, mean episode reward: -21.768968145020185, agent episode reward: [-22.943573632752983, 0.5873027438663989, 0.5873027438663989], time: 111.814
steps: 1149975, episodes: 46000, mean episode reward: -27.452017320298783, agent episode reward: [-22.53906357876812, -2.4564768707653295, -2.4564768707653295], time: 111.279
steps: 1174975, episodes: 47000, mean episode reward: -20.34578173095586, agent episode reward: [-22.265304605809956, 0.9597614374270462, 0.9597614374270462], time: 110.914
steps: 1199975, episodes: 48000, mean episode reward: -27.607899553508584, agent episode reward: [-16.182050157339084, -5.712924698084749, -5.712924698084749], time: 112.891
steps: 1224975, episodes: 49000, mean episode reward: -24.946551747372467, agent episode reward: [-15.894380381095576, -4.526085683138446, -4.526085683138446], time: 111.25
steps: 1249975, episodes: 50000, mean episode reward: -11.25689203293653, agent episode reward: [-14.056037433692456, 1.3995727003779626, 1.3995727003779626], time: 111.329
steps: 1274975, episodes: 51000, mean episode reward: -9.441011896999731, agent episode reward: [-13.984189125601329, 2.2715886143008, 2.2715886143008], time: 111.347
steps: 1299975, episodes: 52000, mean episode reward: -11.276941415222057, agent episode reward: [-14.252609029424416, 1.487833807101182, 1.487833807101182], time: 111.628
steps: 1324975, episodes: 53000, mean episode reward: -8.677707762527865, agent episode reward: [-15.263457268242641, 3.292874752857387, 3.292874752857387], time: 111.428
steps: 1349975, episodes: 54000, mean episode reward: -14.283146004781516, agent episode reward: [-14.802799384522249, 0.259826689870364, 0.259826689870364], time: 110.875
steps: 1374975, episodes: 55000, mean episode reward: -15.777908689847164, agent episode reward: [-13.415366882202282, -1.181270903822443, -1.181270903822443], time: 112.089
steps: 1399975, episodes: 56000, mean episode reward: -10.72357950665507, agent episode reward: [-14.623180542959572, 1.9498005181522504, 1.9498005181522504], time: 111.272
steps: 1424975, episodes: 57000, mean episode reward: -13.982082248720767, agent episode reward: [-14.341003036046626, 0.17946039366293154, 0.17946039366293154], time: 111.116
steps: 1449975, episodes: 58000, mean episode reward: -10.135050005894012, agent episode reward: [-17.345091306610485, 3.6050206503582367, 3.6050206503582367], time: 110.915
steps: 1474975, episodes: 59000, mean episode reward: -10.697231876304356, agent episode reward: [-13.91753048933356, 1.6101493065146026, 1.6101493065146026], time: 111.811
steps: 1499975, episodes: 60000, mean episode reward: -8.485462038055383, agent episode reward: [-15.576822200346989, 3.5456800811458025, 3.5456800811458025], time: 102.121
...Finished total of 60001 episodes.
