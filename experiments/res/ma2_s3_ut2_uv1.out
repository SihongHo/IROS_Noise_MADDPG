Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -26.892143752343944, agent episode reward: [-0.2778923986346619, -26.61425135370928], time: 41.674
steps: 49975, episodes: 2000, mean episode reward: -23.440084709148962, agent episode reward: [-4.934220693066103, -18.505864016082857], time: 58.612
steps: 74975, episodes: 3000, mean episode reward: -13.501389552713526, agent episode reward: [-5.742756796223569, -7.758632756489956], time: 50.264
steps: 99975, episodes: 4000, mean episode reward: -10.812811088661954, agent episode reward: [-3.6678993490727474, -7.1449117395892054], time: 49.389
steps: 124975, episodes: 5000, mean episode reward: -10.509124559928498, agent episode reward: [-2.9975604656243666, -7.511564094304131], time: 50.265
steps: 149975, episodes: 6000, mean episode reward: -10.419772361373143, agent episode reward: [-3.0588069320771427, -7.360965429296], time: 50.947
steps: 174975, episodes: 7000, mean episode reward: -10.001866609136117, agent episode reward: [-2.5947056642154984, -7.40716094492062], time: 52.345
steps: 199975, episodes: 8000, mean episode reward: -10.127126225304847, agent episode reward: [-2.3672259197820473, -7.759900305522799], time: 62.47
steps: 224975, episodes: 9000, mean episode reward: -10.185269435458553, agent episode reward: [-2.409659996081019, -7.775609439377533], time: 63.424
steps: 249975, episodes: 10000, mean episode reward: -9.990235078013008, agent episode reward: [-2.4654464796787185, -7.524788598334289], time: 62.902
steps: 274975, episodes: 11000, mean episode reward: -9.877819029808135, agent episode reward: [-2.0576893189143584, -7.820129710893777], time: 64.381
steps: 299975, episodes: 12000, mean episode reward: -10.164515291796484, agent episode reward: [-2.240111457101331, -7.924403834695152], time: 64.552
steps: 324975, episodes: 13000, mean episode reward: -10.433533275214831, agent episode reward: [-2.5490223920889674, -7.884510883125864], time: 63.819
steps: 349975, episodes: 14000, mean episode reward: -9.847433965068692, agent episode reward: [-1.8373297278736973, -8.010104237194996], time: 64.393
steps: 374975, episodes: 15000, mean episode reward: -10.188851523212467, agent episode reward: [-2.352696957541144, -7.836154565671323], time: 64.047
steps: 399975, episodes: 16000, mean episode reward: -10.423247515850807, agent episode reward: [-2.4842782925708464, -7.938969223279961], time: 63.668
steps: 424975, episodes: 17000, mean episode reward: -10.487320872749253, agent episode reward: [-2.0929923651605415, -8.39432850758871], time: 63.742
steps: 449975, episodes: 18000, mean episode reward: -10.44698536318371, agent episode reward: [-2.5581162546744483, -7.888869108509263], time: 63.643
steps: 474975, episodes: 19000, mean episode reward: -10.078246362142547, agent episode reward: [-1.9512928347758876, -8.126953527366659], time: 63.428
steps: 499975, episodes: 20000, mean episode reward: -10.549107608558048, agent episode reward: [-2.331363506952691, -8.217744101605357], time: 62.98
steps: 524975, episodes: 21000, mean episode reward: -10.27154076094033, agent episode reward: [-2.06600231048446, -8.205538450455872], time: 63.672
steps: 549975, episodes: 22000, mean episode reward: -10.128109171839846, agent episode reward: [-2.547406546307092, -7.580702625532753], time: 64.536
steps: 574975, episodes: 23000, mean episode reward: -10.211206484536316, agent episode reward: [-2.4442047037870176, -7.767001780749298], time: 63.965
steps: 599975, episodes: 24000, mean episode reward: -9.938846163813329, agent episode reward: [-2.194481933374854, -7.744364230438474], time: 64.311
steps: 624975, episodes: 25000, mean episode reward: -10.014354162746587, agent episode reward: [-1.9360896794929217, -8.078264483253665], time: 63.962
steps: 649975, episodes: 26000, mean episode reward: -9.968518939628414, agent episode reward: [-2.443219756575627, -7.5252991830527876], time: 63.71
steps: 674975, episodes: 27000, mean episode reward: -10.401085290322099, agent episode reward: [-2.100272005544644, -8.300813284777455], time: 64.978
steps: 699975, episodes: 28000, mean episode reward: -9.976539824099921, agent episode reward: [-1.9183833841271012, -8.05815643997282], time: 67.197
steps: 724975, episodes: 29000, mean episode reward: -9.886305100320623, agent episode reward: [-2.050403917650379, -7.835901182670244], time: 66.744
steps: 749975, episodes: 30000, mean episode reward: -9.92150537490071, agent episode reward: [-1.6570386096867653, -8.264466765213946], time: 67.981
steps: 774975, episodes: 31000, mean episode reward: -9.961321950622565, agent episode reward: [-1.8567418974874241, -8.104580053135141], time: 67.939
steps: 799975, episodes: 32000, mean episode reward: -9.836024374003308, agent episode reward: [-1.9239028519569792, -7.912121522046331], time: 67.867
steps: 824975, episodes: 33000, mean episode reward: -9.802480327575587, agent episode reward: [-1.8108767164584147, -7.991603611117174], time: 68.011
steps: 849975, episodes: 34000, mean episode reward: -9.723826520858466, agent episode reward: [-1.8000587619984918, -7.923767758859974], time: 68.421
steps: 874975, episodes: 35000, mean episode reward: -9.43845290273237, agent episode reward: [-1.8106652360875524, -7.627787666644817], time: 67.682
steps: 899975, episodes: 36000, mean episode reward: -9.470779634979982, agent episode reward: [-1.462956387761528, -8.007823247218454], time: 68.635
steps: 924975, episodes: 37000, mean episode reward: -9.947026411435159, agent episode reward: [-2.141378122790446, -7.805648288644713], time: 67.511
steps: 949975, episodes: 38000, mean episode reward: -9.71480779084037, agent episode reward: [-2.1799376249152473, -7.534870165925122], time: 68.085
steps: 974975, episodes: 39000, mean episode reward: -10.046867074945057, agent episode reward: [-1.8020165059324813, -8.244850569012575], time: 67.764
steps: 999975, episodes: 40000, mean episode reward: -10.089087123723859, agent episode reward: [-1.9367261641136078, -8.15236095961025], time: 67.801
steps: 1024975, episodes: 41000, mean episode reward: -9.722682319663015, agent episode reward: [-1.5387614611223712, -8.183920858540645], time: 68.02
steps: 1049975, episodes: 42000, mean episode reward: -9.824817559354937, agent episode reward: [-1.6911164688048819, -8.133701090550053], time: 68.342
steps: 1074975, episodes: 43000, mean episode reward: -9.901386054481915, agent episode reward: [-1.8678267193287075, -8.033559335153207], time: 68.672
steps: 1099975, episodes: 44000, mean episode reward: -9.685299587953047, agent episode reward: [-1.8000313814814535, -7.885268206471593], time: 68.367
steps: 1124975, episodes: 45000, mean episode reward: -10.271500763806111, agent episode reward: [-2.007009728639922, -8.264491035166188], time: 69.415
steps: 1149975, episodes: 46000, mean episode reward: -10.48179623207949, agent episode reward: [-1.9321338673970279, -8.549662364682463], time: 68.665
steps: 1174975, episodes: 47000, mean episode reward: -10.75603844356352, agent episode reward: [-1.6971444831477338, -9.058893960415787], time: 68.462
steps: 1199975, episodes: 48000, mean episode reward: -10.893181406590926, agent episode reward: [-1.2679378235177277, -9.6252435830732], time: 68.578
steps: 1224975, episodes: 49000, mean episode reward: -11.311921153192092, agent episode reward: [-0.6443382011902402, -10.667582952001853], time: 68.436
steps: 1249975, episodes: 50000, mean episode reward: -10.573709609227791, agent episode reward: [-1.6872692475782092, -8.886440361649584], time: 68.54
steps: 1274975, episodes: 51000, mean episode reward: -10.776790168875145, agent episode reward: [-2.733861076212337, -8.042929092662805], time: 68.266
steps: 1299975, episodes: 52000, mean episode reward: -10.974476782012466, agent episode reward: [-2.7884408873325937, -8.186035894679874], time: 68.151
steps: 1324975, episodes: 53000, mean episode reward: -11.33765308642452, agent episode reward: [-2.7260598059926844, -8.611593280431837], time: 68.1
steps: 1349975, episodes: 54000, mean episode reward: -10.92816420670133, agent episode reward: [-1.9590446832809834, -8.969119523420348], time: 68.45
steps: 1374975, episodes: 55000, mean episode reward: -11.190934317772527, agent episode reward: [-2.210563748360394, -8.980370569412136], time: 68.291
steps: 1399975, episodes: 56000, mean episode reward: -11.21235200806064, agent episode reward: [-2.5754648854454327, -8.636887122615208], time: 69.313
steps: 1424975, episodes: 57000, mean episode reward: -11.29495228804454, agent episode reward: [-3.0715926940297487, -8.22335959401479], time: 68.779
steps: 1449975, episodes: 58000, mean episode reward: -11.113786480392678, agent episode reward: [-3.461628170235881, -7.652158310156798], time: 68.398
steps: 1474975, episodes: 59000, mean episode reward: -11.014370941771872, agent episode reward: [-3.6426119969818336, -7.371758944790039], time: 68.366
steps: 1499975, episodes: 60000, mean episode reward: -10.70150024822504, agent episode reward: [-3.4043200615538125, -7.297180186671227], time: 66.131
...Finished total of 60001 episodes.
