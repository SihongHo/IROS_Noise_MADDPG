Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -1.8279945849334867, agent episode reward: [2.59, 2.59, 2.59, -9.597994584933486], time: 101.684
steps: 49975, episodes: 2000, mean episode reward: -3.7146102395632346, agent episode reward: [3.84, 3.84, 3.84, -15.234610239563235], time: 189.385
steps: 74975, episodes: 3000, mean episode reward: 8.64499980588787, agent episode reward: [5.06, 5.06, 5.06, -6.535000194112131], time: 186.608
steps: 99975, episodes: 4000, mean episode reward: 10.020836837942644, agent episode reward: [5.26, 5.26, 5.26, -5.759163162057355], time: 187.36
steps: 124975, episodes: 5000, mean episode reward: 13.95028270580475, agent episode reward: [7.36, 7.36, 7.36, -8.129717294195252], time: 186.393
steps: 149975, episodes: 6000, mean episode reward: 34.99363412674608, agent episode reward: [17.66, 17.66, 17.66, -17.986365873253927], time: 187.04
steps: 174975, episodes: 7000, mean episode reward: 56.88050906995986, agent episode reward: [28.98, 28.98, 28.98, -30.05949093004015], time: 187.533
steps: 199975, episodes: 8000, mean episode reward: 13.871764345481443, agent episode reward: [9.97, 9.97, 9.97, -16.03823565451856], time: 186.744
steps: 224975, episodes: 9000, mean episode reward: 3.2362801394549705, agent episode reward: [4.32, 4.32, 4.32, -9.723719860545028], time: 187.592
steps: 249975, episodes: 10000, mean episode reward: 1.5836569995219583, agent episode reward: [3.69, 3.69, 3.69, -9.486343000478042], time: 187.567
steps: 274975, episodes: 11000, mean episode reward: 2.876003885517351, agent episode reward: [3.71, 3.71, 3.71, -8.25399611448265], time: 188.128
steps: 299975, episodes: 12000, mean episode reward: 4.940516444814943, agent episode reward: [4.44, 4.44, 4.44, -8.379483555185057], time: 188.299
steps: 324975, episodes: 13000, mean episode reward: 5.456593575552752, agent episode reward: [4.62, 4.62, 4.62, -8.403406424447248], time: 187.248
steps: 349975, episodes: 14000, mean episode reward: 6.615417362765119, agent episode reward: [4.97, 4.97, 4.97, -8.29458263723488], time: 188.611
steps: 374975, episodes: 15000, mean episode reward: 9.463146861999256, agent episode reward: [6.19, 6.19, 6.19, -9.106853138000746], time: 187.429
steps: 399975, episodes: 16000, mean episode reward: 10.209658961923397, agent episode reward: [6.32, 6.32, 6.32, -8.750341038076604], time: 187.39
steps: 424975, episodes: 17000, mean episode reward: 8.661840149671987, agent episode reward: [5.77, 5.77, 5.77, -8.648159850328012], time: 188.372
steps: 449975, episodes: 18000, mean episode reward: 10.39052341596508, agent episode reward: [6.16, 6.16, 6.16, -8.089476584034921], time: 187.304
steps: 474975, episodes: 19000, mean episode reward: 13.931324486081929, agent episode reward: [7.74, 7.74, 7.74, -9.288675513918072], time: 186.819
steps: 499975, episodes: 20000, mean episode reward: 19.356859048337093, agent episode reward: [10.73, 10.73, 10.73, -12.833140951662909], time: 187.631
steps: 524975, episodes: 21000, mean episode reward: 24.4893837067535, agent episode reward: [13.49, 13.49, 13.49, -15.980616293246502], time: 186.894
steps: 549975, episodes: 22000, mean episode reward: 19.73594031710935, agent episode reward: [11.06, 11.06, 11.06, -13.44405968289065], time: 186.689
steps: 574975, episodes: 23000, mean episode reward: 20.526107194557, agent episode reward: [11.45, 11.45, 11.45, -13.823892805443], time: 187.926
steps: 599975, episodes: 24000, mean episode reward: 17.184087489242327, agent episode reward: [9.96, 9.96, 9.96, -12.695912510757672], time: 186.671
steps: 624975, episodes: 25000, mean episode reward: 20.729001508899717, agent episode reward: [11.55, 11.55, 11.55, -13.920998491100283], time: 186.699
steps: 649975, episodes: 26000, mean episode reward: 19.81148095661703, agent episode reward: [11.17, 11.17, 11.17, -13.698519043382971], time: 186.646
steps: 674975, episodes: 27000, mean episode reward: 14.143274080907009, agent episode reward: [9.18, 9.18, 9.18, -13.39672591909299], time: 187.282
steps: 699975, episodes: 28000, mean episode reward: 20.722437207483658, agent episode reward: [11.95, 11.95, 11.95, -15.127562792516343], time: 187.651
steps: 724975, episodes: 29000, mean episode reward: 17.528172549224173, agent episode reward: [10.07, 10.07, 10.07, -12.681827450775826], time: 188.93
steps: 749975, episodes: 30000, mean episode reward: 24.001380424308635, agent episode reward: [13.54, 13.54, 13.54, -16.618619575691365], time: 186.149
steps: 774975, episodes: 31000, mean episode reward: 25.40556907594634, agent episode reward: [14.37, 14.37, 14.37, -17.70443092405366], time: 187.407
steps: 799975, episodes: 32000, mean episode reward: 26.462085519241636, agent episode reward: [14.56, 14.56, 14.56, -17.21791448075837], time: 187.49
steps: 824975, episodes: 33000, mean episode reward: 25.731105929329757, agent episode reward: [14.42, 14.42, 14.42, -17.528894070670244], time: 187.596
steps: 849975, episodes: 34000, mean episode reward: 22.90883206264464, agent episode reward: [13.26, 13.26, 13.26, -16.871167937355363], time: 187.71
steps: 874975, episodes: 35000, mean episode reward: 29.460298698439534, agent episode reward: [16.18, 16.18, 16.18, -19.07970130156047], time: 187.174
steps: 899975, episodes: 36000, mean episode reward: 29.049732638536362, agent episode reward: [16.26, 16.26, 16.26, -19.730267361463632], time: 189.068
steps: 924975, episodes: 37000, mean episode reward: 26.00923389380773, agent episode reward: [15.08, 15.08, 15.08, -19.23076610619227], time: 186.703
steps: 949975, episodes: 38000, mean episode reward: 23.469034806021913, agent episode reward: [13.94, 13.94, 13.94, -18.350965193978087], time: 185.815
steps: 974975, episodes: 39000, mean episode reward: 24.178016280017356, agent episode reward: [14.47, 14.47, 14.47, -19.23198371998264], time: 185.996
steps: 999975, episodes: 40000, mean episode reward: 24.278580579619128, agent episode reward: [14.48, 14.48, 14.48, -19.161419420380874], time: 186.202
steps: 1024975, episodes: 41000, mean episode reward: 27.773910416222215, agent episode reward: [16.13, 16.13, 16.13, -20.616089583777786], time: 187.175
steps: 1049975, episodes: 42000, mean episode reward: 25.665798021563244, agent episode reward: [14.85, 14.85, 14.85, -18.884201978436753], time: 187.735
steps: 1074975, episodes: 43000, mean episode reward: 23.781836911379596, agent episode reward: [14.29, 14.29, 14.29, -19.088163088620405], time: 187.073
steps: 1099975, episodes: 44000, mean episode reward: 27.01297229502748, agent episode reward: [15.53, 15.53, 15.53, -19.57702770497252], time: 186.602
steps: 1124975, episodes: 45000, mean episode reward: 24.651927546165332, agent episode reward: [13.99, 13.99, 13.99, -17.31807245383467], time: 187.477
steps: 1149975, episodes: 46000, mean episode reward: 27.24625792192394, agent episode reward: [15.4, 15.4, 15.4, -18.953742078076058], time: 184.184
steps: 1174975, episodes: 47000, mean episode reward: 26.907133675388643, agent episode reward: [15.06, 15.06, 15.06, -18.272866324611357], time: 183.588
steps: 1199975, episodes: 48000, mean episode reward: 27.41171225651226, agent episode reward: [15.31, 15.31, 15.31, -18.518287743487736], time: 185.241
steps: 1224975, episodes: 49000, mean episode reward: 26.277706429503333, agent episode reward: [14.68, 14.68, 14.68, -17.762293570496666], time: 184.261
steps: 1249975, episodes: 50000, mean episode reward: 29.59642011677369, agent episode reward: [16.53, 16.53, 16.53, -19.99357988322631], time: 183.83
steps: 1274975, episodes: 51000, mean episode reward: 31.14428733652219, agent episode reward: [17.37, 17.37, 17.37, -20.96571266347781], time: 184.854
steps: 1299975, episodes: 52000, mean episode reward: 30.201878785045324, agent episode reward: [16.82, 16.82, 16.82, -20.258121214954677], time: 183.666
steps: 1324975, episodes: 53000, mean episode reward: 24.67268500374536, agent episode reward: [14.31, 14.31, 14.31, -18.25731499625464], time: 183.079
steps: 1349975, episodes: 54000, mean episode reward: 33.36293742369558, agent episode reward: [18.2, 18.2, 18.2, -21.237062576304417], time: 184.398
steps: 1374975, episodes: 55000, mean episode reward: 29.578427298082342, agent episode reward: [16.57, 16.57, 16.57, -20.13157270191766], time: 185.077
steps: 1399975, episodes: 56000, mean episode reward: 30.59125042931451, agent episode reward: [17.22, 17.22, 17.22, -21.06874957068549], time: 183.34
steps: 1424975, episodes: 57000, mean episode reward: 27.035838007127733, agent episode reward: [15.64, 15.64, 15.64, -19.88416199287227], time: 183.061
steps: 1449975, episodes: 58000, mean episode reward: 22.983468078238605, agent episode reward: [13.67, 13.67, 13.67, -18.026531921761393], time: 183.309
steps: 1474975, episodes: 59000, mean episode reward: 31.256591804688515, agent episode reward: [17.66, 17.66, 17.66, -21.723408195311485], time: 184.839
steps: 1499975, episodes: 60000, mean episode reward: 22.853855823294996, agent episode reward: [13.78, 13.78, 13.78, -18.486144176705004], time: 183.595
...Finished total of 60001 episodes.
