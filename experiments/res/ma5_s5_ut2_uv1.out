Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -2.179713391704497, agent episode reward: [2.34, 2.34, 2.34, -9.199713391704497], time: 101.328
steps: 49975, episodes: 2000, mean episode reward: -19.98576798962951, agent episode reward: [3.37, 3.37, 3.37, -30.095767989629515], time: 189.07
steps: 74975, episodes: 3000, mean episode reward: 8.617047124703916, agent episode reward: [5.1, 5.1, 5.1, -6.682952875296082], time: 188.016
steps: 99975, episodes: 4000, mean episode reward: 9.695449283520341, agent episode reward: [5.07, 5.07, 5.07, -5.514550716479658], time: 188.603
steps: 124975, episodes: 5000, mean episode reward: 9.899533295714868, agent episode reward: [5.14, 5.14, 5.14, -5.520466704285132], time: 188.596
steps: 149975, episodes: 6000, mean episode reward: 10.507218911301527, agent episode reward: [5.42, 5.42, 5.42, -5.752781088698475], time: 188.039
steps: 174975, episodes: 7000, mean episode reward: 14.070744305900561, agent episode reward: [7.22, 7.22, 7.22, -7.589255694099438], time: 187.802
steps: 199975, episodes: 8000, mean episode reward: 15.286148505604226, agent episode reward: [7.86, 7.86, 7.86, -8.29385149439577], time: 188.16
steps: 224975, episodes: 9000, mean episode reward: 21.425598682159116, agent episode reward: [11.33, 11.33, 11.33, -12.564401317840884], time: 189.179
steps: 249975, episodes: 10000, mean episode reward: 32.873552915934695, agent episode reward: [16.93, 16.93, 16.93, -17.916447084065297], time: 188.084
steps: 274975, episodes: 11000, mean episode reward: 30.395390124159103, agent episode reward: [15.73, 15.73, 15.73, -16.794609875840894], time: 190.047
steps: 299975, episodes: 12000, mean episode reward: 29.4270426706076, agent episode reward: [15.78, 15.78, 15.78, -17.9129573293924], time: 188.1
steps: 324975, episodes: 13000, mean episode reward: 23.807185659537208, agent episode reward: [14.34, 14.34, 14.34, -19.21281434046279], time: 188.518
steps: 349975, episodes: 14000, mean episode reward: 25.863629904115665, agent episode reward: [15.22, 15.22, 15.22, -19.79637009588433], time: 190.03
steps: 374975, episodes: 15000, mean episode reward: 26.4227209043091, agent episode reward: [16.0, 16.0, 16.0, -21.5772790956909], time: 188.663
steps: 399975, episodes: 16000, mean episode reward: 33.37258796219327, agent episode reward: [19.03, 19.03, 19.03, -23.71741203780674], time: 188.218
steps: 424975, episodes: 17000, mean episode reward: 32.04118713263067, agent episode reward: [17.97, 17.97, 17.97, -21.868812867369332], time: 188.233
steps: 449975, episodes: 18000, mean episode reward: 28.422860578438538, agent episode reward: [16.05, 16.05, 16.05, -19.727139421561464], time: 190.083
steps: 474975, episodes: 19000, mean episode reward: 24.834345500570222, agent episode reward: [14.7, 14.7, 14.7, -19.265654499429775], time: 189.059
steps: 499975, episodes: 20000, mean episode reward: 20.117307399686712, agent episode reward: [12.56, 12.56, 12.56, -17.562692600313287], time: 189.227
steps: 524975, episodes: 21000, mean episode reward: 17.569883722564665, agent episode reward: [12.03, 12.03, 12.03, -18.52011627743533], time: 189.782
steps: 549975, episodes: 22000, mean episode reward: 20.729191256380822, agent episode reward: [12.88, 12.88, 12.88, -17.91080874361918], time: 189.606
steps: 574975, episodes: 23000, mean episode reward: 18.427947754767242, agent episode reward: [11.5, 11.5, 11.5, -16.072052245232758], time: 189.069
steps: 599975, episodes: 24000, mean episode reward: 16.085049180054988, agent episode reward: [10.64, 10.64, 10.64, -15.834950819945016], time: 188.567
steps: 624975, episodes: 25000, mean episode reward: 19.16524350327903, agent episode reward: [11.6, 11.6, 11.6, -15.63475649672097], time: 188.346
steps: 649975, episodes: 26000, mean episode reward: 20.9436391547058, agent episode reward: [12.17, 12.17, 12.17, -15.566360845294202], time: 188.352
steps: 674975, episodes: 27000, mean episode reward: 19.527131757530043, agent episode reward: [11.63, 11.63, 11.63, -15.362868242469961], time: 190.009
steps: 699975, episodes: 28000, mean episode reward: 19.556159099863685, agent episode reward: [11.47, 11.47, 11.47, -14.853840900136316], time: 188.957
steps: 724975, episodes: 29000, mean episode reward: 21.74183821872827, agent episode reward: [12.14, 12.14, 12.14, -14.678161781271728], time: 190.416
steps: 749975, episodes: 30000, mean episode reward: 22.990796634614902, agent episode reward: [12.86, 12.86, 12.86, -15.589203365385098], time: 188.48
steps: 774975, episodes: 31000, mean episode reward: 22.404444458691316, agent episode reward: [12.5, 12.5, 12.5, -15.095555541308682], time: 188.618
steps: 799975, episodes: 32000, mean episode reward: 23.043535008237882, agent episode reward: [12.7, 12.7, 12.7, -15.056464991762118], time: 189.878
steps: 824975, episodes: 33000, mean episode reward: 26.04874970447105, agent episode reward: [14.23, 14.23, 14.23, -16.64125029552895], time: 188.537
steps: 849975, episodes: 34000, mean episode reward: 24.859729286656158, agent episode reward: [13.55, 13.55, 13.55, -15.790270713343842], time: 188.972
steps: 874975, episodes: 35000, mean episode reward: 27.08878840063021, agent episode reward: [14.55, 14.55, 14.55, -16.561211599369788], time: 189.416
steps: 899975, episodes: 36000, mean episode reward: 28.793104070208305, agent episode reward: [15.58, 15.58, 15.58, -17.946895929791697], time: 190.995
steps: 924975, episodes: 37000, mean episode reward: 28.104387876978606, agent episode reward: [15.4, 15.4, 15.4, -18.09561212302139], time: 187.316
steps: 949975, episodes: 38000, mean episode reward: 33.91090301732147, agent episode reward: [18.17, 18.17, 18.17, -20.59909698267853], time: 188.132
steps: 974975, episodes: 39000, mean episode reward: 35.48161179762331, agent episode reward: [19.05, 19.05, 19.05, -21.668388202376693], time: 189.432
steps: 999975, episodes: 40000, mean episode reward: 38.09935232702096, agent episode reward: [20.43, 20.43, 20.43, -23.190647672979036], time: 189.567
steps: 1024975, episodes: 41000, mean episode reward: 45.476865132310806, agent episode reward: [24.24, 24.24, 24.24, -27.243134867689196], time: 188.494
steps: 1049975, episodes: 42000, mean episode reward: 44.527887628182924, agent episode reward: [23.95, 23.95, 23.95, -27.322112371817074], time: 188.738
steps: 1074975, episodes: 43000, mean episode reward: 35.92593667605202, agent episode reward: [20.65, 20.65, 20.65, -26.02406332394797], time: 189.164
steps: 1099975, episodes: 44000, mean episode reward: 32.40199545926496, agent episode reward: [18.98, 18.98, 18.98, -24.538004540735034], time: 187.615
steps: 1124975, episodes: 45000, mean episode reward: 34.66102144775577, agent episode reward: [20.09, 20.09, 20.09, -25.60897855224422], time: 187.909
steps: 1149975, episodes: 46000, mean episode reward: 31.931800879440612, agent episode reward: [19.23, 19.23, 19.23, -25.75819912055939], time: 184.865
steps: 1174975, episodes: 47000, mean episode reward: 32.117288427239366, agent episode reward: [19.95, 19.95, 19.95, -27.732711572760635], time: 186.282
steps: 1199975, episodes: 48000, mean episode reward: 32.70511463484562, agent episode reward: [19.79, 19.79, 19.79, -26.664885365154376], time: 187.332
steps: 1224975, episodes: 49000, mean episode reward: 30.139706084346297, agent episode reward: [18.31, 18.31, 18.31, -24.790293915653702], time: 185.55
steps: 1249975, episodes: 50000, mean episode reward: 29.28924578045834, agent episode reward: [17.95, 17.95, 17.95, -24.56075421954166], time: 185.809
steps: 1274975, episodes: 51000, mean episode reward: 28.272094781399232, agent episode reward: [17.5, 17.5, 17.5, -24.227905218600764], time: 187.366
steps: 1299975, episodes: 52000, mean episode reward: 22.2412983178399, agent episode reward: [14.38, 14.38, 14.38, -20.898701682160105], time: 184.772
steps: 1324975, episodes: 53000, mean episode reward: 26.340212534190854, agent episode reward: [15.91, 15.91, 15.91, -21.389787465809146], time: 185.217
steps: 1349975, episodes: 54000, mean episode reward: 28.36046019661374, agent episode reward: [16.48, 16.48, 16.48, -21.079539803386258], time: 185.104
steps: 1374975, episodes: 55000, mean episode reward: 25.435202457413556, agent episode reward: [15.43, 15.43, 15.43, -20.854797542586443], time: 186.224
steps: 1399975, episodes: 56000, mean episode reward: 24.036543558163583, agent episode reward: [14.69, 14.69, 14.69, -20.033456441836417], time: 184.651
steps: 1424975, episodes: 57000, mean episode reward: 18.944139739741775, agent episode reward: [12.39, 12.39, 12.39, -18.225860260258226], time: 184.215
steps: 1449975, episodes: 58000, mean episode reward: 19.44287371692624, agent episode reward: [12.4, 12.4, 12.4, -17.757126283073767], time: 185.507
steps: 1474975, episodes: 59000, mean episode reward: 17.90485475223264, agent episode reward: [11.44, 11.44, 11.44, -16.415145247767363], time: 185.945
steps: 1499975, episodes: 60000, mean episode reward: 16.518373055264657, agent episode reward: [11.18, 11.18, 11.18, -17.02162694473535], time: 175.392
...Finished total of 60001 episodes.
