Using good policy maddpg and adv policy maddpg
Uncertainty type is:  None ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -6.207711126645968, agent episode reward: [2.17, 2.17, 2.17, -12.717711126645966], time: 141.965
steps: 49975, episodes: 2000, mean episode reward: 6.9016223788550075, agent episode reward: [4.19, 4.19, 4.19, -5.668377621144993], time: 170.741
steps: 74975, episodes: 3000, mean episode reward: 7.385074489894405, agent episode reward: [3.87, 3.87, 3.87, -4.224925510105594], time: 168.635
steps: 99975, episodes: 4000, mean episode reward: 12.587316243997238, agent episode reward: [6.48, 6.48, 6.48, -6.852683756002762], time: 169.709
steps: 124975, episodes: 5000, mean episode reward: 38.83961027503145, agent episode reward: [19.96, 19.96, 19.96, -21.040389724968556], time: 169.092
steps: 149975, episodes: 6000, mean episode reward: 56.82435003339122, agent episode reward: [32.19, 32.19, 32.19, -39.74564996660878], time: 168.996
steps: 174975, episodes: 7000, mean episode reward: 37.1618598434406, agent episode reward: [22.73, 22.73, 22.73, -31.028140156559402], time: 170.019
steps: 199975, episodes: 8000, mean episode reward: 25.056336226076933, agent episode reward: [16.11, 16.11, 16.11, -23.27366377392307], time: 169.468
steps: 224975, episodes: 9000, mean episode reward: 11.457853238503041, agent episode reward: [9.23, 9.23, 9.23, -16.23214676149696], time: 169.431
steps: 249975, episodes: 10000, mean episode reward: 11.396414414228952, agent episode reward: [8.41, 8.41, 8.41, -13.833585585771049], time: 169.699
steps: 274975, episodes: 11000, mean episode reward: 8.75976747810521, agent episode reward: [7.23, 7.23, 7.23, -12.93023252189479], time: 168.679
steps: 299975, episodes: 12000, mean episode reward: 9.652654694920294, agent episode reward: [7.07, 7.07, 7.07, -11.557345305079709], time: 170.814
steps: 324975, episodes: 13000, mean episode reward: 11.26291066154199, agent episode reward: [7.43, 7.43, 7.43, -11.027089338458008], time: 169.531
steps: 349975, episodes: 14000, mean episode reward: 10.403332425904821, agent episode reward: [6.62, 6.62, 6.62, -9.45666757409518], time: 169.074
steps: 374975, episodes: 15000, mean episode reward: 6.929619818867281, agent episode reward: [4.6, 4.6, 4.6, -6.8703801811327185], time: 169.618
steps: 399975, episodes: 16000, mean episode reward: 7.989903774669917, agent episode reward: [5.18, 5.18, 5.18, -7.550096225330084], time: 169.284
steps: 424975, episodes: 17000, mean episode reward: 7.339124692875152, agent episode reward: [4.99, 4.99, 4.99, -7.630875307124849], time: 168.911
steps: 449975, episodes: 18000, mean episode reward: 6.52400066407089, agent episode reward: [4.6, 4.6, 4.6, -7.27599933592911], time: 169.292
steps: 474975, episodes: 19000, mean episode reward: 7.115224925622008, agent episode reward: [4.73, 4.73, 4.73, -7.074775074377994], time: 168.468
steps: 499975, episodes: 20000, mean episode reward: 6.014985981982034, agent episode reward: [4.22, 4.22, 4.22, -6.645014018017966], time: 170.132
steps: 524975, episodes: 21000, mean episode reward: 7.506575873835883, agent episode reward: [4.76, 4.76, 4.76, -6.773424126164117], time: 168.363
steps: 549975, episodes: 22000, mean episode reward: 7.405142947218967, agent episode reward: [4.7, 4.7, 4.7, -6.694857052781033], time: 168.754
steps: 574975, episodes: 23000, mean episode reward: 7.728704782708161, agent episode reward: [5.03, 5.03, 5.03, -7.36129521729184], time: 169.306
steps: 599975, episodes: 24000, mean episode reward: 7.717549093371213, agent episode reward: [5.24, 5.24, 5.24, -8.002450906628786], time: 169.014
steps: 624975, episodes: 25000, mean episode reward: 10.154640258195982, agent episode reward: [6.04, 6.04, 6.04, -7.965359741804017], time: 168.95
steps: 649975, episodes: 26000, mean episode reward: 11.629847492364114, agent episode reward: [6.87, 6.87, 6.87, -8.980152507635887], time: 169.5
steps: 674975, episodes: 27000, mean episode reward: 14.983732059146583, agent episode reward: [8.36, 8.36, 8.36, -10.096267940853416], time: 168.815
steps: 699975, episodes: 28000, mean episode reward: 14.171382919388595, agent episode reward: [7.97, 7.97, 7.97, -9.738617080611403], time: 169.916
steps: 724975, episodes: 29000, mean episode reward: 17.39593384874644, agent episode reward: [9.62, 9.62, 9.62, -11.464066151253562], time: 168.765
steps: 749975, episodes: 30000, mean episode reward: 30.06923767957754, agent episode reward: [15.66, 15.66, 15.66, -16.910762320422457], time: 169.023
steps: 774975, episodes: 31000, mean episode reward: 40.71673310923841, agent episode reward: [21.01, 21.01, 21.01, -22.31326689076159], time: 169.309
steps: 799975, episodes: 32000, mean episode reward: 33.57471283161986, agent episode reward: [17.63, 17.63, 17.63, -19.315287168380134], time: 168.865
steps: 824975, episodes: 33000, mean episode reward: 25.92082322801869, agent episode reward: [14.35, 14.35, 14.35, -17.129176771981314], time: 168.985
steps: 849975, episodes: 34000, mean episode reward: 15.428363811079533, agent episode reward: [9.95, 9.95, 9.95, -14.421636188920468], time: 169.798
steps: 874975, episodes: 35000, mean episode reward: 10.559929521409963, agent episode reward: [7.58, 7.58, 7.58, -12.180070478590036], time: 169.03
steps: 899975, episodes: 36000, mean episode reward: 10.081856177797665, agent episode reward: [7.62, 7.62, 7.62, -12.778143822202335], time: 169.458
steps: 924975, episodes: 37000, mean episode reward: 16.573525001584255, agent episode reward: [10.22, 10.22, 10.22, -14.086474998415746], time: 169.571
steps: 949975, episodes: 38000, mean episode reward: 15.716925372648964, agent episode reward: [9.5, 9.5, 9.5, -12.783074627351038], time: 168.925
steps: 974975, episodes: 39000, mean episode reward: 15.983922738545852, agent episode reward: [9.36, 9.36, 9.36, -12.09607726145415], time: 169.196
steps: 999975, episodes: 40000, mean episode reward: 14.856251868036056, agent episode reward: [9.02, 9.02, 9.02, -12.203748131963945], time: 168.621
steps: 1024975, episodes: 41000, mean episode reward: 11.766760270161805, agent episode reward: [8.09, 8.09, 8.09, -12.503239729838194], time: 168.277
steps: 1049975, episodes: 42000, mean episode reward: 11.977137404112716, agent episode reward: [8.51, 8.51, 8.51, -13.552862595887284], time: 169.873
steps: 1074975, episodes: 43000, mean episode reward: 10.851891578722698, agent episode reward: [8.46, 8.46, 8.46, -14.528108421277302], time: 168.507
steps: 1099975, episodes: 44000, mean episode reward: 9.208223398150928, agent episode reward: [7.74, 7.74, 7.74, -14.011776601849073], time: 169.535
steps: 1124975, episodes: 45000, mean episode reward: 14.3144076306087, agent episode reward: [9.71, 9.71, 9.71, -14.815592369391299], time: 168.665
steps: 1149975, episodes: 46000, mean episode reward: 15.711380039404455, agent episode reward: [10.05, 10.05, 10.05, -14.438619960595544], time: 168.948
steps: 1174975, episodes: 47000, mean episode reward: 16.3106925800004, agent episode reward: [10.39, 10.39, 10.39, -14.8593074199996], time: 169.78
steps: 1199975, episodes: 48000, mean episode reward: 17.015007915422206, agent episode reward: [10.71, 10.71, 10.71, -15.114992084577793], time: 169.424
steps: 1224975, episodes: 49000, mean episode reward: 21.403973347521436, agent episode reward: [12.89, 12.89, 12.89, -17.266026652478562], time: 168.63
steps: 1249975, episodes: 50000, mean episode reward: 22.54557336387594, agent episode reward: [13.14, 13.14, 13.14, -16.874426636124067], time: 170.153
steps: 1274975, episodes: 51000, mean episode reward: 21.757144059628853, agent episode reward: [13.11, 13.11, 13.11, -17.57285594037115], time: 168.126
steps: 1299975, episodes: 52000, mean episode reward: 23.047170429093264, agent episode reward: [13.8, 13.8, 13.8, -18.35282957090674], time: 136.224
steps: 1324975, episodes: 53000, mean episode reward: 18.70113672003213, agent episode reward: [12.02, 12.02, 12.02, -17.358863279967863], time: 117.84
steps: 1349975, episodes: 54000, mean episode reward: 20.034872525568943, agent episode reward: [12.76, 12.76, 12.76, -18.245127474431058], time: 118.431
steps: 1374975, episodes: 55000, mean episode reward: 19.564841299674985, agent episode reward: [12.08, 12.08, 12.08, -16.675158700325017], time: 119.747
steps: 1399975, episodes: 56000, mean episode reward: 24.727899007619047, agent episode reward: [14.78, 14.78, 14.78, -19.61210099238096], time: 118.759
steps: 1424975, episodes: 57000, mean episode reward: 23.403106644791382, agent episode reward: [13.82, 13.82, 13.82, -18.056893355208615], time: 118.962
steps: 1449975, episodes: 58000, mean episode reward: 24.241086007825615, agent episode reward: [14.37, 14.37, 14.37, -18.868913992174384], time: 119.308
steps: 1474975, episodes: 59000, mean episode reward: 24.08477730572837, agent episode reward: [14.37, 14.37, 14.37, -19.025222694271626], time: 117.997
steps: 1499975, episodes: 60000, mean episode reward: 21.507256977413196, agent episode reward: [13.22, 13.22, 13.22, -18.152743022586804], time: 117.652
...Finished total of 60001 episodes.
