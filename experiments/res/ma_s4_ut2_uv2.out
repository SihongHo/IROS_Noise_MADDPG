Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -20.712874207872417, agent episode reward: [-37.36633583123226, 8.326730811679921, 8.326730811679921], time: 51.509
steps: 49975, episodes: 2000, mean episode reward: -27.183276450289625, agent episode reward: [-42.71508855507442, 7.765906052392396, 7.765906052392396], time: 69.76
steps: 74975, episodes: 3000, mean episode reward: -3.991689972328144, agent episode reward: [-22.72712266364469, 9.367716345658271, 9.367716345658271], time: 68.586
steps: 99975, episodes: 4000, mean episode reward: 1.2871498869615485, agent episode reward: [-21.144356641803107, 11.215753264382327, 11.215753264382327], time: 69.726
steps: 124975, episodes: 5000, mean episode reward: 4.235980150616112, agent episode reward: [-18.59395591371801, 11.41496803216706, 11.41496803216706], time: 78.703
steps: 149975, episodes: 6000, mean episode reward: 4.009297131402999, agent episode reward: [-15.257703160543752, 9.633500145973375, 9.633500145973375], time: 79.903
steps: 174975, episodes: 7000, mean episode reward: 3.338398479644244, agent episode reward: [-12.282499727411214, 7.810449103527728, 7.810449103527728], time: 79.566
steps: 199975, episodes: 8000, mean episode reward: 2.73953283140311, agent episode reward: [-12.284807345748685, 7.512170088575897, 7.512170088575897], time: 77.252
steps: 224975, episodes: 9000, mean episode reward: 1.6272659884025216, agent episode reward: [-11.21586017417762, 6.421563081290071, 6.421563081290071], time: 79.614
steps: 249975, episodes: 10000, mean episode reward: 1.7844576794195586, agent episode reward: [-11.240296538720104, 6.512377109069832, 6.512377109069832], time: 79.592
steps: 274975, episodes: 11000, mean episode reward: 1.2484577558142629, agent episode reward: [-11.095702374782755, 6.17208006529851, 6.17208006529851], time: 79.061
steps: 299975, episodes: 12000, mean episode reward: 0.5205280461356068, agent episode reward: [-12.544523875521376, 6.532525960828491, 6.532525960828491], time: 77.294
steps: 324975, episodes: 13000, mean episode reward: 0.8882063897694926, agent episode reward: [-12.307730079651494, 6.597968234710493, 6.597968234710493], time: 79.817
steps: 349975, episodes: 14000, mean episode reward: 0.9137517734196845, agent episode reward: [-13.074694764462725, 6.994223268941205, 6.994223268941205], time: 79.943
steps: 374975, episodes: 15000, mean episode reward: 0.4043889830209989, agent episode reward: [-13.365448668264646, 6.8849188256428215, 6.8849188256428215], time: 78.7
steps: 399975, episodes: 16000, mean episode reward: 0.8341150006557533, agent episode reward: [-13.699277191493875, 7.266696096074813, 7.266696096074813], time: 77.99
steps: 424975, episodes: 17000, mean episode reward: 1.6851258538425127, agent episode reward: [-13.814248638328491, 7.749687246085502, 7.749687246085502], time: 79.464
steps: 449975, episodes: 18000, mean episode reward: 1.25991239490376, agent episode reward: [-13.396255714419116, 7.328084054661438, 7.328084054661438], time: 80.588
steps: 474975, episodes: 19000, mean episode reward: 1.4954253922647494, agent episode reward: [-15.15010290059266, 8.322764146428705, 8.322764146428705], time: 80.297
steps: 499975, episodes: 20000, mean episode reward: 1.1095140494415012, agent episode reward: [-17.892789411528284, 9.501151730484892, 9.501151730484892], time: 78.06
steps: 524975, episodes: 21000, mean episode reward: 2.032698671490696, agent episode reward: [-16.295059656968135, 9.163879164229416, 9.163879164229416], time: 78.868
steps: 549975, episodes: 22000, mean episode reward: 2.1075172600366248, agent episode reward: [-16.24489635774772, 9.176206808892173, 9.176206808892173], time: 79.475
steps: 574975, episodes: 23000, mean episode reward: 1.806894069059805, agent episode reward: [-16.190046916888637, 8.998470492974223, 8.998470492974223], time: 77.089
steps: 599975, episodes: 24000, mean episode reward: 1.7134714880842525, agent episode reward: [-15.285885435883802, 8.499678461984029, 8.499678461984029], time: 79.194
steps: 624975, episodes: 25000, mean episode reward: 1.031204792599969, agent episode reward: [-14.039109332395203, 7.5351570624975865, 7.5351570624975865], time: 80.795
steps: 649975, episodes: 26000, mean episode reward: 2.250754897854398, agent episode reward: [-15.375507444929964, 8.813131171392182, 8.813131171392182], time: 80.136
steps: 674975, episodes: 27000, mean episode reward: 0.8421118871193858, agent episode reward: [-15.106507915162624, 7.974309901141005, 7.974309901141005], time: 78.901
steps: 699975, episodes: 28000, mean episode reward: 1.3888991929917178, agent episode reward: [-17.487555017626644, 9.438227105309181, 9.438227105309181], time: 79.548
steps: 724975, episodes: 29000, mean episode reward: 1.0652160407650022, agent episode reward: [-17.097509727939965, 9.081362884352481, 9.081362884352481], time: 79.6
steps: 749975, episodes: 30000, mean episode reward: 0.16545363604517116, agent episode reward: [-19.87457080821988, 10.020012222132525, 10.020012222132525], time: 80.228
steps: 774975, episodes: 31000, mean episode reward: 0.6323897062479993, agent episode reward: [-18.351549162622476, 9.491969434435239, 9.491969434435239], time: 77.315
steps: 799975, episodes: 32000, mean episode reward: -0.6516999132587875, agent episode reward: [-19.39907156838568, 9.373685827563445, 9.373685827563445], time: 78.522
steps: 824975, episodes: 33000, mean episode reward: 0.6563540367470555, agent episode reward: [-18.195215552092836, 9.425784794419943, 9.425784794419943], time: 81.045
steps: 849975, episodes: 34000, mean episode reward: 0.45124060796935966, agent episode reward: [-19.230427017715616, 9.84083381284249, 9.84083381284249], time: 77.511
steps: 874975, episodes: 35000, mean episode reward: 0.1014694615353001, agent episode reward: [-18.634888872851487, 9.368179167193393, 9.368179167193393], time: 77.445
steps: 899975, episodes: 36000, mean episode reward: 0.19482683129982728, agent episode reward: [-17.086576273733943, 8.640701552516886, 8.640701552516886], time: 78.916
steps: 924975, episodes: 37000, mean episode reward: 0.6860618667419173, agent episode reward: [-17.33900410211837, 9.012532984430145, 9.012532984430145], time: 79.283
steps: 949975, episodes: 38000, mean episode reward: -1.8927883989022392, agent episode reward: [-17.071800087440675, 7.589505844269219, 7.589505844269219], time: 80.815
steps: 974975, episodes: 39000, mean episode reward: -1.2287902552277843, agent episode reward: [-17.957936321173165, 8.36457303297269, 8.36457303297269], time: 80.441
steps: 999975, episodes: 40000, mean episode reward: -1.6725854001051748, agent episode reward: [-18.428890133005734, 8.378152366450278, 8.378152366450278], time: 80.376
steps: 1024975, episodes: 41000, mean episode reward: -2.0996816366826176, agent episode reward: [-16.63661099757106, 7.268464680444222, 7.268464680444222], time: 78.235
steps: 1049975, episodes: 42000, mean episode reward: -2.4783192167542776, agent episode reward: [-15.848915731322846, 6.685298257284285, 6.685298257284285], time: 78.37
steps: 1074975, episodes: 43000, mean episode reward: -2.605058405472921, agent episode reward: [-12.090208805060715, 4.7425751997938965, 4.7425751997938965], time: 77.879
steps: 1099975, episodes: 44000, mean episode reward: -2.1885582890388897, agent episode reward: [-12.234183494278811, 5.022812602619961, 5.022812602619961], time: 78.936
steps: 1124975, episodes: 45000, mean episode reward: -2.7368401773048774, agent episode reward: [-11.566552500310806, 4.414856161502964, 4.414856161502964], time: 79.768
steps: 1149975, episodes: 46000, mean episode reward: -2.4968583528291393, agent episode reward: [-11.728035177486051, 4.615588412328456, 4.615588412328456], time: 77.372
steps: 1174975, episodes: 47000, mean episode reward: -2.2283008735982364, agent episode reward: [-11.089971435165364, 4.430835280783564, 4.430835280783564], time: 75.981
steps: 1199975, episodes: 48000, mean episode reward: -2.5822148074060913, agent episode reward: [-11.826069701918058, 4.621927447255983, 4.621927447255983], time: 77.819
steps: 1224975, episodes: 49000, mean episode reward: -2.442952985168902, agent episode reward: [-10.757245856181322, 4.157146435506209, 4.157146435506209], time: 76.015
steps: 1249975, episodes: 50000, mean episode reward: -2.136327547148128, agent episode reward: [-10.654777051520446, 4.259224752186159, 4.259224752186159], time: 76.545
steps: 1274975, episodes: 51000, mean episode reward: -2.232927083123942, agent episode reward: [-10.882735810980426, 4.324904363928242, 4.324904363928242], time: 76.827
steps: 1299975, episodes: 52000, mean episode reward: -2.828072750070723, agent episode reward: [-11.21369096620945, 4.192809108069363, 4.192809108069363], time: 75.974
steps: 1324975, episodes: 53000, mean episode reward: -2.8664719734814037, agent episode reward: [-10.879919243220218, 4.006723634869406, 4.006723634869406], time: 78.124
steps: 1349975, episodes: 54000, mean episode reward: -1.6571061096799353, agent episode reward: [-11.730805787035736, 5.036849838677902, 5.036849838677902], time: 76.75
steps: 1374975, episodes: 55000, mean episode reward: -2.125843030481211, agent episode reward: [-11.489491458547986, 4.681824214033386, 4.681824214033386], time: 79.571
steps: 1399975, episodes: 56000, mean episode reward: -1.4686939701398127, agent episode reward: [-11.826565952946503, 5.178935991403344, 5.178935991403344], time: 77.171
steps: 1424975, episodes: 57000, mean episode reward: -2.1547064267666074, agent episode reward: [-12.375014124260408, 5.110153848746901, 5.110153848746901], time: 76.719
steps: 1449975, episodes: 58000, mean episode reward: -2.2470183144605245, agent episode reward: [-11.600022008619616, 4.676501847079546, 4.676501847079546], time: 75.942
steps: 1474975, episodes: 59000, mean episode reward: -1.4103940037313307, agent episode reward: [-11.344021965724743, 4.966813980996705, 4.966813980996705], time: 79.873
steps: 1499975, episodes: 60000, mean episode reward: -2.2085728462544223, agent episode reward: [-11.732706899755515, 4.762067026750547, 4.762067026750547], time: 78.432
