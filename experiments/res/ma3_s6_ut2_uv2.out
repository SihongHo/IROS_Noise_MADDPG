Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -23.685395378413936, agent episode reward: [-24.219316194140585, 0.2669604078633247, 0.2669604078633247], time: 87.216
steps: 49975, episodes: 2000, mean episode reward: -26.48091580114782, agent episode reward: [-19.538364876753025, -3.471275462197397, -3.471275462197397], time: 110.215
steps: 74975, episodes: 3000, mean episode reward: -13.937056422056855, agent episode reward: [-13.614114451611465, -0.16147098522269585, -0.16147098522269585], time: 109.232
steps: 99975, episodes: 4000, mean episode reward: 7.877294559277958, agent episode reward: [-12.099985038213884, 9.98863979874592, 9.98863979874592], time: 109.441
steps: 124975, episodes: 5000, mean episode reward: 7.763750551229336, agent episode reward: [-12.695114744387073, 10.229432647808204, 10.229432647808204], time: 109.97
steps: 149975, episodes: 6000, mean episode reward: 7.773477507731628, agent episode reward: [-12.116987662458682, 9.945232585095157, 9.945232585095157], time: 110.114
steps: 174975, episodes: 7000, mean episode reward: 6.963590529970399, agent episode reward: [-11.500597344760886, 9.232093937365644, 9.232093937365644], time: 110.409
steps: 199975, episodes: 8000, mean episode reward: 6.1068888783638435, agent episode reward: [-11.510440279694832, 8.808664579029337, 8.808664579029337], time: 110.148
steps: 224975, episodes: 9000, mean episode reward: 5.3084799389024155, agent episode reward: [-11.410843569350764, 8.359661754126588, 8.359661754126588], time: 111.09
steps: 249975, episodes: 10000, mean episode reward: 5.631257490666121, agent episode reward: [-12.190788702961381, 8.911023096813752, 8.911023096813752], time: 110.057
steps: 274975, episodes: 11000, mean episode reward: 3.8464570952779193, agent episode reward: [-13.150353168606573, 8.498405131942247, 8.498405131942247], time: 110.397
steps: 299975, episodes: 12000, mean episode reward: 8.048354413093074, agent episode reward: [-14.29038045761098, 11.16936743535203, 11.16936743535203], time: 111.835
steps: 324975, episodes: 13000, mean episode reward: 6.081233665653035, agent episode reward: [-12.833841652427832, 9.457537659040433, 9.457537659040433], time: 111.351
steps: 349975, episodes: 14000, mean episode reward: 9.34603754976247, agent episode reward: [-14.479792000259014, 11.912914775010744, 11.912914775010744], time: 111.065
steps: 374975, episodes: 15000, mean episode reward: 9.902863509484416, agent episode reward: [-15.314185271435978, 12.608524390460195, 12.608524390460195], time: 111.702
steps: 399975, episodes: 16000, mean episode reward: 7.08083333032509, agent episode reward: [-15.358356224330834, 11.219594777327963, 11.219594777327963], time: 110.117
steps: 424975, episodes: 17000, mean episode reward: 6.214809010906232, agent episode reward: [-12.959709841234217, 9.587259426070224, 9.587259426070224], time: 110.939
steps: 449975, episodes: 18000, mean episode reward: 7.29650479983876, agent episode reward: [-14.240396135587941, 10.76845046771335, 10.76845046771335], time: 110.757
steps: 474975, episodes: 19000, mean episode reward: 8.623860508714046, agent episode reward: [-15.347417654288924, 11.985639081501485, 11.985639081501485], time: 111.168
steps: 499975, episodes: 20000, mean episode reward: 8.000435623346135, agent episode reward: [-14.410513906868506, 11.205474765107319, 11.205474765107319], time: 110.547
steps: 524975, episodes: 21000, mean episode reward: 9.255717302801536, agent episode reward: [-14.41970089752697, 11.837709100164252, 11.837709100164252], time: 111.011
steps: 549975, episodes: 22000, mean episode reward: 9.108962752272786, agent episode reward: [-14.413864311379841, 11.761413531826314, 11.761413531826314], time: 110.381
steps: 574975, episodes: 23000, mean episode reward: 10.742127238089354, agent episode reward: [-16.315661438282568, 13.528894338185962, 13.528894338185962], time: 110.719
