Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -24.04720340042714, agent episode reward: [-24.20333165283005, 0.07806412620145582, 0.07806412620145582], time: 86.721
steps: 49975, episodes: 2000, mean episode reward: -26.03082088402546, agent episode reward: [-17.5462016300032, -4.242309627011127, -4.242309627011127], time: 109.374
steps: 74975, episodes: 3000, mean episode reward: -1.5606252846262305, agent episode reward: [-20.625754737615704, 9.532564726494737, 9.532564726494737], time: 108.917
steps: 99975, episodes: 4000, mean episode reward: -13.716127106782118, agent episode reward: [-12.458910223627877, -0.6286084415771201, -0.6286084415771201], time: 110.146
steps: 124975, episodes: 5000, mean episode reward: -11.405026768072862, agent episode reward: [-20.84249360870177, 4.718733420314455, 4.718733420314455], time: 109.961
steps: 149975, episodes: 6000, mean episode reward: -10.832740525471872, agent episode reward: [-12.912581779301249, 1.0399206269146868, 1.0399206269146868], time: 110.884
steps: 174975, episodes: 7000, mean episode reward: 3.587849451473874, agent episode reward: [-22.819057214497544, 13.203453332985708, 13.203453332985708], time: 110.541
steps: 199975, episodes: 8000, mean episode reward: 14.130644353980337, agent episode reward: [-27.372594454652745, 20.751619404316543, 20.751619404316543], time: 110.318
steps: 224975, episodes: 9000, mean episode reward: 16.22357716549045, agent episode reward: [-28.02463225369875, 22.1241047095946, 22.1241047095946], time: 110.594
steps: 249975, episodes: 10000, mean episode reward: 15.167291598990898, agent episode reward: [-19.318681096981585, 17.24298634798624, 17.24298634798624], time: 110.156
steps: 274975, episodes: 11000, mean episode reward: 18.48193659621776, agent episode reward: [-22.32281925177378, 20.402377923995772, 20.402377923995772], time: 109.823
steps: 299975, episodes: 12000, mean episode reward: 16.354683568679373, agent episode reward: [-21.968337075575626, 19.161510322127498, 19.161510322127498], time: 111.412
steps: 324975, episodes: 13000, mean episode reward: 16.78596424373299, agent episode reward: [-22.465256842535485, 19.625610543134236, 19.625610543134236], time: 110.56
steps: 349975, episodes: 14000, mean episode reward: 14.657419273761565, agent episode reward: [-22.321789841107748, 18.489604557434657, 18.489604557434657], time: 110.717
steps: 374975, episodes: 15000, mean episode reward: 18.614188708740755, agent episode reward: [-24.299080886080187, 21.456634797410473, 21.456634797410473], time: 109.583
steps: 399975, episodes: 16000, mean episode reward: 18.568053958549356, agent episode reward: [-23.73549042625375, 21.151772192401555, 21.151772192401555], time: 110.329
steps: 424975, episodes: 17000, mean episode reward: 16.632624841501087, agent episode reward: [-23.945154960974207, 20.28888990123765, 20.28888990123765], time: 110.593
steps: 449975, episodes: 18000, mean episode reward: 13.999035779130141, agent episode reward: [-32.492036369994985, 23.245536074562562, 23.245536074562562], time: 109.746
steps: 474975, episodes: 19000, mean episode reward: 18.20021957974316, agent episode reward: [-25.069332010259036, 21.634775795001097, 21.634775795001097], time: 109.994
steps: 499975, episodes: 20000, mean episode reward: 19.540862408656857, agent episode reward: [-23.187695218770696, 21.364278813713774, 21.364278813713774], time: 110.007
steps: 524975, episodes: 21000, mean episode reward: 18.36670899794434, agent episode reward: [-22.3777528604398, 20.37223092919207, 20.37223092919207], time: 109.365
steps: 549975, episodes: 22000, mean episode reward: 20.807357467153373, agent episode reward: [-24.54293505551894, 22.67514626133616, 22.67514626133616], time: 110.831
steps: 574975, episodes: 23000, mean episode reward: 21.387942970860422, agent episode reward: [-24.94396515405422, 23.16595406245732, 23.16595406245732], time: 111.061
