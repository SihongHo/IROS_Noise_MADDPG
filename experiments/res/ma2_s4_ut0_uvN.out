Using good policy maddpg and adv policy maddpg
Uncertainty type is:  None ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -22.347780325770167, agent episode reward: [-33.587518953931415, 5.619869314080622, 5.619869314080622], time: 47.315
steps: 49975, episodes: 2000, mean episode reward: -24.136224365952973, agent episode reward: [-37.40065927492082, 6.632217454483924, 6.632217454483924], time: 66.455
steps: 74975, episodes: 3000, mean episode reward: 5.129911473397101, agent episode reward: [-9.82974541359895, 7.4798284434980244, 7.4798284434980244], time: 68.301
steps: 99975, episodes: 4000, mean episode reward: 3.329727884699918, agent episode reward: [-8.994498270083291, 6.162113077391605, 6.162113077391605], time: 77.089
steps: 124975, episodes: 5000, mean episode reward: 2.608946063983062, agent episode reward: [-9.103758755937731, 5.8563524099603965, 5.8563524099603965], time: 87.132
steps: 149975, episodes: 6000, mean episode reward: 2.127387056935853, agent episode reward: [-9.799042913596974, 5.963214985266413, 5.963214985266413], time: 86.823
steps: 174975, episodes: 7000, mean episode reward: 1.5926426847374848, agent episode reward: [-10.165550044931694, 5.87909636483459, 5.87909636483459], time: 87.47
steps: 199975, episodes: 8000, mean episode reward: 1.5709222566264238, agent episode reward: [-10.012035878572057, 5.791479067599239, 5.791479067599239], time: 87.162
steps: 224975, episodes: 9000, mean episode reward: 1.0179185985566979, agent episode reward: [-10.362449896716619, 5.690184247636658, 5.690184247636658], time: 87.19
steps: 249975, episodes: 10000, mean episode reward: 0.43681354594327226, agent episode reward: [-10.443552620431362, 5.440183083187317, 5.440183083187317], time: 87.103
steps: 274975, episodes: 11000, mean episode reward: 0.877697870821306, agent episode reward: [-10.998251179989483, 5.937974525405394, 5.937974525405394], time: 87.051
steps: 299975, episodes: 12000, mean episode reward: 0.4241126457837196, agent episode reward: [-11.370179280556886, 5.897145963170303, 5.897145963170303], time: 86.975
steps: 324975, episodes: 13000, mean episode reward: 0.4371301649253001, agent episode reward: [-11.148718119049652, 5.792924141987476, 5.792924141987476], time: 87.612
steps: 349975, episodes: 14000, mean episode reward: -0.07605923675184183, agent episode reward: [-10.900648474424363, 5.4122946188362615, 5.4122946188362615], time: 87.136
steps: 374975, episodes: 15000, mean episode reward: -0.44005330988555974, agent episode reward: [-10.784395201632723, 5.172170945873582, 5.172170945873582], time: 86.771
steps: 399975, episodes: 16000, mean episode reward: -0.6294885735494419, agent episode reward: [-10.912467125589341, 5.141489276019949, 5.141489276019949], time: 87.475
steps: 424975, episodes: 17000, mean episode reward: -0.17264588338782577, agent episode reward: [-10.722943588894834, 5.275148852753504, 5.275148852753504], time: 87.543
steps: 449975, episodes: 18000, mean episode reward: -0.09356837351934041, agent episode reward: [-11.458769586664603, 5.682600606572631, 5.682600606572631], time: 87.128
steps: 474975, episodes: 19000, mean episode reward: -0.0013318554299045075, agent episode reward: [-11.50495521826695, 5.751811681418523, 5.751811681418523], time: 87.126
steps: 499975, episodes: 20000, mean episode reward: 0.2187700123241773, agent episode reward: [-12.36037884612089, 6.289574429222534, 6.289574429222534], time: 87.006
steps: 524975, episodes: 21000, mean episode reward: 0.3899483557772918, agent episode reward: [-12.722086540556772, 6.556017448167031, 6.556017448167031], time: 87.364
steps: 549975, episodes: 22000, mean episode reward: 0.27850904207239663, agent episode reward: [-12.559465170762625, 6.418987106417511, 6.418987106417511], time: 87.876
steps: 574975, episodes: 23000, mean episode reward: 0.7828327231092976, agent episode reward: [-12.483138956445599, 6.632985839777448, 6.632985839777448], time: 87.715
steps: 599975, episodes: 24000, mean episode reward: 0.9754174289733443, agent episode reward: [-11.934185725195544, 6.454801577084443, 6.454801577084443], time: 87.317
steps: 624975, episodes: 25000, mean episode reward: 1.0845636480272296, agent episode reward: [-12.352049986466206, 6.7183068172467175, 6.7183068172467175], time: 86.856
steps: 649975, episodes: 26000, mean episode reward: 0.6944783661707239, agent episode reward: [-12.651083850930092, 6.672781108550409, 6.672781108550409], time: 87.65
steps: 674975, episodes: 27000, mean episode reward: 0.6386447199481304, agent episode reward: [-12.436765109821458, 6.5377049148847926, 6.5377049148847926], time: 87.785
steps: 699975, episodes: 28000, mean episode reward: 0.5154981138729404, agent episode reward: [-12.703489645543518, 6.609493879708229, 6.609493879708229], time: 87.365
steps: 724975, episodes: 29000, mean episode reward: 0.5116778780620533, agent episode reward: [-13.292341377478774, 6.902009627770412, 6.902009627770412], time: 87.533
steps: 749975, episodes: 30000, mean episode reward: 0.19681758551200237, agent episode reward: [-13.370043323974315, 6.783430454743159, 6.783430454743159], time: 87.185
steps: 774975, episodes: 31000, mean episode reward: 1.0094345775721303, agent episode reward: [-12.811272461176626, 6.910353519374379, 6.910353519374379], time: 87.435
steps: 799975, episodes: 32000, mean episode reward: 0.9445387197653227, agent episode reward: [-13.648437017188048, 7.296487868476684, 7.296487868476684], time: 87.371
steps: 824975, episodes: 33000, mean episode reward: 1.3758042609644923, agent episode reward: [-12.907972082270224, 7.1418881716173575, 7.1418881716173575], time: 87.623
steps: 849975, episodes: 34000, mean episode reward: 1.0175847766278199, agent episode reward: [-13.250713240395402, 7.13414900851161, 7.13414900851161], time: 87.204
steps: 874975, episodes: 35000, mean episode reward: 1.5280078122383862, agent episode reward: [-13.387097139506972, 7.457552475872679, 7.457552475872679], time: 87.144
steps: 899975, episodes: 36000, mean episode reward: 1.2632125722482146, agent episode reward: [-13.235298237337094, 7.249255404792655, 7.249255404792655], time: 87.176
steps: 924975, episodes: 37000, mean episode reward: 0.9867189013517396, agent episode reward: [-13.654745582090388, 7.320732241721063, 7.320732241721063], time: 87.216
steps: 949975, episodes: 38000, mean episode reward: 1.2156987038971458, agent episode reward: [-13.883626333461114, 7.5496625186791295, 7.5496625186791295], time: 87.238
steps: 974975, episodes: 39000, mean episode reward: 1.4303728539356402, agent episode reward: [-13.577150298122683, 7.503761576029163, 7.503761576029163], time: 87.451
steps: 999975, episodes: 40000, mean episode reward: 0.7414063077598437, agent episode reward: [-14.165327770543819, 7.453367039151831, 7.453367039151831], time: 87.358
steps: 1024975, episodes: 41000, mean episode reward: 1.6266275940200288, agent episode reward: [-13.584484911067428, 7.605556252543728, 7.605556252543728], time: 87.825
steps: 1049975, episodes: 42000, mean episode reward: 1.2842422338086368, agent episode reward: [-13.097945201973765, 7.191093717891201, 7.191093717891201], time: 87.946
steps: 1074975, episodes: 43000, mean episode reward: 1.3006566118861274, agent episode reward: [-13.943247729968439, 7.621952170927283, 7.621952170927283], time: 87.606
steps: 1099975, episodes: 44000, mean episode reward: 1.2068701663323504, agent episode reward: [-14.529072259359634, 7.867971212845992, 7.867971212845992], time: 87.209
steps: 1124975, episodes: 45000, mean episode reward: 1.2499336240962466, agent episode reward: [-13.475681374735705, 7.3628074994159745, 7.3628074994159745], time: 87.092
steps: 1149975, episodes: 46000, mean episode reward: 2.2441062558816363, agent episode reward: [-13.639255926106614, 7.941681090994125, 7.941681090994125], time: 87.397
steps: 1174975, episodes: 47000, mean episode reward: 2.0223843302222737, agent episode reward: [-14.29913664145582, 8.160760485839047, 8.160760485839047], time: 87.4
steps: 1199975, episodes: 48000, mean episode reward: 2.293477782289597, agent episode reward: [-15.166345906256105, 8.72991184427285, 8.72991184427285], time: 87.389
steps: 1224975, episodes: 49000, mean episode reward: 2.583039640690725, agent episode reward: [-13.842586664691694, 8.212813152691211, 8.212813152691211], time: 87.483
steps: 1249975, episodes: 50000, mean episode reward: 1.9201433140189825, agent episode reward: [-14.086779788526906, 8.003461551272945, 8.003461551272945], time: 87.44
steps: 1274975, episodes: 51000, mean episode reward: 2.522516825483182, agent episode reward: [-14.340340957916798, 8.43142889169999, 8.43142889169999], time: 87.15
steps: 1299975, episodes: 52000, mean episode reward: 2.7430849833941138, agent episode reward: [-13.986435065643061, 8.36476002451859, 8.36476002451859], time: 87.4
steps: 1324975, episodes: 53000, mean episode reward: 2.2130343960513517, agent episode reward: [-13.936938979669456, 8.074986687860404, 8.074986687860404], time: 87.709
steps: 1349975, episodes: 54000, mean episode reward: 3.178640010941674, agent episode reward: [-13.832106821915978, 8.505373416428826, 8.505373416428826], time: 87.616
steps: 1374975, episodes: 55000, mean episode reward: 2.4805067118775037, agent episode reward: [-13.535927442488056, 8.00821707718278, 8.00821707718278], time: 87.209
steps: 1399975, episodes: 56000, mean episode reward: 2.789660090741047, agent episode reward: [-13.562916829175672, 8.17628845995836, 8.17628845995836], time: 87.553
steps: 1424975, episodes: 57000, mean episode reward: 2.817374873299408, agent episode reward: [-13.083565309578335, 7.950470091438871, 7.950470091438871], time: 88.004
steps: 1449975, episodes: 58000, mean episode reward: 3.4433328272411607, agent episode reward: [-13.350520977123598, 8.39692690218238, 8.39692690218238], time: 87.86
steps: 1474975, episodes: 59000, mean episode reward: 3.9325137722663084, agent episode reward: [-13.75015718397914, 8.841335478122724, 8.841335478122724], time: 88.112
steps: 1499975, episodes: 60000, mean episode reward: 3.79502435190414, agent episode reward: [-13.561210516095246, 8.678117433999693, 8.678117433999693], time: 88.567
...Finished total of 60001 episodes.
