Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: 0.9972607828262214, agent episode reward: [2.83, 2.83, 2.83, -7.49273921717378], time: 122.394
steps: 49975, episodes: 2000, mean episode reward: 0.2341327003124179, agent episode reward: [3.69, 3.69, 3.69, -10.835867299687582], time: 161.265
steps: 74975, episodes: 3000, mean episode reward: 6.031440153802909, agent episode reward: [4.31, 4.31, 4.31, -6.898559846197092], time: 162.774
steps: 99975, episodes: 4000, mean episode reward: 9.137835290881098, agent episode reward: [5.11, 5.11, 5.11, -6.192164709118902], time: 161.412
steps: 124975, episodes: 5000, mean episode reward: 9.513967531287479, agent episode reward: [4.97, 4.97, 4.97, -5.396032468712521], time: 162.967
steps: 149975, episodes: 6000, mean episode reward: 9.838703862349236, agent episode reward: [5.19, 5.19, 5.19, -5.731296137650765], time: 162.176
steps: 174975, episodes: 7000, mean episode reward: 13.48074960781653, agent episode reward: [6.97, 6.97, 6.97, -7.4292503921834685], time: 163.004
steps: 199975, episodes: 8000, mean episode reward: 26.619068645449758, agent episode reward: [13.48, 13.48, 13.48, -13.820931354550243], time: 162.271
steps: 224975, episodes: 9000, mean episode reward: 59.77466467131731, agent episode reward: [30.39, 30.39, 30.39, -31.39533532868269], time: 161.737
steps: 249975, episodes: 10000, mean episode reward: 48.01382160842967, agent episode reward: [26.92, 26.92, 26.92, -32.74617839157032], time: 162.055
steps: 274975, episodes: 11000, mean episode reward: 27.64235363004992, agent episode reward: [18.55, 18.55, 18.55, -28.007646369950077], time: 162.324
steps: 299975, episodes: 12000, mean episode reward: 28.689993735307436, agent episode reward: [19.41, 19.41, 19.41, -29.540006264692565], time: 162.368
steps: 324975, episodes: 13000, mean episode reward: 21.245837743373446, agent episode reward: [15.73, 15.73, 15.73, -25.944162256626555], time: 161.901
steps: 349975, episodes: 14000, mean episode reward: 14.289773869350261, agent episode reward: [11.52, 11.52, 11.52, -20.27022613064974], time: 163.539
steps: 374975, episodes: 15000, mean episode reward: 12.506544115865232, agent episode reward: [10.14, 10.14, 10.14, -17.91345588413477], time: 162.259
steps: 399975, episodes: 16000, mean episode reward: 11.401903800684172, agent episode reward: [9.61, 9.61, 9.61, -17.428096199315828], time: 162.059
steps: 424975, episodes: 17000, mean episode reward: 15.696181820940893, agent episode reward: [11.46, 11.46, 11.46, -18.68381817905911], time: 163.281
steps: 449975, episodes: 18000, mean episode reward: 19.75509562828387, agent episode reward: [13.05, 13.05, 13.05, -19.394904371716134], time: 162.269
steps: 474975, episodes: 19000, mean episode reward: 17.659108336830524, agent episode reward: [11.85, 11.85, 11.85, -17.890891663169477], time: 161.987
steps: 499975, episodes: 20000, mean episode reward: 13.560028888671036, agent episode reward: [10.05, 10.05, 10.05, -16.589971111328964], time: 161.193
steps: 524975, episodes: 21000, mean episode reward: 19.925025372300173, agent episode reward: [12.8, 12.8, 12.8, -18.474974627699826], time: 163.414
steps: 549975, episodes: 22000, mean episode reward: 21.553395880895312, agent episode reward: [13.66, 13.66, 13.66, -19.426604119104685], time: 161.714
steps: 574975, episodes: 23000, mean episode reward: 22.359943272674027, agent episode reward: [13.65, 13.65, 13.65, -18.590056727325976], time: 163.504
steps: 599975, episodes: 24000, mean episode reward: 25.92542845504084, agent episode reward: [15.83, 15.83, 15.83, -21.564571544959158], time: 163.142
steps: 624975, episodes: 25000, mean episode reward: 26.698406878664375, agent episode reward: [15.66, 15.66, 15.66, -20.281593121335625], time: 162.028
steps: 649975, episodes: 26000, mean episode reward: 29.29838752040016, agent episode reward: [16.63, 16.63, 16.63, -20.59161247959984], time: 162.738
steps: 674975, episodes: 27000, mean episode reward: 25.139748745488678, agent episode reward: [14.75, 14.75, 14.75, -19.110251254511322], time: 162.307
steps: 699975, episodes: 28000, mean episode reward: 25.766778970594846, agent episode reward: [14.7, 14.7, 14.7, -18.333221029405156], time: 162.948
steps: 724975, episodes: 29000, mean episode reward: 26.613178382635073, agent episode reward: [15.63, 15.63, 15.63, -20.276821617364927], time: 164.207
steps: 749975, episodes: 30000, mean episode reward: 29.22065267115295, agent episode reward: [16.42, 16.42, 16.42, -20.03934732884705], time: 161.537
steps: 774975, episodes: 31000, mean episode reward: 25.9532650051978, agent episode reward: [15.32, 15.32, 15.32, -20.0067349948022], time: 162.584
steps: 799975, episodes: 32000, mean episode reward: 29.783703727412327, agent episode reward: [16.84, 16.84, 16.84, -20.736296272587673], time: 161.88
steps: 824975, episodes: 33000, mean episode reward: 26.091704833619644, agent episode reward: [15.0, 15.0, 15.0, -18.908295166380356], time: 163.762
steps: 849975, episodes: 34000, mean episode reward: 25.821394004666676, agent episode reward: [14.4, 14.4, 14.4, -17.37860599533332], time: 162.979
steps: 874975, episodes: 35000, mean episode reward: 26.936689439195142, agent episode reward: [15.1, 15.1, 15.1, -18.36331056080486], time: 163.484
steps: 899975, episodes: 36000, mean episode reward: 28.247899431704067, agent episode reward: [15.92, 15.92, 15.92, -19.51210056829593], time: 164.549
steps: 924975, episodes: 37000, mean episode reward: 29.343889905374184, agent episode reward: [16.22, 16.22, 16.22, -19.316110094625813], time: 162.385
steps: 949975, episodes: 38000, mean episode reward: 24.71445935898044, agent episode reward: [13.88, 13.88, 13.88, -16.92554064101956], time: 161.798
steps: 974975, episodes: 39000, mean episode reward: 26.09059740453551, agent episode reward: [14.45, 14.45, 14.45, -17.25940259546449], time: 161.53
steps: 999975, episodes: 40000, mean episode reward: 25.08656238696633, agent episode reward: [13.83, 13.83, 13.83, -16.403437613033667], time: 162.711
steps: 1024975, episodes: 41000, mean episode reward: 21.563893275234367, agent episode reward: [12.43, 12.43, 12.43, -15.72610672476563], time: 163.154
steps: 1049975, episodes: 42000, mean episode reward: 19.668633920817996, agent episode reward: [11.49, 11.49, 11.49, -14.801366079182007], time: 163.276
steps: 1074975, episodes: 43000, mean episode reward: 21.089137073751036, agent episode reward: [12.22, 12.22, 12.22, -15.570862926248964], time: 162.709
steps: 1099975, episodes: 44000, mean episode reward: 24.587640612726176, agent episode reward: [13.7, 13.7, 13.7, -16.512359387273825], time: 161.612
steps: 1124975, episodes: 45000, mean episode reward: 25.634177458362643, agent episode reward: [14.32, 14.32, 14.32, -17.32582254163736], time: 163.453
steps: 1149975, episodes: 46000, mean episode reward: 25.728546375221807, agent episode reward: [14.03, 14.03, 14.03, -16.36145362477819], time: 160.711
steps: 1174975, episodes: 47000, mean episode reward: 23.19994781658127, agent episode reward: [12.98, 12.98, 12.98, -15.740052183418728], time: 162.341
steps: 1199975, episodes: 48000, mean episode reward: 19.406836831517257, agent episode reward: [11.1, 11.1, 11.1, -13.893163168482744], time: 160.783
steps: 1224975, episodes: 49000, mean episode reward: 17.80196994300837, agent episode reward: [10.24, 10.24, 10.24, -12.918030056991627], time: 162.19
steps: 1249975, episodes: 50000, mean episode reward: 19.623129523073597, agent episode reward: [11.43, 11.43, 11.43, -14.666870476926404], time: 162.554
steps: 1274975, episodes: 51000, mean episode reward: 17.667336776898416, agent episode reward: [10.47, 10.47, 10.47, -13.742663223101584], time: 160.965
steps: 1299975, episodes: 52000, mean episode reward: 15.089227208176137, agent episode reward: [9.19, 9.19, 9.19, -12.480772791823862], time: 158.728
steps: 1324975, episodes: 53000, mean episode reward: 15.13319394518099, agent episode reward: [9.22, 9.22, 9.22, -12.52680605481901], time: 158.484
steps: 1349975, episodes: 54000, mean episode reward: 17.17180568721239, agent episode reward: [10.1, 10.1, 10.1, -13.12819431278761], time: 159.21
steps: 1374975, episodes: 55000, mean episode reward: 14.307483180083539, agent episode reward: [9.03, 9.03, 9.03, -12.782516819916465], time: 159.937
steps: 1399975, episodes: 56000, mean episode reward: 15.869040955585081, agent episode reward: [9.69, 9.69, 9.69, -13.200959044414917], time: 159.017
steps: 1424975, episodes: 57000, mean episode reward: 14.72983367905545, agent episode reward: [8.97, 8.97, 8.97, -12.18016632094455], time: 158.661
steps: 1449975, episodes: 58000, mean episode reward: 16.250467540653887, agent episode reward: [10.16, 10.16, 10.16, -14.229532459346112], time: 159.909
steps: 1474975, episodes: 59000, mean episode reward: 16.66856117323113, agent episode reward: [10.14, 10.14, 10.14, -13.751438826768872], time: 159.053
steps: 1499975, episodes: 60000, mean episode reward: 15.623274790577772, agent episode reward: [9.88, 9.88, 9.88, -14.016725209422228], time: 154.092
...Finished total of 60001 episodes.
