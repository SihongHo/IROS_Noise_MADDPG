Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -23.314486000278464, agent episode reward: [-34.507407643681795, 5.596460821701663, 5.596460821701663], time: 69.548
steps: 49975, episodes: 2000, mean episode reward: -20.878159636861458, agent episode reward: [-23.15319982918264, 1.1375200961605907, 1.1375200961605907], time: 125.35
steps: 74975, episodes: 3000, mean episode reward: -0.9404731002556973, agent episode reward: [-12.64184333751862, 5.8506851186314615, 5.8506851186314615], time: 135.085
steps: 99975, episodes: 4000, mean episode reward: 3.680775264404402, agent episode reward: [-10.731272254003796, 7.2060237592041, 7.2060237592041], time: 135.565
steps: 124975, episodes: 5000, mean episode reward: 3.8922718549450037, agent episode reward: [-9.485044263290849, 6.688658059117926, 6.688658059117926], time: 135.692
steps: 149975, episodes: 6000, mean episode reward: 3.160757410027643, agent episode reward: [-9.379364570678574, 6.270060990353109, 6.270060990353109], time: 135.673
steps: 174975, episodes: 7000, mean episode reward: 2.5137734036203305, agent episode reward: [-9.450342155234456, 5.982057779427393, 5.982057779427393], time: 136.463
steps: 199975, episodes: 8000, mean episode reward: 1.449258085867012, agent episode reward: [-9.942267782061435, 5.6957629339642235, 5.6957629339642235], time: 135.952
steps: 224975, episodes: 9000, mean episode reward: 2.018060270519311, agent episode reward: [-9.596706570069232, 5.807383420294272, 5.807383420294272], time: 136.19
steps: 249975, episodes: 10000, mean episode reward: 1.6821870636938216, agent episode reward: [-9.925896667324666, 5.8040418655092445, 5.8040418655092445], time: 135.167
steps: 274975, episodes: 11000, mean episode reward: 0.32586324890300217, agent episode reward: [-9.959604550155822, 5.1427338995294125, 5.1427338995294125], time: 136.837
steps: 299975, episodes: 12000, mean episode reward: 0.447233491903716, agent episode reward: [-10.278050899457696, 5.362642195680705, 5.362642195680705], time: 136.273
steps: 324975, episodes: 13000, mean episode reward: -0.4322845223093731, agent episode reward: [-10.58686597166863, 5.077290724679628, 5.077290724679628], time: 135.361
steps: 349975, episodes: 14000, mean episode reward: -0.2676997053895969, agent episode reward: [-11.091952314427212, 5.412126304518807, 5.412126304518807], time: 136.255
steps: 374975, episodes: 15000, mean episode reward: -0.32841702923554134, agent episode reward: [-11.553536506010609, 5.612559738387533, 5.612559738387533], time: 135.474
steps: 399975, episodes: 16000, mean episode reward: 0.04701465982180514, agent episode reward: [-12.056098393054713, 6.051556526438259, 6.051556526438259], time: 135.534
steps: 424975, episodes: 17000, mean episode reward: -0.19706361369109818, agent episode reward: [-11.59175413039356, 5.697345258351231, 5.697345258351231], time: 135.987
steps: 449975, episodes: 18000, mean episode reward: -0.5529908437845346, agent episode reward: [-11.618415321548222, 5.532712238881844, 5.532712238881844], time: 135.543
steps: 474975, episodes: 19000, mean episode reward: -0.007682796526563123, agent episode reward: [-11.58999704146433, 5.791157122468883, 5.791157122468883], time: 136.254
steps: 499975, episodes: 20000, mean episode reward: -1.0281505319806195, agent episode reward: [-11.414821187170489, 5.193335327594934, 5.193335327594934], time: 135.899
steps: 524975, episodes: 21000, mean episode reward: -1.2533654205963438, agent episode reward: [-11.805116214379225, 5.275875396891441, 5.275875396891441], time: 136.148
steps: 549975, episodes: 22000, mean episode reward: -0.743470645529831, agent episode reward: [-12.098841204448117, 5.677685279459144, 5.677685279459144], time: 136.45
steps: 574975, episodes: 23000, mean episode reward: -0.5392138805059228, agent episode reward: [-11.50081285393865, 5.480799486716363, 5.480799486716363], time: 136.149
steps: 599975, episodes: 24000, mean episode reward: -1.1322602016956396, agent episode reward: [-11.691258332054218, 5.27949906517929, 5.27949906517929], time: 137.04
steps: 624975, episodes: 25000, mean episode reward: -0.617719931315612, agent episode reward: [-12.05512799615769, 5.718704032421039, 5.718704032421039], time: 136.035
steps: 649975, episodes: 26000, mean episode reward: -0.3024830040228552, agent episode reward: [-12.460479689905076, 6.0789983429411105, 6.0789983429411105], time: 134.832
steps: 674975, episodes: 27000, mean episode reward: -1.2360481486041985, agent episode reward: [-11.916168897722194, 5.340060374558998, 5.340060374558998], time: 135.385
steps: 699975, episodes: 28000, mean episode reward: -1.0425149127986983, agent episode reward: [-12.087398350778097, 5.522441718989699, 5.522441718989699], time: 135.596
steps: 724975, episodes: 29000, mean episode reward: -1.979104241617797, agent episode reward: [-11.783308105880684, 4.902101932131443, 4.902101932131443], time: 136.046
steps: 749975, episodes: 30000, mean episode reward: -0.7437287571901021, agent episode reward: [-11.26827485923528, 5.262273051022589, 5.262273051022589], time: 136.3
steps: 774975, episodes: 31000, mean episode reward: -1.0908958033075826, agent episode reward: [-12.374034141284163, 5.64156916898829, 5.64156916898829], time: 135.274
steps: 799975, episodes: 32000, mean episode reward: -1.0708809132301336, agent episode reward: [-11.944123035272249, 5.436621061021058, 5.436621061021058], time: 135.268
steps: 824975, episodes: 33000, mean episode reward: -2.383866510217448, agent episode reward: [-11.732531396513641, 4.674332443148096, 4.674332443148096], time: 135.695
steps: 849975, episodes: 34000, mean episode reward: -1.8176328628366756, agent episode reward: [-12.288752653446156, 5.235559895304741, 5.235559895304741], time: 135.934
steps: 874975, episodes: 35000, mean episode reward: -2.247891709698849, agent episode reward: [-12.445290087682945, 5.0986991889920485, 5.0986991889920485], time: 135.857
steps: 899975, episodes: 36000, mean episode reward: -1.7290718157274605, agent episode reward: [-12.36651563980364, 5.31872191203809, 5.31872191203809], time: 136.062
steps: 924975, episodes: 37000, mean episode reward: -2.516897650366908, agent episode reward: [-12.483958649830669, 4.983530499731881, 4.983530499731881], time: 136.228
steps: 949975, episodes: 38000, mean episode reward: -1.3858488508755797, agent episode reward: [-12.74600186947853, 5.6800765093014745, 5.6800765093014745], time: 137.023
steps: 974975, episodes: 39000, mean episode reward: -2.441888554422157, agent episode reward: [-12.135501209504678, 4.846806327541261, 4.846806327541261], time: 135.754
steps: 999975, episodes: 40000, mean episode reward: -2.3606996993597957, agent episode reward: [-12.624932484492343, 5.132116392566274, 5.132116392566274], time: 135.692
steps: 1024975, episodes: 41000, mean episode reward: -2.627487400587127, agent episode reward: [-12.77033589035933, 5.071424244886102, 5.071424244886102], time: 135.275
steps: 1049975, episodes: 42000, mean episode reward: -2.163521524470999, agent episode reward: [-12.356015092029095, 5.096246783779049, 5.096246783779049], time: 135.662
steps: 1074975, episodes: 43000, mean episode reward: -2.9664215145044674, agent episode reward: [-13.419027238081089, 5.226302861788311, 5.226302861788311], time: 135.739
steps: 1099975, episodes: 44000, mean episode reward: -3.663272043102665, agent episode reward: [-12.564357422670284, 4.45054268978381, 4.45054268978381], time: 135.566
steps: 1124975, episodes: 45000, mean episode reward: -4.10895538837974, agent episode reward: [-13.214665099595768, 4.552854855608013, 4.552854855608013], time: 135.584
steps: 1149975, episodes: 46000, mean episode reward: -4.884888936761552, agent episode reward: [-13.469430860673118, 4.292270961955783, 4.292270961955783], time: 135.833
steps: 1174975, episodes: 47000, mean episode reward: -3.9492955727772494, agent episode reward: [-13.447372465846495, 4.749038446534623, 4.749038446534623], time: 136.064
steps: 1199975, episodes: 48000, mean episode reward: -4.115823283472639, agent episode reward: [-12.927374453936494, 4.405775585231928, 4.405775585231928], time: 136.694
steps: 1224975, episodes: 49000, mean episode reward: -3.5495535435896746, agent episode reward: [-13.350018755742799, 4.90023260607656, 4.90023260607656], time: 136.175
steps: 1249975, episodes: 50000, mean episode reward: -3.346095506631305, agent episode reward: [-12.48733635493134, 4.570620424150018, 4.570620424150018], time: 135.93
steps: 1274975, episodes: 51000, mean episode reward: -3.8479208329803187, agent episode reward: [-13.007637339165763, 4.5798582530927225, 4.5798582530927225], time: 136.713
steps: 1299975, episodes: 52000, mean episode reward: -3.5451797923586237, agent episode reward: [-12.254597977537255, 4.3547090925893155, 4.3547090925893155], time: 135.904
steps: 1324975, episodes: 53000, mean episode reward: -4.274564152421177, agent episode reward: [-13.358735461072222, 4.542085654325522, 4.542085654325522], time: 135.742
steps: 1349975, episodes: 54000, mean episode reward: -4.5281209576753305, agent episode reward: [-12.17953183256465, 3.82570543744466, 3.82570543744466], time: 135.929
steps: 1374975, episodes: 55000, mean episode reward: -5.6060302230382915, agent episode reward: [-12.983605848817458, 3.6887878128895832, 3.6887878128895832], time: 136.666
steps: 1399975, episodes: 56000, mean episode reward: -5.8206473632507585, agent episode reward: [-12.293364949295391, 3.2363587930223163, 3.2363587930223163], time: 135.554
steps: 1424975, episodes: 57000, mean episode reward: -6.991454215096223, agent episode reward: [-12.008411942926557, 2.508478863915168, 2.508478863915168], time: 136.046
steps: 1449975, episodes: 58000, mean episode reward: -6.619377180877674, agent episode reward: [-12.254118090630195, 2.81737045487626, 2.81737045487626], time: 136.537
steps: 1474975, episodes: 59000, mean episode reward: -7.097027204563046, agent episode reward: [-11.865009774028344, 2.383991284732649, 2.383991284732649], time: 136.939
steps: 1499975, episodes: 60000, mean episode reward: -7.192405908622161, agent episode reward: [-12.213326130364221, 2.510460110871031, 2.510460110871031], time: 132.125
...Finished total of 60001 episodes.
