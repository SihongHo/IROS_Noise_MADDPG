Using good policy maddpg and adv policy maddpg
Uncertainty type is:  None ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -2.8653666614887285, agent episode reward: [2.49, 2.49, 2.49, -10.335366661488727], time: 139.287
steps: 49975, episodes: 2000, mean episode reward: 0.2691374652615068, agent episode reward: [3.73, 3.73, 3.73, -10.920862534738491], time: 170.304
steps: 74975, episodes: 3000, mean episode reward: 10.963605360013231, agent episode reward: [5.6, 5.6, 5.6, -5.836394639986769], time: 169.254
steps: 99975, episodes: 4000, mean episode reward: 11.40637012214796, agent episode reward: [5.91, 5.91, 5.91, -6.323629877852041], time: 169.465
steps: 124975, episodes: 5000, mean episode reward: 16.025636368117436, agent episode reward: [8.39, 8.39, 8.39, -9.144363631882563], time: 168.499
steps: 149975, episodes: 6000, mean episode reward: 41.40548030518307, agent episode reward: [21.22, 21.22, 21.22, -22.254519694816935], time: 169.077
steps: 174975, episodes: 7000, mean episode reward: 70.30007711800016, agent episode reward: [36.38, 36.38, 36.38, -38.83992288199985], time: 169.504
steps: 199975, episodes: 8000, mean episode reward: 31.498771329957076, agent episode reward: [18.57, 18.57, 18.57, -24.211228670042924], time: 169.628
steps: 224975, episodes: 9000, mean episode reward: 13.142098863624021, agent episode reward: [9.63, 9.63, 9.63, -15.747901136375976], time: 169.901
steps: 249975, episodes: 10000, mean episode reward: 13.764991203967137, agent episode reward: [9.04, 9.04, 9.04, -13.355008796032866], time: 169.251
steps: 274975, episodes: 11000, mean episode reward: 18.033903569324792, agent episode reward: [10.46, 10.46, 10.46, -13.34609643067521], time: 169.285
steps: 299975, episodes: 12000, mean episode reward: 18.042326302999932, agent episode reward: [10.54, 10.54, 10.54, -13.577673697000067], time: 169.985
steps: 324975, episodes: 13000, mean episode reward: 19.538515932996262, agent episode reward: [11.49, 11.49, 11.49, -14.931484067003739], time: 169.213
steps: 349975, episodes: 14000, mean episode reward: 23.801252376313364, agent episode reward: [13.47, 13.47, 13.47, -16.608747623686636], time: 168.962
steps: 374975, episodes: 15000, mean episode reward: 27.18535938875293, agent episode reward: [15.47, 15.47, 15.47, -19.224640611247075], time: 169.513
steps: 399975, episodes: 16000, mean episode reward: 37.34830466548361, agent episode reward: [20.23, 20.23, 20.23, -23.341695334516395], time: 169.15
steps: 424975, episodes: 17000, mean episode reward: 44.92325967307168, agent episode reward: [24.02, 24.02, 24.02, -27.136740326928315], time: 168.802
steps: 449975, episodes: 18000, mean episode reward: 46.308459298057834, agent episode reward: [24.84, 24.84, 24.84, -28.211540701942173], time: 168.721
steps: 474975, episodes: 19000, mean episode reward: 40.08459423473712, agent episode reward: [21.81, 21.81, 21.81, -25.345405765262885], time: 168.878
steps: 499975, episodes: 20000, mean episode reward: 28.51296737738649, agent episode reward: [17.24, 17.24, 17.24, -23.207032622613507], time: 170.243
steps: 524975, episodes: 21000, mean episode reward: 15.087129250531701, agent episode reward: [11.2, 11.2, 11.2, -18.5128707494683], time: 168.403
steps: 549975, episodes: 22000, mean episode reward: 11.592663136857364, agent episode reward: [9.66, 9.66, 9.66, -17.387336863142636], time: 168.706
steps: 574975, episodes: 23000, mean episode reward: 11.718383382712899, agent episode reward: [8.97, 8.97, 8.97, -15.191616617287101], time: 170.093
steps: 599975, episodes: 24000, mean episode reward: 12.753863218594862, agent episode reward: [9.77, 9.77, 9.77, -16.55613678140514], time: 169.368
steps: 624975, episodes: 25000, mean episode reward: 14.428936873546698, agent episode reward: [9.99, 9.99, 9.99, -15.541063126453302], time: 169.087
steps: 649975, episodes: 26000, mean episode reward: 13.896695531740091, agent episode reward: [9.52, 9.52, 9.52, -14.663304468259911], time: 168.835
steps: 674975, episodes: 27000, mean episode reward: 12.761655839915722, agent episode reward: [9.29, 9.29, 9.29, -15.108344160084277], time: 168.525
steps: 699975, episodes: 28000, mean episode reward: 12.646368122396991, agent episode reward: [9.31, 9.31, 9.31, -15.283631877603009], time: 170.082
steps: 724975, episodes: 29000, mean episode reward: 16.33091388517956, agent episode reward: [10.89, 10.89, 10.89, -16.339086114820443], time: 168.577
steps: 749975, episodes: 30000, mean episode reward: 16.78314780725746, agent episode reward: [11.09, 11.09, 11.09, -16.48685219274254], time: 168.899
steps: 774975, episodes: 31000, mean episode reward: 17.383193052262122, agent episode reward: [11.28, 11.28, 11.28, -16.456806947737878], time: 169.825
steps: 799975, episodes: 32000, mean episode reward: 17.034591107186497, agent episode reward: [11.1, 11.1, 11.1, -16.2654088928135], time: 168.706
steps: 824975, episodes: 33000, mean episode reward: 20.39583535523109, agent episode reward: [12.45, 12.45, 12.45, -16.954164644768905], time: 169.658
steps: 849975, episodes: 34000, mean episode reward: 14.839355020231949, agent episode reward: [10.1, 10.1, 10.1, -15.460644979768052], time: 169.213
steps: 874975, episodes: 35000, mean episode reward: 19.92746100045606, agent episode reward: [12.4, 12.4, 12.4, -17.27253899954394], time: 168.383
steps: 899975, episodes: 36000, mean episode reward: 20.025864361385484, agent episode reward: [12.52, 12.52, 12.52, -17.534135638614515], time: 169.597
steps: 924975, episodes: 37000, mean episode reward: 18.493281237846567, agent episode reward: [11.65, 11.65, 11.65, -16.456718762153432], time: 168.603
steps: 949975, episodes: 38000, mean episode reward: 20.519505813086983, agent episode reward: [12.47, 12.47, 12.47, -16.890494186913017], time: 168.265
steps: 974975, episodes: 39000, mean episode reward: 20.384723093615314, agent episode reward: [12.46, 12.46, 12.46, -16.995276906384685], time: 169.396
steps: 999975, episodes: 40000, mean episode reward: 17.461667634766048, agent episode reward: [11.41, 11.41, 11.41, -16.768332365233952], time: 168.249
steps: 1024975, episodes: 41000, mean episode reward: 16.318631891291723, agent episode reward: [10.86, 10.86, 10.86, -16.261368108708275], time: 168.872
steps: 1049975, episodes: 42000, mean episode reward: 13.656037674944493, agent episode reward: [9.49, 9.49, 9.49, -14.813962325055504], time: 168.715
steps: 1074975, episodes: 43000, mean episode reward: 13.770446777777908, agent episode reward: [9.81, 9.81, 9.81, -15.659553222222094], time: 169.111
steps: 1099975, episodes: 44000, mean episode reward: 12.281761180133053, agent episode reward: [8.84, 8.84, 8.84, -14.238238819866949], time: 169.324
steps: 1124975, episodes: 45000, mean episode reward: 10.924416610447269, agent episode reward: [8.16, 8.16, 8.16, -13.555583389552732], time: 168.491
steps: 1149975, episodes: 46000, mean episode reward: 10.962479636432644, agent episode reward: [7.83, 7.83, 7.83, -12.527520363567357], time: 169.325
steps: 1174975, episodes: 47000, mean episode reward: 11.412547546770895, agent episode reward: [8.07, 8.07, 8.07, -12.797452453229106], time: 170.096
steps: 1199975, episodes: 48000, mean episode reward: 9.854842961533281, agent episode reward: [7.23, 7.23, 7.23, -11.83515703846672], time: 169.639
steps: 1224975, episodes: 49000, mean episode reward: 11.586504838230054, agent episode reward: [7.99, 7.99, 7.99, -12.383495161769947], time: 169.376
steps: 1249975, episodes: 50000, mean episode reward: 12.629758110865614, agent episode reward: [8.55, 8.55, 8.55, -13.020241889134384], time: 170.477
steps: 1274975, episodes: 51000, mean episode reward: 12.854126781534186, agent episode reward: [8.53, 8.53, 8.53, -12.735873218465814], time: 168.98
steps: 1299975, episodes: 52000, mean episode reward: 11.55819020375658, agent episode reward: [7.94, 7.94, 7.94, -12.26180979624342], time: 139.629
steps: 1324975, episodes: 53000, mean episode reward: 9.714760466780291, agent episode reward: [6.99, 6.99, 6.99, -11.255239533219708], time: 118.451
steps: 1349975, episodes: 54000, mean episode reward: 11.2747405828886, agent episode reward: [7.65, 7.65, 7.65, -11.675259417111402], time: 118.336
steps: 1374975, episodes: 55000, mean episode reward: 12.109862166341056, agent episode reward: [7.96, 7.96, 7.96, -11.770137833658945], time: 119.546
steps: 1399975, episodes: 56000, mean episode reward: 12.984508699218445, agent episode reward: [8.63, 8.63, 8.63, -12.905491300781556], time: 117.819
steps: 1424975, episodes: 57000, mean episode reward: 11.689244589534082, agent episode reward: [7.98, 7.98, 7.98, -12.250755410465919], time: 118.654
steps: 1449975, episodes: 58000, mean episode reward: 12.50301767541419, agent episode reward: [8.25, 8.25, 8.25, -12.24698232458581], time: 118.043
steps: 1474975, episodes: 59000, mean episode reward: 13.121114175120491, agent episode reward: [8.72, 8.72, 8.72, -13.038885824879507], time: 117.964
steps: 1499975, episodes: 60000, mean episode reward: 14.358427851363082, agent episode reward: [9.06, 9.06, 9.06, -12.821572148636918], time: 118.473
...Finished total of 60001 episodes.
