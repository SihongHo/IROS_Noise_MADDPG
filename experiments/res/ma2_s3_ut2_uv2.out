Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -26.010386420139046, agent episode reward: [0.26866920031760333, -26.279055620456653], time: 43.29
steps: 49975, episodes: 2000, mean episode reward: -22.511319967752115, agent episode reward: [-4.330697303198101, -18.18062266455401], time: 57.607
steps: 74975, episodes: 3000, mean episode reward: -13.462690916669093, agent episode reward: [-5.482698925239923, -7.979991991429171], time: 49.787
steps: 99975, episodes: 4000, mean episode reward: -11.20665426593326, agent episode reward: [-3.665613074475597, -7.541041191457663], time: 49.585
steps: 124975, episodes: 5000, mean episode reward: -11.096287523631037, agent episode reward: [-3.4671869026159365, -7.629100621015103], time: 51.171
steps: 149975, episodes: 6000, mean episode reward: -10.307804290701137, agent episode reward: [-2.7490029767738506, -7.558801313927287], time: 50.674
steps: 174975, episodes: 7000, mean episode reward: -10.073003992744054, agent episode reward: [-2.5793487697681257, -7.493655222975927], time: 57.134
steps: 199975, episodes: 8000, mean episode reward: -9.877434034662063, agent episode reward: [-2.3394479524684053, -7.537986082193656], time: 63.041
steps: 224975, episodes: 9000, mean episode reward: -10.209931377146038, agent episode reward: [-2.3005140665571138, -7.909417310588923], time: 62.691
steps: 249975, episodes: 10000, mean episode reward: -10.003574119089581, agent episode reward: [-2.4183191175598058, -7.5852550015297755], time: 63.068
steps: 274975, episodes: 11000, mean episode reward: -9.917297070300133, agent episode reward: [-2.626220974171053, -7.29107609612908], time: 64.245
steps: 299975, episodes: 12000, mean episode reward: -10.234162786032316, agent episode reward: [-2.5999667465392333, -7.634196039493085], time: 63.668
steps: 324975, episodes: 13000, mean episode reward: -9.800774571564805, agent episode reward: [-2.0267064199288094, -7.774068151635995], time: 63.231
steps: 349975, episodes: 14000, mean episode reward: -9.954447199836416, agent episode reward: [-2.662945114419093, -7.2915020854173225], time: 63.894
steps: 374975, episodes: 15000, mean episode reward: -10.098497320081602, agent episode reward: [-2.643270538958026, -7.455226781123576], time: 63.538
steps: 399975, episodes: 16000, mean episode reward: -9.802271513242454, agent episode reward: [-2.221489532708253, -7.580781980534201], time: 63.82
steps: 424975, episodes: 17000, mean episode reward: -9.846719984031115, agent episode reward: [-2.2099422664500894, -7.636777717581026], time: 63.414
steps: 449975, episodes: 18000, mean episode reward: -9.902524770059648, agent episode reward: [-2.5011900928155195, -7.4013346772441295], time: 64.036
steps: 474975, episodes: 19000, mean episode reward: -10.003350833368735, agent episode reward: [-2.4642149413401673, -7.539135892028567], time: 63.732
steps: 499975, episodes: 20000, mean episode reward: -10.181223917683626, agent episode reward: [-1.9948995778571774, -8.186324339826447], time: 64.472
steps: 524975, episodes: 21000, mean episode reward: -10.331626790426844, agent episode reward: [-2.384257156203881, -7.947369634222963], time: 63.385
steps: 549975, episodes: 22000, mean episode reward: -10.255319770316907, agent episode reward: [-2.419257310298537, -7.836062460018371], time: 64.254
steps: 574975, episodes: 23000, mean episode reward: -10.183625073857588, agent episode reward: [-2.5019165657109026, -7.681708508146687], time: 63.454
steps: 599975, episodes: 24000, mean episode reward: -10.308989494291637, agent episode reward: [-2.6096405100578486, -7.699348984233791], time: 63.096
steps: 624975, episodes: 25000, mean episode reward: -9.670376130771672, agent episode reward: [-2.1888792783731508, -7.481496852398522], time: 64.452
steps: 649975, episodes: 26000, mean episode reward: -10.115420742263545, agent episode reward: [-2.2708484417940045, -7.84457230046954], time: 63.997
steps: 674975, episodes: 27000, mean episode reward: -10.008155479968783, agent episode reward: [-2.0744549878507015, -7.933700492118082], time: 65.504
steps: 699975, episodes: 28000, mean episode reward: -10.045636324315215, agent episode reward: [-2.3243458516212354, -7.721290472693979], time: 67.443
steps: 724975, episodes: 29000, mean episode reward: -9.990477844304099, agent episode reward: [-2.3004287725246315, -7.690049071779466], time: 67.613
steps: 749975, episodes: 30000, mean episode reward: -10.00528457787213, agent episode reward: [-2.409495348278136, -7.595789229593994], time: 68.48
steps: 774975, episodes: 31000, mean episode reward: -10.399502616448864, agent episode reward: [-2.6774906551571083, -7.722011961291753], time: 68.562
steps: 799975, episodes: 32000, mean episode reward: -10.029918891321435, agent episode reward: [-2.0580866291674678, -7.971832262153965], time: 68.208
steps: 824975, episodes: 33000, mean episode reward: -10.072610846595747, agent episode reward: [-2.5676513905798624, -7.504959456015886], time: 68.143
steps: 849975, episodes: 34000, mean episode reward: -9.879012554415361, agent episode reward: [-2.435959832127639, -7.443052722287723], time: 68.066
steps: 874975, episodes: 35000, mean episode reward: -9.951944469713748, agent episode reward: [-2.2799525563307155, -7.671991913383032], time: 68.221
steps: 899975, episodes: 36000, mean episode reward: -9.520057557843508, agent episode reward: [-1.6808223202302923, -7.839235237613216], time: 68.715
steps: 924975, episodes: 37000, mean episode reward: -10.203647580474398, agent episode reward: [-2.083372971956963, -8.120274608517434], time: 68.464
steps: 949975, episodes: 38000, mean episode reward: -10.28706516589796, agent episode reward: [-2.0468412310980555, -8.240223934799905], time: 68.169
steps: 974975, episodes: 39000, mean episode reward: -9.926903241341698, agent episode reward: [-1.817196839383497, -8.109706401958197], time: 67.967
steps: 999975, episodes: 40000, mean episode reward: -9.917511592139432, agent episode reward: [-1.5750484127770128, -8.342463179362417], time: 68.691
steps: 1024975, episodes: 41000, mean episode reward: -9.936069589155668, agent episode reward: [-1.4719864611116023, -8.464083128044065], time: 68.34
steps: 1049975, episodes: 42000, mean episode reward: -9.318791097085155, agent episode reward: [-0.9680053415151522, -8.350785755570003], time: 68.139
steps: 1074975, episodes: 43000, mean episode reward: -9.383943647599173, agent episode reward: [-1.0545328253125685, -8.329410822286603], time: 68.258
steps: 1099975, episodes: 44000, mean episode reward: -9.68688163502945, agent episode reward: [-0.8064525709200969, -8.880429064109356], time: 68.815
steps: 1124975, episodes: 45000, mean episode reward: -9.96548949751302, agent episode reward: [-0.9369846233830182, -9.028504874130004], time: 69.407
steps: 1149975, episodes: 46000, mean episode reward: -9.683980631900353, agent episode reward: [-0.9607490490684404, -8.723231582831914], time: 68.418
steps: 1174975, episodes: 47000, mean episode reward: -9.851777652006458, agent episode reward: [-0.870489888483037, -8.981287763523422], time: 68.634
steps: 1199975, episodes: 48000, mean episode reward: -10.069655758339495, agent episode reward: [-1.705080426181259, -8.364575332158234], time: 68.883
steps: 1224975, episodes: 49000, mean episode reward: -10.207966490950483, agent episode reward: [-2.4711477491630367, -7.736818741787447], time: 68.061
steps: 1249975, episodes: 50000, mean episode reward: -10.140140059686393, agent episode reward: [-2.2130859399157443, -7.927054119770649], time: 68.61
steps: 1274975, episodes: 51000, mean episode reward: -10.23124814018227, agent episode reward: [-2.2111644523736387, -8.020083687808633], time: 68.437
steps: 1299975, episodes: 52000, mean episode reward: -10.389307276108797, agent episode reward: [-2.285929994387556, -8.103377281721238], time: 68.143
steps: 1324975, episodes: 53000, mean episode reward: -10.208552742100645, agent episode reward: [-2.3025256727501904, -7.906027069350454], time: 68.2
steps: 1349975, episodes: 54000, mean episode reward: -9.957152991601586, agent episode reward: [-2.072135623788172, -7.885017367813414], time: 68.446
steps: 1374975, episodes: 55000, mean episode reward: -9.810588866167665, agent episode reward: [-1.3917081286591375, -8.418880737508527], time: 68.615
steps: 1399975, episodes: 56000, mean episode reward: -10.161785145119945, agent episode reward: [-1.182475807464898, -8.979309337655046], time: 69.007
steps: 1424975, episodes: 57000, mean episode reward: -9.85879464849793, agent episode reward: [-0.7321113347837774, -9.126683313714155], time: 68.342
steps: 1449975, episodes: 58000, mean episode reward: -9.952847614250631, agent episode reward: [-1.2477829272154608, -8.70506468703517], time: 68.655
steps: 1474975, episodes: 59000, mean episode reward: -9.918692978194116, agent episode reward: [-0.963671406676242, -8.955021571517875], time: 68.442
steps: 1499975, episodes: 60000, mean episode reward: -10.64989461810431, agent episode reward: [-1.686982895823402, -8.962911722280907], time: 63.676
...Finished total of 60001 episodes.
