Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -3.2788718595712387, agent episode reward: [2.2, 2.2, 2.2, -9.87887185957124], time: 102.611
steps: 49975, episodes: 2000, mean episode reward: -5.797614455835206, agent episode reward: [3.96, 3.96, 3.96, -17.677614455835204], time: 148.464
steps: 74975, episodes: 3000, mean episode reward: 8.414391696803358, agent episode reward: [4.8, 4.8, 4.8, -5.985608303196642], time: 152.065
steps: 99975, episodes: 4000, mean episode reward: 8.733341513064953, agent episode reward: [4.6, 4.6, 4.6, -5.066658486935046], time: 152.714
steps: 124975, episodes: 5000, mean episode reward: 10.466792294927446, agent episode reward: [5.42, 5.42, 5.42, -5.793207705072555], time: 153.736
steps: 149975, episodes: 6000, mean episode reward: 10.430145733775731, agent episode reward: [5.45, 5.45, 5.45, -5.919854266224268], time: 152.171
steps: 174975, episodes: 7000, mean episode reward: 14.184110485448084, agent episode reward: [7.32, 7.32, 7.32, -7.775889514551915], time: 152.693
steps: 199975, episodes: 8000, mean episode reward: 27.703772880091577, agent episode reward: [14.07, 14.07, 14.07, -14.506227119908424], time: 153.015
steps: 224975, episodes: 9000, mean episode reward: 35.30551341820157, agent episode reward: [18.04, 18.04, 18.04, -18.814486581798427], time: 153.409
steps: 249975, episodes: 10000, mean episode reward: 27.633452882982162, agent episode reward: [14.62, 14.62, 14.62, -16.226547117017837], time: 153.373
steps: 274975, episodes: 11000, mean episode reward: 21.343798614266984, agent episode reward: [12.54, 12.54, 12.54, -16.276201385733017], time: 153.71
steps: 299975, episodes: 12000, mean episode reward: 14.569493345871553, agent episode reward: [9.75, 9.75, 9.75, -14.680506654128447], time: 152.35
steps: 324975, episodes: 13000, mean episode reward: 12.43954213132356, agent episode reward: [8.67, 8.67, 8.67, -13.57045786867644], time: 152.538
steps: 349975, episodes: 14000, mean episode reward: 12.466736027827471, agent episode reward: [7.81, 7.81, 7.81, -10.963263972172529], time: 152.94
steps: 374975, episodes: 15000, mean episode reward: 16.440003939388756, agent episode reward: [9.72, 9.72, 9.72, -12.719996060611244], time: 153.407
steps: 399975, episodes: 16000, mean episode reward: 15.703404278147108, agent episode reward: [8.99, 8.99, 8.99, -11.266595721852894], time: 153.181
steps: 424975, episodes: 17000, mean episode reward: 16.6902157278881, agent episode reward: [9.41, 9.41, 9.41, -11.5397842721119], time: 157.102
steps: 449975, episodes: 18000, mean episode reward: 13.384200916849922, agent episode reward: [8.29, 8.29, 8.29, -11.485799083150077], time: 161.694
steps: 474975, episodes: 19000, mean episode reward: 13.55081918493906, agent episode reward: [8.54, 8.54, 8.54, -12.069180815060939], time: 162.209
steps: 499975, episodes: 20000, mean episode reward: 15.521337383923342, agent episode reward: [9.95, 9.95, 9.95, -14.328662616076658], time: 162.173
steps: 524975, episodes: 21000, mean episode reward: 13.708498475763355, agent episode reward: [8.73, 8.73, 8.73, -12.481501524236647], time: 162.101
steps: 549975, episodes: 22000, mean episode reward: 16.61083005933361, agent episode reward: [10.08, 10.08, 10.08, -13.62916994066639], time: 162.395
steps: 574975, episodes: 23000, mean episode reward: 21.304677538836337, agent episode reward: [12.17, 12.17, 12.17, -15.205322461163664], time: 163.193
steps: 599975, episodes: 24000, mean episode reward: 18.94494743573403, agent episode reward: [11.36, 11.36, 11.36, -15.13505256426597], time: 163.256
steps: 624975, episodes: 25000, mean episode reward: 18.05620617910187, agent episode reward: [10.83, 10.83, 10.83, -14.433793820898133], time: 162.648
steps: 649975, episodes: 26000, mean episode reward: 23.507878619650846, agent episode reward: [13.61, 13.61, 13.61, -17.32212138034916], time: 162.755
steps: 674975, episodes: 27000, mean episode reward: 20.90496791728488, agent episode reward: [13.25, 13.25, 13.25, -18.84503208271512], time: 162.424
steps: 699975, episodes: 28000, mean episode reward: 19.06220707737983, agent episode reward: [11.74, 11.74, 11.74, -16.15779292262017], time: 162.35
steps: 724975, episodes: 29000, mean episode reward: 18.56393023736478, agent episode reward: [11.52, 11.52, 11.52, -15.99606976263522], time: 163.586
steps: 749975, episodes: 30000, mean episode reward: 19.844840755811674, agent episode reward: [12.13, 12.13, 12.13, -16.54515924418833], time: 177.407
steps: 774975, episodes: 31000, mean episode reward: 27.43696098147781, agent episode reward: [15.47, 15.47, 15.47, -18.97303901852219], time: 175.785
steps: 799975, episodes: 32000, mean episode reward: 30.602514529178524, agent episode reward: [17.15, 17.15, 17.15, -20.84748547082148], time: 162.53
steps: 824975, episodes: 33000, mean episode reward: 38.64263768345416, agent episode reward: [21.27, 21.27, 21.27, -25.167362316545848], time: 162.694
steps: 849975, episodes: 34000, mean episode reward: 36.566901667662016, agent episode reward: [20.1, 20.1, 20.1, -23.733098332337978], time: 162.42
steps: 874975, episodes: 35000, mean episode reward: 32.22022934118532, agent episode reward: [18.64, 18.64, 18.64, -23.69977065881468], time: 162.362
steps: 899975, episodes: 36000, mean episode reward: 29.421160904495657, agent episode reward: [17.41, 17.41, 17.41, -22.80883909550435], time: 163.014
steps: 924975, episodes: 37000, mean episode reward: 24.57438813068562, agent episode reward: [14.72, 14.72, 14.72, -19.58561186931438], time: 162.529
steps: 949975, episodes: 38000, mean episode reward: 19.26062547044934, agent episode reward: [12.75, 12.75, 12.75, -18.98937452955066], time: 162.906
steps: 974975, episodes: 39000, mean episode reward: 18.336460354960874, agent episode reward: [12.66, 12.66, 12.66, -19.643539645039127], time: 162.355
steps: 999975, episodes: 40000, mean episode reward: 22.182802841150817, agent episode reward: [14.02, 14.02, 14.02, -19.87719715884918], time: 162.557
steps: 1024975, episodes: 41000, mean episode reward: 18.372369549366592, agent episode reward: [12.51, 12.51, 12.51, -19.15763045063341], time: 163.062
steps: 1049975, episodes: 42000, mean episode reward: 20.081861730243915, agent episode reward: [13.04, 13.04, 13.04, -19.038138269756082], time: 162.367
steps: 1074975, episodes: 43000, mean episode reward: 19.96476218976473, agent episode reward: [12.67, 12.67, 12.67, -18.04523781023527], time: 162.584
steps: 1099975, episodes: 44000, mean episode reward: 18.904784344714745, agent episode reward: [12.48, 12.48, 12.48, -18.535215655285253], time: 163.117
steps: 1124975, episodes: 45000, mean episode reward: 19.30696403118434, agent episode reward: [12.81, 12.81, 12.81, -19.12303596881566], time: 163.045
steps: 1149975, episodes: 46000, mean episode reward: 18.227250063662375, agent episode reward: [12.06, 12.06, 12.06, -17.95274993633763], time: 161.339
steps: 1174975, episodes: 47000, mean episode reward: 16.553226652122802, agent episode reward: [11.77, 11.77, 11.77, -18.7567733478772], time: 163.211
steps: 1199975, episodes: 48000, mean episode reward: 18.551862324620533, agent episode reward: [11.77, 11.77, 11.77, -16.75813767537947], time: 162.917
steps: 1224975, episodes: 49000, mean episode reward: 20.587705918389023, agent episode reward: [12.77, 12.77, 12.77, -17.722294081610976], time: 162.513
steps: 1249975, episodes: 50000, mean episode reward: 22.185797660373492, agent episode reward: [13.74, 13.74, 13.74, -19.034202339626507], time: 162.303
steps: 1274975, episodes: 51000, mean episode reward: 21.870755964332275, agent episode reward: [13.36, 13.36, 13.36, -18.209244035667727], time: 163.782
steps: 1299975, episodes: 52000, mean episode reward: 19.27501232428329, agent episode reward: [11.93, 11.93, 11.93, -16.51498767571671], time: 163.06
steps: 1324975, episodes: 53000, mean episode reward: 20.033555866546113, agent episode reward: [12.07, 12.07, 12.07, -16.176444133453884], time: 162.91
steps: 1349975, episodes: 54000, mean episode reward: 18.431687211660797, agent episode reward: [11.33, 11.33, 11.33, -15.5583127883392], time: 163.599
steps: 1374975, episodes: 55000, mean episode reward: 19.639977513843903, agent episode reward: [12.19, 12.19, 12.19, -16.930022486156098], time: 164.178
steps: 1399975, episodes: 56000, mean episode reward: 21.60408166375852, agent episode reward: [12.81, 12.81, 12.81, -16.825918336241486], time: 161.532
steps: 1424975, episodes: 57000, mean episode reward: 19.422213895419237, agent episode reward: [11.91, 11.91, 11.91, -16.307786104580764], time: 162.891
steps: 1449975, episodes: 58000, mean episode reward: 19.44728791620422, agent episode reward: [12.33, 12.33, 12.33, -17.542712083795777], time: 160.697
steps: 1474975, episodes: 59000, mean episode reward: 19.01112873671867, agent episode reward: [12.12, 12.12, 12.12, -17.348871263281332], time: 154.973
steps: 1499975, episodes: 60000, mean episode reward: 15.56039805187283, agent episode reward: [10.74, 10.74, 10.74, -16.659601948127168], time: 147.91
...Finished total of 60001 episodes.
