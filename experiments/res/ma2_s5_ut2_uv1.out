Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -3.2788718595712387, agent episode reward: [2.2, 2.2, 2.2, -9.87887185957124], time: 102.611
steps: 49975, episodes: 2000, mean episode reward: -5.797614455835206, agent episode reward: [3.96, 3.96, 3.96, -17.677614455835204], time: 148.464
steps: 74975, episodes: 3000, mean episode reward: 8.414391696803358, agent episode reward: [4.8, 4.8, 4.8, -5.985608303196642], time: 152.065
steps: 99975, episodes: 4000, mean episode reward: 8.733341513064953, agent episode reward: [4.6, 4.6, 4.6, -5.066658486935046], time: 152.714
steps: 124975, episodes: 5000, mean episode reward: 10.466792294927446, agent episode reward: [5.42, 5.42, 5.42, -5.793207705072555], time: 153.736
steps: 149975, episodes: 6000, mean episode reward: 10.430145733775731, agent episode reward: [5.45, 5.45, 5.45, -5.919854266224268], time: 152.171
steps: 174975, episodes: 7000, mean episode reward: 14.184110485448084, agent episode reward: [7.32, 7.32, 7.32, -7.775889514551915], time: 152.693
steps: 199975, episodes: 8000, mean episode reward: 27.703772880091577, agent episode reward: [14.07, 14.07, 14.07, -14.506227119908424], time: 153.015
steps: 224975, episodes: 9000, mean episode reward: 35.30551341820157, agent episode reward: [18.04, 18.04, 18.04, -18.814486581798427], time: 153.409
steps: 249975, episodes: 10000, mean episode reward: 27.633452882982162, agent episode reward: [14.62, 14.62, 14.62, -16.226547117017837], time: 153.373
steps: 274975, episodes: 11000, mean episode reward: 21.343798614266984, agent episode reward: [12.54, 12.54, 12.54, -16.276201385733017], time: 153.71
steps: 299975, episodes: 12000, mean episode reward: 14.569493345871553, agent episode reward: [9.75, 9.75, 9.75, -14.680506654128447], time: 152.35
steps: 324975, episodes: 13000, mean episode reward: 12.43954213132356, agent episode reward: [8.67, 8.67, 8.67, -13.57045786867644], time: 152.538
steps: 349975, episodes: 14000, mean episode reward: 12.466736027827471, agent episode reward: [7.81, 7.81, 7.81, -10.963263972172529], time: 152.94
steps: 374975, episodes: 15000, mean episode reward: 16.440003939388756, agent episode reward: [9.72, 9.72, 9.72, -12.719996060611244], time: 153.407
steps: 399975, episodes: 16000, mean episode reward: 15.703404278147108, agent episode reward: [8.99, 8.99, 8.99, -11.266595721852894], time: 153.181
steps: 424975, episodes: 17000, mean episode reward: 16.6902157278881, agent episode reward: [9.41, 9.41, 9.41, -11.5397842721119], time: 157.102
steps: 449975, episodes: 18000, mean episode reward: 13.384200916849922, agent episode reward: [8.29, 8.29, 8.29, -11.485799083150077], time: 161.694
steps: 474975, episodes: 19000, mean episode reward: 13.55081918493906, agent episode reward: [8.54, 8.54, 8.54, -12.069180815060939], time: 162.209
steps: 499975, episodes: 20000, mean episode reward: 15.521337383923342, agent episode reward: [9.95, 9.95, 9.95, -14.328662616076658], time: 162.173
steps: 524975, episodes: 21000, mean episode reward: 13.708498475763355, agent episode reward: [8.73, 8.73, 8.73, -12.481501524236647], time: 162.101
steps: 549975, episodes: 22000, mean episode reward: 16.61083005933361, agent episode reward: [10.08, 10.08, 10.08, -13.62916994066639], time: 162.395
steps: 574975, episodes: 23000, mean episode reward: 21.304677538836337, agent episode reward: [12.17, 12.17, 12.17, -15.205322461163664], time: 163.193
steps: 599975, episodes: 24000, mean episode reward: 18.94494743573403, agent episode reward: [11.36, 11.36, 11.36, -15.13505256426597], time: 163.256
steps: 624975, episodes: 25000, mean episode reward: 18.05620617910187, agent episode reward: [10.83, 10.83, 10.83, -14.433793820898133], time: 162.648
steps: 649975, episodes: 26000, mean episode reward: 23.507878619650846, agent episode reward: [13.61, 13.61, 13.61, -17.32212138034916], time: 162.755
steps: 674975, episodes: 27000, mean episode reward: 20.90496791728488, agent episode reward: [13.25, 13.25, 13.25, -18.84503208271512], time: 162.424
steps: 699975, episodes: 28000, mean episode reward: 19.06220707737983, agent episode reward: [11.74, 11.74, 11.74, -16.15779292262017], time: 162.35
steps: 724975, episodes: 29000, mean episode reward: 18.56393023736478, agent episode reward: [11.52, 11.52, 11.52, -15.99606976263522], time: 163.586
steps: 749975, episodes: 30000, mean episode reward: 19.844840755811674, agent episode reward: [12.13, 12.13, 12.13, -16.54515924418833], time: 177.407
steps: 774975, episodes: 31000, mean episode reward: 27.43696098147781, agent episode reward: [15.47, 15.47, 15.47, -18.97303901852219], time: 175.785
steps: 799975, episodes: 32000, mean episode reward: 30.602514529178524, agent episode reward: [17.15, 17.15, 17.15, -20.84748547082148], time: 162.53
steps: 824975, episodes: 33000, mean episode reward: 38.64263768345416, agent episode reward: [21.27, 21.27, 21.27, -25.167362316545848], time: 162.694
