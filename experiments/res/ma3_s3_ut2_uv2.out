Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -26.74960479919012, agent episode reward: [2.3302741687835287, -29.079878967973652], time: 42.459
steps: 49975, episodes: 2000, mean episode reward: -22.43038491357237, agent episode reward: [-1.8349808618167804, -20.59540405175559], time: 51.315
steps: 74975, episodes: 3000, mean episode reward: -14.776966350389683, agent episode reward: [-6.472894237082463, -8.304072113307221], time: 54.922
steps: 99975, episodes: 4000, mean episode reward: -11.752378874125979, agent episode reward: [-4.463837888756078, -7.2885409853699015], time: 64.756
steps: 124975, episodes: 5000, mean episode reward: -11.181416003955958, agent episode reward: [-3.9005401490405784, -7.280875854915381], time: 64.812
steps: 149975, episodes: 6000, mean episode reward: -10.625519455827193, agent episode reward: [-2.995292042523247, -7.630227413303949], time: 64.862
steps: 174975, episodes: 7000, mean episode reward: -11.355884282109352, agent episode reward: [-3.585064140127848, -7.770820141981505], time: 64.242
steps: 199975, episodes: 8000, mean episode reward: -10.498957788183143, agent episode reward: [-2.7193750533503587, -7.7795827348327835], time: 64.565
steps: 224975, episodes: 9000, mean episode reward: -10.589239122326594, agent episode reward: [-2.664794968497341, -7.9244441538292545], time: 64.801
steps: 249975, episodes: 10000, mean episode reward: -10.142047929507493, agent episode reward: [-2.752340430800077, -7.389707498707415], time: 65.058
steps: 274975, episodes: 11000, mean episode reward: -10.361574246112982, agent episode reward: [-2.6685597341886407, -7.693014511924341], time: 64.481
steps: 299975, episodes: 12000, mean episode reward: -10.265575271461648, agent episode reward: [-2.711187766940805, -7.554387504520843], time: 64.682
steps: 324975, episodes: 13000, mean episode reward: -10.014196849198939, agent episode reward: [-2.3384639086972854, -7.675732940501653], time: 64.675
steps: 349975, episodes: 14000, mean episode reward: -10.107351466442106, agent episode reward: [-2.241444092401009, -7.865907374041096], time: 64.788
steps: 374975, episodes: 15000, mean episode reward: -9.821596078301493, agent episode reward: [-2.170445524927727, -7.651150553373767], time: 64.814
steps: 399975, episodes: 16000, mean episode reward: -9.624202010752903, agent episode reward: [-2.127421740309025, -7.496780270443878], time: 63.775
steps: 424975, episodes: 17000, mean episode reward: -10.158307543989874, agent episode reward: [-1.8302936659409907, -8.328013878048885], time: 63.456
steps: 449975, episodes: 18000, mean episode reward: -9.822939425822376, agent episode reward: [-1.7100051701491172, -8.11293425567326], time: 63.878
steps: 474975, episodes: 19000, mean episode reward: -9.644147355711413, agent episode reward: [-1.3480365788021422, -8.29611077690927], time: 64.548
steps: 499975, episodes: 20000, mean episode reward: -10.21693730882662, agent episode reward: [-1.7145508440686608, -8.502386464757958], time: 64.507
steps: 524975, episodes: 21000, mean episode reward: -10.248880502444445, agent episode reward: [-1.6671222273062376, -8.581758275138206], time: 64.362
steps: 549975, episodes: 22000, mean episode reward: -10.023650902943137, agent episode reward: [-1.35644551709268, -8.667205385850457], time: 66.24
steps: 574975, episodes: 23000, mean episode reward: -10.246215681274046, agent episode reward: [-1.1969916312588662, -9.049224050015182], time: 67.15
steps: 599975, episodes: 24000, mean episode reward: -10.085691070835837, agent episode reward: [-1.6437178737385931, -8.441973197097242], time: 68.525
steps: 624975, episodes: 25000, mean episode reward: -9.82911090341864, agent episode reward: [-1.4799189818291278, -8.349191921589512], time: 69.088
steps: 649975, episodes: 26000, mean episode reward: -9.77034308897437, agent episode reward: [-1.4268246231557704, -8.343518465818601], time: 69.13
steps: 674975, episodes: 27000, mean episode reward: -9.691440510531908, agent episode reward: [-1.3117200627944758, -8.379720447737432], time: 69.023
steps: 699975, episodes: 28000, mean episode reward: -9.974438019805929, agent episode reward: [-1.536577071982629, -8.4378609478233], time: 69.161
steps: 724975, episodes: 29000, mean episode reward: -10.11706319074022, agent episode reward: [-1.567105339190845, -8.549957851549378], time: 69.004
steps: 749975, episodes: 30000, mean episode reward: -10.248517461237967, agent episode reward: [-2.509137052490586, -7.739380408747381], time: 68.487
steps: 774975, episodes: 31000, mean episode reward: -9.92601079354183, agent episode reward: [-2.3466082685553733, -7.579402524986455], time: 68.847
steps: 799975, episodes: 32000, mean episode reward: -9.715838983997337, agent episode reward: [-2.0914374383704937, -7.624401545626844], time: 69.075
steps: 824975, episodes: 33000, mean episode reward: -10.081648168638955, agent episode reward: [-1.9626568225175982, -8.118991346121355], time: 68.755
steps: 849975, episodes: 34000, mean episode reward: -9.727620957155825, agent episode reward: [-2.183474037491013, -7.5441469196648105], time: 68.892
steps: 874975, episodes: 35000, mean episode reward: -10.069152599882662, agent episode reward: [-2.3588123671527264, -7.710340232729936], time: 68.68
steps: 899975, episodes: 36000, mean episode reward: -10.049567995802358, agent episode reward: [-2.423798333669931, -7.625769662132426], time: 69.283
steps: 924975, episodes: 37000, mean episode reward: -10.079226370592366, agent episode reward: [-2.6478100461936735, -7.4314163243986915], time: 68.884
steps: 949975, episodes: 38000, mean episode reward: -9.872253147540848, agent episode reward: [-2.379481841987473, -7.492771305553376], time: 68.973
steps: 974975, episodes: 39000, mean episode reward: -10.279621845577621, agent episode reward: [-2.679237497558144, -7.600384348019476], time: 68.869
steps: 999975, episodes: 40000, mean episode reward: -9.86667316962659, agent episode reward: [-2.2488963116203764, -7.617776858006215], time: 69.014
steps: 1024975, episodes: 41000, mean episode reward: -9.656716464171385, agent episode reward: [-2.085208522610594, -7.5715079415607915], time: 68.694
steps: 1049975, episodes: 42000, mean episode reward: -9.894855483651385, agent episode reward: [-2.336178051674742, -7.558677431976645], time: 68.793
steps: 1074975, episodes: 43000, mean episode reward: -9.501791743773563, agent episode reward: [-2.05361678909935, -7.448174954674214], time: 68.881
steps: 1099975, episodes: 44000, mean episode reward: -9.728359611660931, agent episode reward: [-1.7902641918580746, -7.938095419802856], time: 68.8
steps: 1124975, episodes: 45000, mean episode reward: -9.56251170560386, agent episode reward: [-1.4896578237234552, -8.072853881880404], time: 69.584
steps: 1149975, episodes: 46000, mean episode reward: -10.204229517689797, agent episode reward: [-1.300730044916129, -8.90349947277367], time: 68.971
steps: 1174975, episodes: 47000, mean episode reward: -9.924394664077235, agent episode reward: [-1.1351333872522307, -8.789261276825005], time: 68.406
steps: 1199975, episodes: 48000, mean episode reward: -10.192310787442002, agent episode reward: [-1.7191307288352604, -8.47318005860674], time: 68.918
steps: 1224975, episodes: 49000, mean episode reward: -10.754334811188423, agent episode reward: [-2.095314480700783, -8.659020330487637], time: 69.23
steps: 1249975, episodes: 50000, mean episode reward: -10.637574347524527, agent episode reward: [-2.107170990370119, -8.530403357154409], time: 69.27
steps: 1274975, episodes: 51000, mean episode reward: -11.001909249663841, agent episode reward: [-1.6313588205113914, -9.370550429152448], time: 69.133
steps: 1299975, episodes: 52000, mean episode reward: -10.976446340825653, agent episode reward: [-1.0279732316208288, -9.948473109204821], time: 69.376
steps: 1324975, episodes: 53000, mean episode reward: -10.982061776427786, agent episode reward: [-0.622964461550506, -10.359097314877282], time: 68.971
steps: 1349975, episodes: 54000, mean episode reward: -10.97241083228821, agent episode reward: [-1.2731445782422361, -9.699266254045972], time: 69.362
steps: 1374975, episodes: 55000, mean episode reward: -11.05291471129122, agent episode reward: [-1.383755755827606, -9.669158955463613], time: 69.171
steps: 1399975, episodes: 56000, mean episode reward: -10.842018193704009, agent episode reward: [-1.9480059801242815, -8.894012213579726], time: 69.853
steps: 1424975, episodes: 57000, mean episode reward: -11.433570694713078, agent episode reward: [-1.5334394792343133, -9.900131215478767], time: 68.42
steps: 1449975, episodes: 58000, mean episode reward: -11.131346397145839, agent episode reward: [-1.4180007785920528, -9.713345618553785], time: 68.362
steps: 1474975, episodes: 59000, mean episode reward: -10.916339109521719, agent episode reward: [-1.3945727552566902, -9.521766354265027], time: 67.957
steps: 1499975, episodes: 60000, mean episode reward: -11.405536548274114, agent episode reward: [-1.6031582652705147, -9.8023782830036], time: 64.413
...Finished total of 60001 episodes.
