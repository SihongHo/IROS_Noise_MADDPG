Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -25.940198006223735, agent episode reward: [-0.41824070298169486, -25.521957303242036], time: 18.604
steps: 49975, episodes: 2000, mean episode reward: -24.355948705239964, agent episode reward: [-8.327507661979599, -16.028441043260365], time: 30.31
steps: 74975, episodes: 3000, mean episode reward: -13.074929209750211, agent episode reward: [-5.9614729557937745, -7.113456253956436], time: 30.371
steps: 99975, episodes: 4000, mean episode reward: -10.673591070976105, agent episode reward: [-3.794844989885228, -6.878746081090875], time: 30.746
steps: 124975, episodes: 5000, mean episode reward: -10.128840275687395, agent episode reward: [-3.1544406108097585, -6.974399664877637], time: 30.661
steps: 149975, episodes: 6000, mean episode reward: -9.97501565111191, agent episode reward: [-2.9283770417926083, -7.0466386093193005], time: 29.541
steps: 174975, episodes: 7000, mean episode reward: -9.92240975011154, agent episode reward: [-2.6465310799991455, -7.275878670112394], time: 29.076
steps: 199975, episodes: 8000, mean episode reward: -9.831888360141495, agent episode reward: [-2.3940795957715526, -7.437808764369941], time: 29.905
steps: 224975, episodes: 9000, mean episode reward: -9.65405595018951, agent episode reward: [-2.664121920958055, -6.989934029231457], time: 28.368
steps: 249975, episodes: 10000, mean episode reward: -10.009412078323585, agent episode reward: [-2.6831901295403586, -7.326221948783228], time: 29.112
steps: 274975, episodes: 11000, mean episode reward: -9.825041514618839, agent episode reward: [-2.563352003826631, -7.261689510792207], time: 30.758
steps: 299975, episodes: 12000, mean episode reward: -9.780733179845658, agent episode reward: [-2.6813770775292323, -7.099356102316427], time: 30.091
steps: 324975, episodes: 13000, mean episode reward: -9.331920670639873, agent episode reward: [-1.9503507107338742, -7.381569959905999], time: 30.399
steps: 349975, episodes: 14000, mean episode reward: -9.204968905951656, agent episode reward: [-1.6396123843748107, -7.565356521576845], time: 28.529
steps: 374975, episodes: 15000, mean episode reward: -9.270927318315142, agent episode reward: [-1.6518715467414444, -7.619055771573698], time: 29.317
steps: 399975, episodes: 16000, mean episode reward: -9.198916908364513, agent episode reward: [-1.6703723328758697, -7.528544575488643], time: 30.074
steps: 424975, episodes: 17000, mean episode reward: -9.690225082058372, agent episode reward: [-1.940197528383182, -7.75002755367519], time: 30.414
steps: 449975, episodes: 18000, mean episode reward: -9.724153338116551, agent episode reward: [-2.1099466603899764, -7.614206677726576], time: 30.497
steps: 474975, episodes: 19000, mean episode reward: -9.467707358634655, agent episode reward: [-2.0491984929280753, -7.418508865706581], time: 30.586
steps: 499975, episodes: 20000, mean episode reward: -9.76729799386115, agent episode reward: [-2.251176659861065, -7.516121334000086], time: 30.645
steps: 524975, episodes: 21000, mean episode reward: -9.49644390224865, agent episode reward: [-2.0970202756838785, -7.399423626564771], time: 30.794
steps: 549975, episodes: 22000, mean episode reward: -9.86406703203781, agent episode reward: [-2.3408431816881756, -7.523223850349633], time: 31.335
steps: 574975, episodes: 23000, mean episode reward: -9.58863271679053, agent episode reward: [-2.192086035218188, -7.3965466815723415], time: 30.159
steps: 599975, episodes: 24000, mean episode reward: -9.803666504157315, agent episode reward: [-2.0397783655004726, -7.763888138656841], time: 29.167
steps: 624975, episodes: 25000, mean episode reward: -9.759359549414002, agent episode reward: [-2.162508640940313, -7.596850908473689], time: 29.262
steps: 649975, episodes: 26000, mean episode reward: -9.522353737855664, agent episode reward: [-2.32810825581325, -7.194245482042413], time: 28.821
steps: 674975, episodes: 27000, mean episode reward: -9.330650207931226, agent episode reward: [-2.146168091259278, -7.184482116671948], time: 29.829
steps: 699975, episodes: 28000, mean episode reward: -9.754227455591208, agent episode reward: [-2.5443502860127443, -7.209877169578464], time: 30.659
steps: 724975, episodes: 29000, mean episode reward: -9.534107354280765, agent episode reward: [-2.6387283527850576, -6.895379001495707], time: 29.668
steps: 749975, episodes: 30000, mean episode reward: -9.75790224336641, agent episode reward: [-2.8777152608949055, -6.880186982471503], time: 30.049
steps: 774975, episodes: 31000, mean episode reward: -9.479718082628002, agent episode reward: [-2.5687261493628073, -6.910991933265193], time: 30.379
steps: 799975, episodes: 32000, mean episode reward: -9.814959073225149, agent episode reward: [-2.4733713247472133, -7.3415877484779335], time: 30.528
steps: 824975, episodes: 33000, mean episode reward: -9.649975919646975, agent episode reward: [-2.6366721032154308, -7.013303816431545], time: 30.789
steps: 849975, episodes: 34000, mean episode reward: -9.491871409864386, agent episode reward: [-1.8702660694051618, -7.621605340459222], time: 30.289
steps: 874975, episodes: 35000, mean episode reward: -10.086139043107538, agent episode reward: [-2.3453735761857297, -7.740765466921807], time: 30.478
steps: 899975, episodes: 36000, mean episode reward: -9.316809503177973, agent episode reward: [-1.9902612358171639, -7.32654826736081], time: 31.098
steps: 924975, episodes: 37000, mean episode reward: -9.916619117081499, agent episode reward: [-2.279352762057148, -7.637266355024351], time: 30.867
steps: 949975, episodes: 38000, mean episode reward: -9.42768986489499, agent episode reward: [-2.220448073350397, -7.207241791544593], time: 30.599
steps: 974975, episodes: 39000, mean episode reward: -9.34594141651809, agent episode reward: [-1.9321873149146154, -7.413754101603476], time: 29.605
steps: 999975, episodes: 40000, mean episode reward: -9.765949060751348, agent episode reward: [-1.8482663868469416, -7.917682673904404], time: 29.177
steps: 1024975, episodes: 41000, mean episode reward: -9.828925507525444, agent episode reward: [-1.758819118427156, -8.070106389098287], time: 29.193
steps: 1049975, episodes: 42000, mean episode reward: -9.719869565964975, agent episode reward: [-0.8491036009600016, -8.870765965004972], time: 29.409
steps: 1074975, episodes: 43000, mean episode reward: -9.551556921959037, agent episode reward: [-1.131564592303027, -8.419992329656008], time: 28.961
steps: 1099975, episodes: 44000, mean episode reward: -9.55782925097626, agent episode reward: [-1.5408135333211008, -8.01701571765516], time: 29.665
steps: 1124975, episodes: 45000, mean episode reward: -10.140251789982972, agent episode reward: [-2.5495654943844146, -7.590686295598558], time: 30.745
steps: 1149975, episodes: 46000, mean episode reward: -10.019169669947289, agent episode reward: [-2.7581862123268985, -7.26098345762039], time: 29.64
steps: 1174975, episodes: 47000, mean episode reward: -10.168110443447327, agent episode reward: [-2.209158291839804, -7.958952151607522], time: 28.458
steps: 1199975, episodes: 48000, mean episode reward: -9.859240344729747, agent episode reward: [-1.6732707572096481, -8.185969587520098], time: 28.573
steps: 1224975, episodes: 49000, mean episode reward: -9.770196195817636, agent episode reward: [-1.1330728956936975, -8.637123300123937], time: 29.857
steps: 1249975, episodes: 50000, mean episode reward: -9.754587750407314, agent episode reward: [-0.9255774478075798, -8.829010302599734], time: 29.672
steps: 1274975, episodes: 51000, mean episode reward: -10.142732497504841, agent episode reward: [-0.739447552321858, -9.403284945182984], time: 29.505
steps: 1299975, episodes: 52000, mean episode reward: -10.172060324655432, agent episode reward: [-0.4457168694858357, -9.726343455169594], time: 30.735
steps: 1324975, episodes: 53000, mean episode reward: -10.439884146373105, agent episode reward: [-0.8398641323065668, -9.600020014066537], time: 31.018
steps: 1349975, episodes: 54000, mean episode reward: -10.308634591675323, agent episode reward: [-2.82010479590709, -7.488529795768233], time: 30.935
steps: 1374975, episodes: 55000, mean episode reward: -10.108487781004486, agent episode reward: [-2.982750267988974, -7.125737513015511], time: 31.117
steps: 1399975, episodes: 56000, mean episode reward: -10.392823432047932, agent episode reward: [-3.2829218193235095, -7.109901612724423], time: 31.592
steps: 1424975, episodes: 57000, mean episode reward: -10.251251259452776, agent episode reward: [-3.252137233542301, -6.999114025910475], time: 31.337
steps: 1449975, episodes: 58000, mean episode reward: -10.28642208735947, agent episode reward: [-3.2369187892063547, -7.049503298153118], time: 30.981
steps: 1474975, episodes: 59000, mean episode reward: -10.07137831447446, agent episode reward: [-2.69034183430358, -7.38103648017088], time: 30.776
steps: 1499975, episodes: 60000, mean episode reward: -10.055400473255688, agent episode reward: [-2.8066060434450506, -7.248794429810638], time: 30.791
...Finished total of 60001 episodes.
