Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -23.665718453739167, agent episode reward: [-35.365104935368215, 5.849693240814523, 5.849693240814523], time: 51.22
steps: 49975, episodes: 2000, mean episode reward: -18.526766804898326, agent episode reward: [-25.666234620319425, 3.5697339077105488, 3.5697339077105488], time: 85.758
steps: 74975, episodes: 3000, mean episode reward: -0.07181470987166141, agent episode reward: [-12.792424875522848, 6.360305082825594, 6.360305082825594], time: 84.218
steps: 99975, episodes: 4000, mean episode reward: 3.2921766188804376, agent episode reward: [-9.874548974173223, 6.58336279652683, 6.58336279652683], time: 86.547
steps: 124975, episodes: 5000, mean episode reward: 2.383619915129375, agent episode reward: [-9.990320388656391, 6.186970151892883, 6.186970151892883], time: 85.881
steps: 149975, episodes: 6000, mean episode reward: 1.907344986533825, agent episode reward: [-9.377244748361958, 5.642294867447891, 5.642294867447891], time: 85.417
steps: 174975, episodes: 7000, mean episode reward: 2.153998538180836, agent episode reward: [-9.89091125697531, 6.022454897578073, 6.022454897578073], time: 86.421
steps: 199975, episodes: 8000, mean episode reward: 2.693216859026269, agent episode reward: [-9.893097891961895, 6.293157375494083, 6.293157375494083], time: 86.529
steps: 224975, episodes: 9000, mean episode reward: 2.125200801997983, agent episode reward: [-10.747300636073298, 6.436250719035641, 6.436250719035641], time: 86.674
steps: 249975, episodes: 10000, mean episode reward: 1.0805553783177382, agent episode reward: [-10.899583701530924, 5.990069539924331, 5.990069539924331], time: 86.755
steps: 274975, episodes: 11000, mean episode reward: 1.5067017341009028, agent episode reward: [-10.848602328533762, 6.177652031317331, 6.177652031317331], time: 88.072
steps: 299975, episodes: 12000, mean episode reward: 0.6074983452754088, agent episode reward: [-11.101921956547692, 5.85471015091155, 5.85471015091155], time: 87.375
steps: 324975, episodes: 13000, mean episode reward: -0.17684573767685163, agent episode reward: [-11.501370724963817, 5.6622624936434836, 5.6622624936434836], time: 87.788
steps: 349975, episodes: 14000, mean episode reward: 0.4833589530655532, agent episode reward: [-10.853365832104858, 5.668362392585206, 5.668362392585206], time: 87.37
steps: 374975, episodes: 15000, mean episode reward: 1.2092652583594636, agent episode reward: [-11.538130951212063, 6.373698104785762, 6.373698104785762], time: 85.609
steps: 399975, episodes: 16000, mean episode reward: 0.20353271009434357, agent episode reward: [-11.191713241875886, 5.697622975985114, 5.697622975985114], time: 86.286
steps: 424975, episodes: 17000, mean episode reward: 0.7014769910381576, agent episode reward: [-10.895228167549245, 5.798352579293701, 5.798352579293701], time: 86.648
steps: 449975, episodes: 18000, mean episode reward: 0.28464746550095227, agent episode reward: [-11.421120640722256, 5.852884053111604, 5.852884053111604], time: 87.543
steps: 474975, episodes: 19000, mean episode reward: -0.43410228194691003, agent episode reward: [-11.412117161312908, 5.489007439682999, 5.489007439682999], time: 87.827
steps: 499975, episodes: 20000, mean episode reward: 0.40784408837729813, agent episode reward: [-11.464702333086182, 5.9362732107317395, 5.9362732107317395], time: 85.851
steps: 524975, episodes: 21000, mean episode reward: -0.49494633633256907, agent episode reward: [-11.663363144167729, 5.5842084039175806, 5.5842084039175806], time: 86.245
steps: 549975, episodes: 22000, mean episode reward: -0.7833444056974428, agent episode reward: [-11.512799634416067, 5.364727614359312, 5.364727614359312], time: 87.008
steps: 574975, episodes: 23000, mean episode reward: -0.601939791649232, agent episode reward: [-10.80488651179026, 5.101473360070514, 5.101473360070514], time: 86.886
steps: 599975, episodes: 24000, mean episode reward: -0.5896595022801264, agent episode reward: [-10.77742384780242, 5.0938821727611465, 5.0938821727611465], time: 87.714
steps: 624975, episodes: 25000, mean episode reward: 0.4506009535764267, agent episode reward: [-11.389514209086537, 5.920057581331482, 5.920057581331482], time: 87.132
steps: 649975, episodes: 26000, mean episode reward: -1.1461333881588625, agent episode reward: [-10.752909797006032, 4.803388204423586, 4.803388204423586], time: 86.898
steps: 674975, episodes: 27000, mean episode reward: -0.9345119498909747, agent episode reward: [-10.977344646775265, 5.021416348442144, 5.021416348442144], time: 86.199
steps: 699975, episodes: 28000, mean episode reward: -0.8359230974314662, agent episode reward: [-12.081866319597237, 5.622971611082884, 5.622971611082884], time: 86.708
steps: 724975, episodes: 29000, mean episode reward: -1.321566070187648, agent episode reward: [-11.388365537649271, 5.033399733730812, 5.033399733730812], time: 86.13
steps: 749975, episodes: 30000, mean episode reward: -0.988355472745712, agent episode reward: [-12.19931448932692, 5.605479508290606, 5.605479508290606], time: 86.885
steps: 774975, episodes: 31000, mean episode reward: -1.1203380855119323, agent episode reward: [-11.29424351858831, 5.086952716538189, 5.086952716538189], time: 87.601
steps: 799975, episodes: 32000, mean episode reward: -1.3871724014184423, agent episode reward: [-12.291873038151827, 5.452350318366692, 5.452350318366692], time: 87.676
steps: 824975, episodes: 33000, mean episode reward: -1.423036648604183, agent episode reward: [-11.349409770776147, 4.9631865610859816, 4.9631865610859816], time: 86.25
steps: 849975, episodes: 34000, mean episode reward: -1.6947383226785437, agent episode reward: [-11.534045181930985, 4.91965342962622, 4.91965342962622], time: 86.465
steps: 874975, episodes: 35000, mean episode reward: -1.746289463179308, agent episode reward: [-12.407632766090039, 5.330671651455366, 5.330671651455366], time: 86.78
steps: 899975, episodes: 36000, mean episode reward: -1.1356829456873143, agent episode reward: [-11.79086955764595, 5.327593305979318, 5.327593305979318], time: 86.759
steps: 924975, episodes: 37000, mean episode reward: -1.5091688767206592, agent episode reward: [-11.253535072006358, 4.872183097642849, 4.872183097642849], time: 85.56
steps: 949975, episodes: 38000, mean episode reward: -1.4402556283307801, agent episode reward: [-12.400480769480088, 5.480112570574653, 5.480112570574653], time: 88.005
steps: 974975, episodes: 39000, mean episode reward: -1.5818308099164693, agent episode reward: [-12.387067830456816, 5.402618510270173, 5.402618510270173], time: 87.187
steps: 999975, episodes: 40000, mean episode reward: -2.2830836751783097, agent episode reward: [-11.17367628561668, 4.445296305219184, 4.445296305219184], time: 81.768
steps: 1024975, episodes: 41000, mean episode reward: -2.5715601686649263, agent episode reward: [-11.72904835817485, 4.578744094754963, 4.578744094754963], time: 79.223
steps: 1049975, episodes: 42000, mean episode reward: -2.117011374457457, agent episode reward: [-12.131327413466373, 5.007158019504459, 5.007158019504459], time: 79.732
steps: 1074975, episodes: 43000, mean episode reward: -2.193077774670015, agent episode reward: [-11.892739456090371, 4.84983084071018, 4.84983084071018], time: 80.544
steps: 1099975, episodes: 44000, mean episode reward: -2.5532329228757207, agent episode reward: [-12.014371941579551, 4.730569509351915, 4.730569509351915], time: 80.459
steps: 1124975, episodes: 45000, mean episode reward: -4.051520351488316, agent episode reward: [-12.293406609229612, 4.120943128870649, 4.120943128870649], time: 80.292
steps: 1149975, episodes: 46000, mean episode reward: -4.149879136981016, agent episode reward: [-12.392751450323786, 4.121436156671385, 4.121436156671385], time: 80.101
steps: 1174975, episodes: 47000, mean episode reward: -4.645028749952636, agent episode reward: [-12.135833527040786, 3.745402388544075, 3.745402388544075], time: 78.625
steps: 1199975, episodes: 48000, mean episode reward: -5.0900926578741394, agent episode reward: [-12.328181969591228, 3.619044655858543, 3.619044655858543], time: 79.751
steps: 1224975, episodes: 49000, mean episode reward: -4.377828313484408, agent episode reward: [-12.313551281984914, 3.967861484250253, 3.967861484250253], time: 78.566
steps: 1249975, episodes: 50000, mean episode reward: -3.5751936467435925, agent episode reward: [-12.612452204244018, 4.518629278750214, 4.518629278750214], time: 78.766
steps: 1274975, episodes: 51000, mean episode reward: -3.5364753118215484, agent episode reward: [-12.618280601746509, 4.54090264496248, 4.54090264496248], time: 78.789
steps: 1299975, episodes: 52000, mean episode reward: -3.8865217887852186, agent episode reward: [-12.419831586363564, 4.266654898789173, 4.266654898789173], time: 79.562
steps: 1324975, episodes: 53000, mean episode reward: -3.3980025654359824, agent episode reward: [-11.922999067050284, 4.262498250807151, 4.262498250807151], time: 80.202
steps: 1349975, episodes: 54000, mean episode reward: -3.3467287209477043, agent episode reward: [-13.198887181336678, 4.926079230194487, 4.926079230194487], time: 79.659
steps: 1374975, episodes: 55000, mean episode reward: -3.836656827784203, agent episode reward: [-12.812780621170088, 4.488061896692943, 4.488061896692943], time: 80.151
steps: 1399975, episodes: 56000, mean episode reward: -3.0932446694606015, agent episode reward: [-12.868163631868818, 4.8874594812041074, 4.8874594812041074], time: 79.543
steps: 1424975, episodes: 57000, mean episode reward: -2.7630425971854318, agent episode reward: [-12.698051323029642, 4.967504362922104, 4.967504362922104], time: 80.247
steps: 1449975, episodes: 58000, mean episode reward: -3.1807812886751496, agent episode reward: [-13.432794485978443, 5.126006598651648, 5.126006598651648], time: 78.47
steps: 1474975, episodes: 59000, mean episode reward: -2.8606036277574565, agent episode reward: [-13.201244263508903, 5.170320317875723, 5.170320317875723], time: 76.727
steps: 1499975, episodes: 60000, mean episode reward: -2.7578368142357803, agent episode reward: [-12.793802777869066, 5.017982981816643, 5.017982981816643], time: 75.202
