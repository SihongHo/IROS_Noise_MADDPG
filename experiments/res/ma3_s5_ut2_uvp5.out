Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -0.9947157838493615, agent episode reward: [1.9, 1.9, 1.9, -6.694715783849362], time: 114.383
steps: 49975, episodes: 2000, mean episode reward: -3.417791030696438, agent episode reward: [3.17, 3.17, 3.17, -12.927791030696438], time: 148.833
steps: 74975, episodes: 3000, mean episode reward: 5.448300935254211, agent episode reward: [3.96, 3.96, 3.96, -6.431699064745789], time: 151.884
steps: 99975, episodes: 4000, mean episode reward: 7.901284521174543, agent episode reward: [4.82, 4.82, 4.82, -6.558715478825458], time: 151.692
steps: 124975, episodes: 5000, mean episode reward: 7.705296828015872, agent episode reward: [4.32, 4.32, 4.32, -5.254703171984129], time: 152.074
steps: 149975, episodes: 6000, mean episode reward: 11.476434933712852, agent episode reward: [6.38, 6.38, 6.38, -7.663565066287148], time: 151.559
steps: 174975, episodes: 7000, mean episode reward: 14.30994779432311, agent episode reward: [7.36, 7.36, 7.36, -7.7700522056768895], time: 151.967
steps: 199975, episodes: 8000, mean episode reward: 24.43966036904285, agent episode reward: [12.59, 12.59, 12.59, -13.330339630957146], time: 153.24
steps: 224975, episodes: 9000, mean episode reward: 41.92685736696067, agent episode reward: [21.69, 21.69, 21.69, -23.143142633039332], time: 152.536
steps: 249975, episodes: 10000, mean episode reward: 37.45543183697716, agent episode reward: [20.21, 20.21, 20.21, -23.174568163022837], time: 153.346
steps: 274975, episodes: 11000, mean episode reward: 33.30926538655956, agent episode reward: [19.78, 19.78, 19.78, -26.030734613440444], time: 153.199
steps: 299975, episodes: 12000, mean episode reward: 12.955469718200423, agent episode reward: [13.44, 13.44, 13.44, -27.364530281799578], time: 152.592
steps: 324975, episodes: 13000, mean episode reward: 21.96825314530089, agent episode reward: [14.08, 14.08, 14.08, -20.27174685469911], time: 152.627
steps: 349975, episodes: 14000, mean episode reward: 14.771623901203867, agent episode reward: [9.47, 9.47, 9.47, -13.638376098796133], time: 152.99
steps: 374975, episodes: 15000, mean episode reward: 14.637149194534645, agent episode reward: [9.35, 9.35, 9.35, -13.412850805465354], time: 152.424
steps: 399975, episodes: 16000, mean episode reward: 16.624417240074905, agent episode reward: [9.59, 9.59, 9.59, -12.145582759925098], time: 152.455
steps: 424975, episodes: 17000, mean episode reward: 19.02403796265982, agent episode reward: [10.96, 10.96, 10.96, -13.85596203734018], time: 157.13
steps: 449975, episodes: 18000, mean episode reward: 12.463219921066761, agent episode reward: [8.04, 8.04, 8.04, -11.65678007893324], time: 161.927
steps: 474975, episodes: 19000, mean episode reward: 13.240955083044012, agent episode reward: [8.19, 8.19, 8.19, -11.329044916955986], time: 161.305
steps: 499975, episodes: 20000, mean episode reward: 11.58749808435782, agent episode reward: [7.13, 7.13, 7.13, -9.802501915642182], time: 161.524
steps: 524975, episodes: 21000, mean episode reward: 16.362593897366146, agent episode reward: [9.2, 9.2, 9.2, -11.237406102633855], time: 161.87
steps: 549975, episodes: 22000, mean episode reward: 20.96923223017445, agent episode reward: [11.71, 11.71, 11.71, -14.16076776982555], time: 161.484
steps: 574975, episodes: 23000, mean episode reward: 15.295281792140461, agent episode reward: [9.42, 9.42, 9.42, -12.964718207859537], time: 161.936
steps: 599975, episodes: 24000, mean episode reward: 18.246705778830332, agent episode reward: [10.66, 10.66, 10.66, -13.733294221169672], time: 160.192
steps: 624975, episodes: 25000, mean episode reward: 25.97118451993171, agent episode reward: [14.39, 14.39, 14.39, -17.19881548006829], time: 161.752
steps: 649975, episodes: 26000, mean episode reward: 33.0380616472029, agent episode reward: [18.12, 18.12, 18.12, -21.321938352797098], time: 161.407
steps: 674975, episodes: 27000, mean episode reward: 30.03918660532959, agent episode reward: [16.82, 16.82, 16.82, -20.42081339467041], time: 160.594
steps: 699975, episodes: 28000, mean episode reward: 29.380323292150152, agent episode reward: [16.59, 16.59, 16.59, -20.389676707849848], time: 161.017
steps: 724975, episodes: 29000, mean episode reward: 29.48318017264231, agent episode reward: [16.41, 16.41, 16.41, -19.746819827357694], time: 163.024
steps: 749975, episodes: 30000, mean episode reward: 23.13896633501374, agent episode reward: [13.64, 13.64, 13.64, -17.781033664986257], time: 177.029
steps: 774975, episodes: 31000, mean episode reward: 28.724484653470565, agent episode reward: [15.86, 15.86, 15.86, -18.855515346529437], time: 177.213
steps: 799975, episodes: 32000, mean episode reward: 29.1618342459818, agent episode reward: [15.94, 15.94, 15.94, -18.6581657540182], time: 161.326
