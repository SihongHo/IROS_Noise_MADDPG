Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -0.9947157838493615, agent episode reward: [1.9, 1.9, 1.9, -6.694715783849362], time: 114.383
steps: 49975, episodes: 2000, mean episode reward: -3.417791030696438, agent episode reward: [3.17, 3.17, 3.17, -12.927791030696438], time: 148.833
steps: 74975, episodes: 3000, mean episode reward: 5.448300935254211, agent episode reward: [3.96, 3.96, 3.96, -6.431699064745789], time: 151.884
steps: 99975, episodes: 4000, mean episode reward: 7.901284521174543, agent episode reward: [4.82, 4.82, 4.82, -6.558715478825458], time: 151.692
steps: 124975, episodes: 5000, mean episode reward: 7.705296828015872, agent episode reward: [4.32, 4.32, 4.32, -5.254703171984129], time: 152.074
steps: 149975, episodes: 6000, mean episode reward: 11.476434933712852, agent episode reward: [6.38, 6.38, 6.38, -7.663565066287148], time: 151.559
steps: 174975, episodes: 7000, mean episode reward: 14.30994779432311, agent episode reward: [7.36, 7.36, 7.36, -7.7700522056768895], time: 151.967
steps: 199975, episodes: 8000, mean episode reward: 24.43966036904285, agent episode reward: [12.59, 12.59, 12.59, -13.330339630957146], time: 153.24
steps: 224975, episodes: 9000, mean episode reward: 41.92685736696067, agent episode reward: [21.69, 21.69, 21.69, -23.143142633039332], time: 152.536
steps: 249975, episodes: 10000, mean episode reward: 37.45543183697716, agent episode reward: [20.21, 20.21, 20.21, -23.174568163022837], time: 153.346
steps: 274975, episodes: 11000, mean episode reward: 33.30926538655956, agent episode reward: [19.78, 19.78, 19.78, -26.030734613440444], time: 153.199
steps: 299975, episodes: 12000, mean episode reward: 12.955469718200423, agent episode reward: [13.44, 13.44, 13.44, -27.364530281799578], time: 152.592
steps: 324975, episodes: 13000, mean episode reward: 21.96825314530089, agent episode reward: [14.08, 14.08, 14.08, -20.27174685469911], time: 152.627
steps: 349975, episodes: 14000, mean episode reward: 14.771623901203867, agent episode reward: [9.47, 9.47, 9.47, -13.638376098796133], time: 152.99
steps: 374975, episodes: 15000, mean episode reward: 14.637149194534645, agent episode reward: [9.35, 9.35, 9.35, -13.412850805465354], time: 152.424
steps: 399975, episodes: 16000, mean episode reward: 16.624417240074905, agent episode reward: [9.59, 9.59, 9.59, -12.145582759925098], time: 152.455
steps: 424975, episodes: 17000, mean episode reward: 19.02403796265982, agent episode reward: [10.96, 10.96, 10.96, -13.85596203734018], time: 157.13
steps: 449975, episodes: 18000, mean episode reward: 12.463219921066761, agent episode reward: [8.04, 8.04, 8.04, -11.65678007893324], time: 161.927
steps: 474975, episodes: 19000, mean episode reward: 13.240955083044012, agent episode reward: [8.19, 8.19, 8.19, -11.329044916955986], time: 161.305
steps: 499975, episodes: 20000, mean episode reward: 11.58749808435782, agent episode reward: [7.13, 7.13, 7.13, -9.802501915642182], time: 161.524
steps: 524975, episodes: 21000, mean episode reward: 16.362593897366146, agent episode reward: [9.2, 9.2, 9.2, -11.237406102633855], time: 161.87
steps: 549975, episodes: 22000, mean episode reward: 20.96923223017445, agent episode reward: [11.71, 11.71, 11.71, -14.16076776982555], time: 161.484
steps: 574975, episodes: 23000, mean episode reward: 15.295281792140461, agent episode reward: [9.42, 9.42, 9.42, -12.964718207859537], time: 161.936
steps: 599975, episodes: 24000, mean episode reward: 18.246705778830332, agent episode reward: [10.66, 10.66, 10.66, -13.733294221169672], time: 160.192
steps: 624975, episodes: 25000, mean episode reward: 25.97118451993171, agent episode reward: [14.39, 14.39, 14.39, -17.19881548006829], time: 161.752
steps: 649975, episodes: 26000, mean episode reward: 33.0380616472029, agent episode reward: [18.12, 18.12, 18.12, -21.321938352797098], time: 161.407
steps: 674975, episodes: 27000, mean episode reward: 30.03918660532959, agent episode reward: [16.82, 16.82, 16.82, -20.42081339467041], time: 160.594
steps: 699975, episodes: 28000, mean episode reward: 29.380323292150152, agent episode reward: [16.59, 16.59, 16.59, -20.389676707849848], time: 161.017
steps: 724975, episodes: 29000, mean episode reward: 29.48318017264231, agent episode reward: [16.41, 16.41, 16.41, -19.746819827357694], time: 163.024
steps: 749975, episodes: 30000, mean episode reward: 23.13896633501374, agent episode reward: [13.64, 13.64, 13.64, -17.781033664986257], time: 177.029
steps: 774975, episodes: 31000, mean episode reward: 28.724484653470565, agent episode reward: [15.86, 15.86, 15.86, -18.855515346529437], time: 177.213
steps: 799975, episodes: 32000, mean episode reward: 29.1618342459818, agent episode reward: [15.94, 15.94, 15.94, -18.6581657540182], time: 161.326
steps: 824975, episodes: 33000, mean episode reward: 30.339039834866636, agent episode reward: [16.43, 16.43, 16.43, -18.950960165133367], time: 162.071
steps: 849975, episodes: 34000, mean episode reward: 29.14587070986449, agent episode reward: [15.68, 15.68, 15.68, -17.894129290135517], time: 161.845
steps: 874975, episodes: 35000, mean episode reward: 31.56870446834959, agent episode reward: [16.85, 16.85, 16.85, -18.98129553165041], time: 161.188
steps: 899975, episodes: 36000, mean episode reward: 31.51038987261011, agent episode reward: [16.79, 16.79, 16.79, -18.859610127389896], time: 162.584
steps: 924975, episodes: 37000, mean episode reward: 30.124017585495444, agent episode reward: [15.93, 15.93, 15.93, -17.66598241450456], time: 161.567
steps: 949975, episodes: 38000, mean episode reward: 32.782660795074506, agent episode reward: [17.32, 17.32, 17.32, -19.17733920492549], time: 160.664
steps: 974975, episodes: 39000, mean episode reward: 28.234670765905694, agent episode reward: [15.53, 15.53, 15.53, -18.355329234094306], time: 160.607
steps: 999975, episodes: 40000, mean episode reward: 25.67725523338558, agent episode reward: [14.2, 14.2, 14.2, -16.922744766614425], time: 160.14
steps: 1024975, episodes: 41000, mean episode reward: 27.399870326956428, agent episode reward: [15.22, 15.22, 15.22, -18.260129673043572], time: 162.0
steps: 1049975, episodes: 42000, mean episode reward: 26.385143402385278, agent episode reward: [14.65, 14.65, 14.65, -17.56485659761472], time: 162.28
steps: 1074975, episodes: 43000, mean episode reward: 24.881549891023177, agent episode reward: [14.18, 14.18, 14.18, -17.658450108976822], time: 162.4
steps: 1099975, episodes: 44000, mean episode reward: 20.78205552959619, agent episode reward: [12.19, 12.19, 12.19, -15.787944470403815], time: 161.048
steps: 1124975, episodes: 45000, mean episode reward: 19.335816817563522, agent episode reward: [11.39, 11.39, 11.39, -14.834183182436476], time: 163.264
steps: 1149975, episodes: 46000, mean episode reward: 19.16842775528532, agent episode reward: [11.25, 11.25, 11.25, -14.58157224471468], time: 161.964
steps: 1174975, episodes: 47000, mean episode reward: 21.21007457679728, agent episode reward: [12.16, 12.16, 12.16, -15.26992542320272], time: 160.277
steps: 1199975, episodes: 48000, mean episode reward: 19.722145896328133, agent episode reward: [11.53, 11.53, 11.53, -14.867854103671867], time: 161.655
steps: 1224975, episodes: 49000, mean episode reward: 18.553500938905845, agent episode reward: [11.03, 11.03, 11.03, -14.536499061094151], time: 160.864
steps: 1249975, episodes: 50000, mean episode reward: 18.059283955002094, agent episode reward: [10.45, 10.45, 10.45, -13.290716044997902], time: 161.691
steps: 1274975, episodes: 51000, mean episode reward: 20.80617221572421, agent episode reward: [12.14, 12.14, 12.14, -15.613827784275788], time: 162.643
steps: 1299975, episodes: 52000, mean episode reward: 19.36251048251598, agent episode reward: [11.4, 11.4, 11.4, -14.83748951748402], time: 162.543
steps: 1324975, episodes: 53000, mean episode reward: 18.542720491604005, agent episode reward: [10.66, 10.66, 10.66, -13.437279508395992], time: 162.517
steps: 1349975, episodes: 54000, mean episode reward: 23.259519849567727, agent episode reward: [12.82, 12.82, 12.82, -15.200480150432272], time: 162.289
steps: 1374975, episodes: 55000, mean episode reward: 21.65557708457039, agent episode reward: [12.64, 12.64, 12.64, -16.264422915429613], time: 163.305
steps: 1399975, episodes: 56000, mean episode reward: 19.872273441521166, agent episode reward: [11.62, 11.62, 11.62, -14.987726558478832], time: 162.4
steps: 1424975, episodes: 57000, mean episode reward: 23.068756309298376, agent episode reward: [13.36, 13.36, 13.36, -17.011243690701622], time: 162.31
steps: 1449975, episodes: 58000, mean episode reward: 19.582255793874968, agent episode reward: [11.81, 11.81, 11.81, -15.847744206125038], time: 160.165
steps: 1474975, episodes: 59000, mean episode reward: 23.35781362683201, agent episode reward: [14.1, 14.1, 14.1, -18.942186373167992], time: 154.19
steps: 1499975, episodes: 60000, mean episode reward: 22.926991960518112, agent episode reward: [13.58, 13.58, 13.58, -17.81300803948189], time: 148.609
...Finished total of 60001 episodes.
