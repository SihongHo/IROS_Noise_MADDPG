Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -3.055602897923527, agent episode reward: [2.26, 2.26, 2.26, -9.835602897923527], time: 93.983
steps: 49975, episodes: 2000, mean episode reward: -15.492493479309049, agent episode reward: [3.26, 3.26, 3.26, -25.272493479309052], time: 129.386
steps: 74975, episodes: 3000, mean episode reward: 8.017292300143305, agent episode reward: [4.98, 4.98, 4.98, -6.922707699856695], time: 126.631
steps: 99975, episodes: 4000, mean episode reward: 9.020463221404267, agent episode reward: [4.8, 4.8, 4.8, -5.3795367785957335], time: 130.136
steps: 124975, episodes: 5000, mean episode reward: 10.312072432705746, agent episode reward: [5.44, 5.44, 5.44, -6.007927567294254], time: 127.651
steps: 149975, episodes: 6000, mean episode reward: 11.20747089194653, agent episode reward: [5.8, 5.8, 5.8, -6.19252910805347], time: 128.839
steps: 174975, episodes: 7000, mean episode reward: 14.248500111493625, agent episode reward: [7.38, 7.38, 7.38, -7.891499888506374], time: 128.627
steps: 199975, episodes: 8000, mean episode reward: 14.918384787245158, agent episode reward: [7.68, 7.68, 7.68, -8.121615212754845], time: 126.791
steps: 224975, episodes: 9000, mean episode reward: 16.093868597947797, agent episode reward: [8.48, 8.48, 8.48, -9.346131402052206], time: 128.265
steps: 249975, episodes: 10000, mean episode reward: 22.275449632421424, agent episode reward: [11.52, 11.52, 11.52, -12.284550367578577], time: 128.319
steps: 274975, episodes: 11000, mean episode reward: 45.21024889082789, agent episode reward: [23.12, 23.12, 23.12, -24.1497511091721], time: 128.513
steps: 299975, episodes: 12000, mean episode reward: 45.494433238589394, agent episode reward: [23.62, 23.62, 23.62, -25.365566761410605], time: 129.246
steps: 324975, episodes: 13000, mean episode reward: 21.485858822923603, agent episode reward: [12.66, 12.66, 12.66, -16.49414117707639], time: 129.906
steps: 349975, episodes: 14000, mean episode reward: 11.897230955724385, agent episode reward: [9.55, 9.55, 9.55, -16.752769044275613], time: 130.498
steps: 374975, episodes: 15000, mean episode reward: 11.040694926751911, agent episode reward: [9.23, 9.23, 9.23, -16.64930507324809], time: 128.967
steps: 399975, episodes: 16000, mean episode reward: 16.082115036936447, agent episode reward: [9.97, 9.97, 9.97, -13.827884963063552], time: 129.412
steps: 424975, episodes: 17000, mean episode reward: 17.930915903954702, agent episode reward: [10.47, 10.47, 10.47, -13.479084096045298], time: 128.951
steps: 449975, episodes: 18000, mean episode reward: 13.042583933096362, agent episode reward: [8.39, 8.39, 8.39, -12.127416066903638], time: 129.705
steps: 474975, episodes: 19000, mean episode reward: 14.54950832093757, agent episode reward: [9.65, 9.65, 9.65, -14.40049167906243], time: 130.81
steps: 499975, episodes: 20000, mean episode reward: 14.77424306861135, agent episode reward: [10.03, 10.03, 10.03, -15.31575693138865], time: 128.484
steps: 524975, episodes: 21000, mean episode reward: 15.30201054288548, agent episode reward: [10.74, 10.74, 10.74, -16.91798945711452], time: 128.856
steps: 549975, episodes: 22000, mean episode reward: 17.73711801260678, agent episode reward: [11.38, 11.38, 11.38, -16.402881987393222], time: 129.288
steps: 574975, episodes: 23000, mean episode reward: 15.575946480155567, agent episode reward: [10.76, 10.76, 10.76, -16.704053519844432], time: 131.183
steps: 599975, episodes: 24000, mean episode reward: 20.275564577934297, agent episode reward: [12.34, 12.34, 12.34, -16.744435422065703], time: 129.811
steps: 624975, episodes: 25000, mean episode reward: 24.018152723865178, agent episode reward: [14.46, 14.46, 14.46, -19.361847276134824], time: 126.361
steps: 649975, episodes: 26000, mean episode reward: 26.45318892341097, agent episode reward: [15.16, 15.16, 15.16, -19.02681107658903], time: 128.405
steps: 674975, episodes: 27000, mean episode reward: 31.049773447567883, agent episode reward: [17.09, 17.09, 17.09, -20.220226552432116], time: 119.799
steps: 699975, episodes: 28000, mean episode reward: 36.63616630999175, agent episode reward: [19.66, 19.66, 19.66, -22.34383369000825], time: 119.616
steps: 724975, episodes: 29000, mean episode reward: 44.19195127222466, agent episode reward: [23.41, 23.41, 23.41, -26.03804872777534], time: 117.057
steps: 749975, episodes: 30000, mean episode reward: 43.32948355505198, agent episode reward: [23.15, 23.15, 23.15, -26.12051644494803], time: 116.622
steps: 774975, episodes: 31000, mean episode reward: 49.45074807030579, agent episode reward: [26.0, 26.0, 26.0, -28.549251929694208], time: 117.064
steps: 799975, episodes: 32000, mean episode reward: 46.5134952361147, agent episode reward: [25.1, 25.1, 25.1, -28.7865047638853], time: 118.659
steps: 824975, episodes: 33000, mean episode reward: 38.9901537658135, agent episode reward: [21.83, 21.83, 21.83, -26.499846234186496], time: 116.858
steps: 849975, episodes: 34000, mean episode reward: 41.69584865878317, agent episode reward: [23.07, 23.07, 23.07, -27.514151341216824], time: 116.907
steps: 874975, episodes: 35000, mean episode reward: 34.611810401811994, agent episode reward: [19.91, 19.91, 19.91, -25.118189598188007], time: 119.168
steps: 899975, episodes: 36000, mean episode reward: 32.27555629850892, agent episode reward: [18.65, 18.65, 18.65, -23.674443701491086], time: 116.177
steps: 924975, episodes: 37000, mean episode reward: 27.083281326695154, agent episode reward: [16.21, 16.21, 16.21, -21.54671867330485], time: 115.2
steps: 949975, episodes: 38000, mean episode reward: 20.95938872491837, agent episode reward: [14.3, 14.3, 14.3, -21.940611275081636], time: 116.146
steps: 974975, episodes: 39000, mean episode reward: 22.529915617339284, agent episode reward: [14.7, 14.7, 14.7, -21.570084382660717], time: 115.418
steps: 999975, episodes: 40000, mean episode reward: 22.98116504888414, agent episode reward: [15.04, 15.04, 15.04, -22.13883495111586], time: 114.74
steps: 1024975, episodes: 41000, mean episode reward: 21.85955543679911, agent episode reward: [14.13, 14.13, 14.13, -20.53044456320089], time: 91.089
steps: 1049975, episodes: 42000, mean episode reward: 24.793986957849302, agent episode reward: [15.5, 15.5, 15.5, -21.706013042150698], time: 87.126
steps: 1074975, episodes: 43000, mean episode reward: 22.227987008689926, agent episode reward: [15.03, 15.03, 15.03, -22.86201299131007], time: 88.387
steps: 1099975, episodes: 44000, mean episode reward: 23.300847638732417, agent episode reward: [15.32, 15.32, 15.32, -22.65915236126758], time: 90.365
steps: 1124975, episodes: 45000, mean episode reward: 25.78021653878472, agent episode reward: [15.63, 15.63, 15.63, -21.109783461215283], time: 88.582
steps: 1149975, episodes: 46000, mean episode reward: 25.349407576244854, agent episode reward: [15.3, 15.3, 15.3, -20.550592423755145], time: 87.585
steps: 1174975, episodes: 47000, mean episode reward: 24.409237132068593, agent episode reward: [14.68, 14.68, 14.68, -19.63076286793141], time: 85.772
steps: 1199975, episodes: 48000, mean episode reward: 21.56464996182469, agent episode reward: [13.32, 13.32, 13.32, -18.395350038175312], time: 86.68
steps: 1224975, episodes: 49000, mean episode reward: 26.658350537695654, agent episode reward: [15.74, 15.74, 15.74, -20.56164946230435], time: 87.744
steps: 1249975, episodes: 50000, mean episode reward: 24.576214983411358, agent episode reward: [14.78, 14.78, 14.78, -19.763785016588642], time: 88.139
steps: 1274975, episodes: 51000, mean episode reward: 29.598887224083374, agent episode reward: [16.79, 16.79, 16.79, -20.771112775916627], time: 87.187
steps: 1299975, episodes: 52000, mean episode reward: 26.753493936689278, agent episode reward: [15.52, 15.52, 15.52, -19.80650606331072], time: 86.035
steps: 1324975, episodes: 53000, mean episode reward: 24.983100148126038, agent episode reward: [14.48, 14.48, 14.48, -18.45689985187396], time: 87.255
steps: 1349975, episodes: 54000, mean episode reward: 23.742845964553986, agent episode reward: [13.91, 13.91, 13.91, -17.987154035446014], time: 88.827
steps: 1374975, episodes: 55000, mean episode reward: 21.951043243602058, agent episode reward: [12.95, 12.95, 12.95, -16.898956756397943], time: 85.269
steps: 1399975, episodes: 56000, mean episode reward: 20.903343202515856, agent episode reward: [12.45, 12.45, 12.45, -16.446656797484145], time: 90.573
steps: 1424975, episodes: 57000, mean episode reward: 19.364163730930194, agent episode reward: [11.58, 11.58, 11.58, -15.375836269069802], time: 85.364
steps: 1449975, episodes: 58000, mean episode reward: 22.16908475361763, agent episode reward: [13.56, 13.56, 13.56, -18.51091524638237], time: 90.181
steps: 1474975, episodes: 59000, mean episode reward: 18.288670466335795, agent episode reward: [11.62, 11.62, 11.62, -16.571329533664205], time: 91.196
steps: 1499975, episodes: 60000, mean episode reward: 20.229708702433125, agent episode reward: [12.41, 12.41, 12.41, -17.000291297566875], time: 91.735
