Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -2.4771305752212283, agent episode reward: [2.16, 2.16, 2.16, -8.957130575221228], time: 116.395
steps: 49975, episodes: 2000, mean episode reward: -5.371810901766345, agent episode reward: [4.52, 4.52, 4.52, -18.931810901766344], time: 150.503
steps: 74975, episodes: 3000, mean episode reward: 8.19342657375722, agent episode reward: [5.18, 5.18, 5.18, -7.346573426242779], time: 152.29
steps: 99975, episodes: 4000, mean episode reward: 10.270079614238457, agent episode reward: [5.72, 5.72, 5.72, -6.889920385761543], time: 152.563
steps: 124975, episodes: 5000, mean episode reward: 17.534133138753024, agent episode reward: [9.24, 9.24, 9.24, -10.185866861246977], time: 153.293
steps: 149975, episodes: 6000, mean episode reward: 24.570517466269102, agent episode reward: [12.47, 12.47, 12.47, -12.839482533730898], time: 152.338
steps: 174975, episodes: 7000, mean episode reward: 25.310796719597462, agent episode reward: [13.42, 13.42, 13.42, -14.949203280402536], time: 151.363
steps: 199975, episodes: 8000, mean episode reward: 18.044737661424193, agent episode reward: [9.94, 9.94, 9.94, -11.775262338575805], time: 152.701
steps: 224975, episodes: 9000, mean episode reward: 13.577361547121019, agent episode reward: [8.45, 8.45, 8.45, -11.772638452878985], time: 153.011
steps: 249975, episodes: 10000, mean episode reward: 14.121302055544605, agent episode reward: [8.47, 8.47, 8.47, -11.288697944455397], time: 153.609
steps: 274975, episodes: 11000, mean episode reward: 12.760471424015307, agent episode reward: [7.85, 7.85, 7.85, -10.789528575984692], time: 152.927
steps: 299975, episodes: 12000, mean episode reward: 13.143028675296925, agent episode reward: [7.74, 7.74, 7.74, -10.076971324703074], time: 153.015
steps: 324975, episodes: 13000, mean episode reward: 15.291447479720553, agent episode reward: [8.43, 8.43, 8.43, -9.998552520279446], time: 152.476
steps: 349975, episodes: 14000, mean episode reward: 13.660846385264803, agent episode reward: [8.06, 8.06, 8.06, -10.519153614735199], time: 153.569
steps: 374975, episodes: 15000, mean episode reward: 15.13163257763063, agent episode reward: [8.77, 8.77, 8.77, -11.178367422369373], time: 153.041
steps: 399975, episodes: 16000, mean episode reward: 13.363582341046953, agent episode reward: [7.59, 7.59, 7.59, -9.406417658953048], time: 154.057
steps: 424975, episodes: 17000, mean episode reward: 14.934349852783756, agent episode reward: [8.31, 8.31, 8.31, -9.995650147216244], time: 159.141
steps: 449975, episodes: 18000, mean episode reward: 19.977966248729963, agent episode reward: [10.88, 10.88, 10.88, -12.662033751270036], time: 162.958
steps: 474975, episodes: 19000, mean episode reward: 25.363895501243338, agent episode reward: [13.32, 13.32, 13.32, -14.59610449875666], time: 162.209
steps: 499975, episodes: 20000, mean episode reward: 32.98625379002696, agent episode reward: [17.14, 17.14, 17.14, -18.433746209973048], time: 162.157
steps: 524975, episodes: 21000, mean episode reward: 28.90715114166293, agent episode reward: [15.34, 15.34, 15.34, -17.11284885833707], time: 162.621
steps: 549975, episodes: 22000, mean episode reward: 26.317573341091986, agent episode reward: [14.32, 14.32, 14.32, -16.642426658908015], time: 162.672
steps: 574975, episodes: 23000, mean episode reward: 20.661901940286754, agent episode reward: [11.71, 11.71, 11.71, -14.468098059713244], time: 163.443
steps: 599975, episodes: 24000, mean episode reward: 20.548794711615443, agent episode reward: [11.98, 11.98, 11.98, -15.391205288384556], time: 161.724
steps: 624975, episodes: 25000, mean episode reward: 18.12121473082929, agent episode reward: [10.82, 10.82, 10.82, -14.338785269170709], time: 163.442
steps: 649975, episodes: 26000, mean episode reward: 15.950374673964834, agent episode reward: [10.43, 10.43, 10.43, -15.339625326035167], time: 160.583
steps: 674975, episodes: 27000, mean episode reward: 16.510015113448567, agent episode reward: [10.72, 10.72, 10.72, -15.649984886551433], time: 162.242
steps: 699975, episodes: 28000, mean episode reward: 18.393335887941152, agent episode reward: [11.26, 11.26, 11.26, -15.386664112058847], time: 162.573
steps: 724975, episodes: 29000, mean episode reward: 18.51462067535137, agent episode reward: [11.36, 11.36, 11.36, -15.565379324648632], time: 161.791
steps: 749975, episodes: 30000, mean episode reward: 22.328465896190167, agent episode reward: [13.18, 13.18, 13.18, -17.211534103809836], time: 178.258
steps: 774975, episodes: 31000, mean episode reward: 24.291320065962086, agent episode reward: [13.99, 13.99, 13.99, -17.678679934037916], time: 177.77
steps: 799975, episodes: 32000, mean episode reward: 27.5571775248627, agent episode reward: [15.65, 15.65, 15.65, -19.3928224751373], time: 162.543
steps: 824975, episodes: 33000, mean episode reward: 27.169847042643653, agent episode reward: [15.5, 15.5, 15.5, -19.330152957356347], time: 161.151
steps: 849975, episodes: 34000, mean episode reward: 33.750489043508715, agent episode reward: [18.69, 18.69, 18.69, -22.31951095649129], time: 160.999
steps: 874975, episodes: 35000, mean episode reward: 34.18910865976088, agent episode reward: [18.61, 18.61, 18.61, -21.64089134023912], time: 162.113
steps: 899975, episodes: 36000, mean episode reward: 42.56979879700976, agent episode reward: [22.77, 22.77, 22.77, -25.74020120299024], time: 163.235
steps: 924975, episodes: 37000, mean episode reward: 40.824968143848636, agent episode reward: [22.2, 22.2, 22.2, -25.77503185615136], time: 162.692
steps: 949975, episodes: 38000, mean episode reward: 37.29341289976497, agent episode reward: [20.42, 20.42, 20.42, -23.966587100235035], time: 162.49
steps: 974975, episodes: 39000, mean episode reward: 34.683670226987516, agent episode reward: [19.3, 19.3, 19.3, -23.216329773012482], time: 162.494
steps: 999975, episodes: 40000, mean episode reward: 31.377557530164292, agent episode reward: [18.4, 18.4, 18.4, -23.822442469835703], time: 161.066
steps: 1024975, episodes: 41000, mean episode reward: 25.987820661063036, agent episode reward: [15.6, 15.6, 15.6, -20.812179338936964], time: 161.443
steps: 1049975, episodes: 42000, mean episode reward: 21.568762443529618, agent episode reward: [14.19, 14.19, 14.19, -21.001237556470382], time: 162.198
steps: 1074975, episodes: 43000, mean episode reward: 18.00022627453592, agent episode reward: [12.37, 12.37, 12.37, -19.10977372546408], time: 161.447
steps: 1099975, episodes: 44000, mean episode reward: 16.6766373077475, agent episode reward: [11.66, 11.66, 11.66, -18.3033626922525], time: 162.777
steps: 1124975, episodes: 45000, mean episode reward: 15.451295794830083, agent episode reward: [11.22, 11.22, 11.22, -18.208704205169916], time: 164.406
steps: 1149975, episodes: 46000, mean episode reward: 17.07404445011349, agent episode reward: [11.56, 11.56, 11.56, -17.60595554988651], time: 163.175
steps: 1174975, episodes: 47000, mean episode reward: 20.93444807523048, agent episode reward: [13.27, 13.27, 13.27, -18.875551924769518], time: 162.971
steps: 1199975, episodes: 48000, mean episode reward: 20.425667824503318, agent episode reward: [13.08, 13.08, 13.08, -18.81433217549668], time: 163.06
steps: 1224975, episodes: 49000, mean episode reward: 18.381325625443715, agent episode reward: [12.06, 12.06, 12.06, -17.798674374556285], time: 162.808
steps: 1249975, episodes: 50000, mean episode reward: 24.982291527990913, agent episode reward: [14.79, 14.79, 14.79, -19.387708472009088], time: 162.89
steps: 1274975, episodes: 51000, mean episode reward: 21.456113417625026, agent episode reward: [13.27, 13.27, 13.27, -18.353886582374972], time: 163.564
steps: 1299975, episodes: 52000, mean episode reward: 22.253432574791667, agent episode reward: [13.22, 13.22, 13.22, -17.406567425208333], time: 163.103
steps: 1324975, episodes: 53000, mean episode reward: 24.444067485045583, agent episode reward: [14.28, 14.28, 14.28, -18.395932514954417], time: 162.779
steps: 1349975, episodes: 54000, mean episode reward: 26.05095431063975, agent episode reward: [14.94, 14.94, 14.94, -18.76904568936025], time: 162.798
steps: 1374975, episodes: 55000, mean episode reward: 26.652795889316458, agent episode reward: [15.55, 15.55, 15.55, -19.99720411068354], time: 163.411
steps: 1399975, episodes: 56000, mean episode reward: 23.45157082828137, agent episode reward: [13.82, 13.82, 13.82, -18.008429171718635], time: 162.465
steps: 1424975, episodes: 57000, mean episode reward: 27.19019708731171, agent episode reward: [15.92, 15.92, 15.92, -20.56980291268829], time: 161.644
steps: 1449975, episodes: 58000, mean episode reward: 25.212565077023655, agent episode reward: [14.97, 14.97, 14.97, -19.697434922976345], time: 157.691
steps: 1474975, episodes: 59000, mean episode reward: 24.084301051354366, agent episode reward: [14.36, 14.36, 14.36, -18.995698948645636], time: 153.675
steps: 1499975, episodes: 60000, mean episode reward: 24.404992742474917, agent episode reward: [14.5, 14.5, 14.5, -19.095007257525083], time: 131.716
...Finished total of 60001 episodes.
