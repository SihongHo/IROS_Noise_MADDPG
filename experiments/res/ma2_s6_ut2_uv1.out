Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -23.639303986505883, agent episode reward: [-24.083395533411892, 0.2220457734530048, 0.2220457734530048], time: 76.497
steps: 49975, episodes: 2000, mean episode reward: -7.844194088240016, agent episode reward: [-11.03336229581295, 1.5945841037864665, 1.5945841037864665], time: 109.856
steps: 74975, episodes: 3000, mean episode reward: -3.9209258038933896, agent episode reward: [-14.046294606605246, 5.062684401355929, 5.062684401355929], time: 108.605
steps: 99975, episodes: 4000, mean episode reward: 12.265806799024801, agent episode reward: [-29.575451024952145, 20.920628911988477, 20.920628911988477], time: 108.908
steps: 124975, episodes: 5000, mean episode reward: 17.412278830962816, agent episode reward: [-23.35123411656736, 20.381756473765083, 20.381756473765083], time: 109.37
steps: 149975, episodes: 6000, mean episode reward: 26.462779896451863, agent episode reward: [-30.309231016019545, 28.386005456235704, 28.386005456235704], time: 109.567
steps: 174975, episodes: 7000, mean episode reward: 17.170111087860707, agent episode reward: [-20.944089442447357, 19.057100265154034, 19.057100265154034], time: 109.491
steps: 199975, episodes: 8000, mean episode reward: 16.587420171794214, agent episode reward: [-20.41653627548079, 18.501978223637504, 18.501978223637504], time: 109.863
steps: 224975, episodes: 9000, mean episode reward: 17.721487035383205, agent episode reward: [-21.8182163736688, 19.769851704526, 19.769851704526], time: 110.234
steps: 249975, episodes: 10000, mean episode reward: 15.687981456059457, agent episode reward: [-20.28705473913856, 17.987518097599008, 17.987518097599008], time: 110.031
steps: 274975, episodes: 11000, mean episode reward: 16.908589054092563, agent episode reward: [-22.916549772210896, 19.91256941315173, 19.91256941315173], time: 110.565
steps: 299975, episodes: 12000, mean episode reward: 5.3220551972459305, agent episode reward: [-16.117837981294443, 10.719946589270188, 10.719946589270188], time: 110.11
steps: 324975, episodes: 13000, mean episode reward: 7.0976048904366, agent episode reward: [-11.396605129544508, 9.247105009990554, 9.247105009990554], time: 109.588
steps: 349975, episodes: 14000, mean episode reward: 7.23513889024648, agent episode reward: [-12.193882110047555, 9.714510500147018, 9.714510500147018], time: 109.473
steps: 374975, episodes: 15000, mean episode reward: 16.616723526530606, agent episode reward: [-20.26915316388552, 18.442938345208063, 18.442938345208063], time: 109.995
steps: 399975, episodes: 16000, mean episode reward: 22.349580049714422, agent episode reward: [-25.727953209358397, 24.03876662953641, 24.03876662953641], time: 110.974
steps: 424975, episodes: 17000, mean episode reward: 23.87826452702503, agent episode reward: [-27.621936314181358, 25.750100420603193, 25.750100420603193], time: 110.793
steps: 449975, episodes: 18000, mean episode reward: 18.74643785149687, agent episode reward: [-31.410990128940792, 25.078713990218834, 25.078713990218834], time: 110.596
steps: 474975, episodes: 19000, mean episode reward: 14.44701875172923, agent episode reward: [-34.513403482784994, 24.480211117257117, 24.480211117257117], time: 110.664
steps: 499975, episodes: 20000, mean episode reward: 27.42046153281824, agent episode reward: [-34.786084664043834, 31.103273098431035, 31.103273098431035], time: 109.921
steps: 524975, episodes: 21000, mean episode reward: 30.77877498509075, agent episode reward: [-33.79017354381549, 32.284474264453124, 32.284474264453124], time: 109.805
steps: 549975, episodes: 22000, mean episode reward: 25.28556664804439, agent episode reward: [-28.336482857252705, 26.811024752648546, 26.811024752648546], time: 109.833
steps: 574975, episodes: 23000, mean episode reward: 22.346515758164518, agent episode reward: [-25.530529737911507, 23.93852274803801, 23.93852274803801], time: 110.482
