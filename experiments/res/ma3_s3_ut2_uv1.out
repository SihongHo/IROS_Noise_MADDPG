Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -26.953729995668386, agent episode reward: [1.1873731186937302, -28.141103114362114], time: 42.216
steps: 49975, episodes: 2000, mean episode reward: -24.647698043943407, agent episode reward: [-6.755600784126965, -17.892097259816442], time: 51.368
steps: 74975, episodes: 3000, mean episode reward: -13.71271591434385, agent episode reward: [-5.808077128738399, -7.9046387856054485], time: 52.989
steps: 99975, episodes: 4000, mean episode reward: -11.04599962071683, agent episode reward: [-3.8782446867719247, -7.167754933944904], time: 63.513
steps: 124975, episodes: 5000, mean episode reward: -10.217667472603129, agent episode reward: [-3.0359761605816464, -7.181691312021483], time: 64.512
steps: 149975, episodes: 6000, mean episode reward: -10.543086436866925, agent episode reward: [-3.047821667827632, -7.495264769039295], time: 64.061
steps: 174975, episodes: 7000, mean episode reward: -9.85359253010335, agent episode reward: [-2.282872207745297, -7.570720322358055], time: 64.918
steps: 199975, episodes: 8000, mean episode reward: -10.223660817087431, agent episode reward: [-2.841587001862137, -7.3820738152252945], time: 64.909
steps: 224975, episodes: 9000, mean episode reward: -10.28992565247017, agent episode reward: [-2.73517508918446, -7.554750563285712], time: 64.539
steps: 249975, episodes: 10000, mean episode reward: -9.953875702587549, agent episode reward: [-2.6518315544936595, -7.302044148093889], time: 64.609
steps: 274975, episodes: 11000, mean episode reward: -9.686780822184236, agent episode reward: [-2.122913062545828, -7.563867759638405], time: 64.686
steps: 299975, episodes: 12000, mean episode reward: -9.93359303511111, agent episode reward: [-2.283892175688288, -7.649700859422824], time: 64.313
steps: 324975, episodes: 13000, mean episode reward: -9.886704231033361, agent episode reward: [-2.0930412870025665, -7.793662944030796], time: 64.81
steps: 349975, episodes: 14000, mean episode reward: -10.112607443479448, agent episode reward: [-2.070334820166299, -8.04227262331315], time: 64.578
steps: 374975, episodes: 15000, mean episode reward: -9.910527425268404, agent episode reward: [-1.683507890301654, -8.227019534966749], time: 64.954
steps: 399975, episodes: 16000, mean episode reward: -10.0437912923939, agent episode reward: [-1.7202085863870968, -8.323582706006803], time: 65.033
steps: 424975, episodes: 17000, mean episode reward: -9.876455227325426, agent episode reward: [-1.3030713433556247, -8.5733838839698], time: 65.225
steps: 449975, episodes: 18000, mean episode reward: -10.399304597547493, agent episode reward: [-1.530762734159153, -8.868541863388343], time: 65.171
steps: 474975, episodes: 19000, mean episode reward: -10.2060127798295, agent episode reward: [-1.9901181763291407, -8.215894603500358], time: 64.572
steps: 499975, episodes: 20000, mean episode reward: -10.209793462858128, agent episode reward: [-1.89579035552598, -8.314003107332148], time: 64.549
steps: 524975, episodes: 21000, mean episode reward: -10.00757218329632, agent episode reward: [-1.7182238181510021, -8.289348365145319], time: 64.269
steps: 549975, episodes: 22000, mean episode reward: -10.381261771750491, agent episode reward: [-2.258682476272865, -8.122579295477625], time: 66.371
steps: 574975, episodes: 23000, mean episode reward: -10.51963498230737, agent episode reward: [-2.3674740771289633, -8.152160905178405], time: 67.13
steps: 599975, episodes: 24000, mean episode reward: -9.962157223192929, agent episode reward: [-1.9095488439557744, -8.052608379237153], time: 68.531
steps: 624975, episodes: 25000, mean episode reward: -10.284537461043758, agent episode reward: [-2.053737658151884, -8.230799802891877], time: 69.463
steps: 649975, episodes: 26000, mean episode reward: -10.23077186902926, agent episode reward: [-1.8372152832962083, -8.393556585733052], time: 69.624
steps: 674975, episodes: 27000, mean episode reward: -10.470064923381997, agent episode reward: [-1.6353058782598273, -8.83475904512217], time: 69.472
steps: 699975, episodes: 28000, mean episode reward: -9.677551497773326, agent episode reward: [-1.2088597492232938, -8.468691748550032], time: 69.712
steps: 724975, episodes: 29000, mean episode reward: -9.548241494149272, agent episode reward: [-1.3138724318610386, -8.234369062288234], time: 69.394
steps: 749975, episodes: 30000, mean episode reward: -9.656173288083112, agent episode reward: [-1.4719574289782185, -8.184215859104892], time: 69.299
steps: 774975, episodes: 31000, mean episode reward: -10.314985605534092, agent episode reward: [-2.0417786900030066, -8.273206915531086], time: 69.133
steps: 799975, episodes: 32000, mean episode reward: -9.69633311715017, agent episode reward: [-1.7181378577093291, -7.97819525944084], time: 69.329
steps: 824975, episodes: 33000, mean episode reward: -9.732195357739947, agent episode reward: [-2.0323625921138113, -7.6998327656261365], time: 69.387
steps: 849975, episodes: 34000, mean episode reward: -10.096491286341204, agent episode reward: [-2.1465306900555934, -7.949960596285609], time: 69.273
steps: 874975, episodes: 35000, mean episode reward: -10.071670610774518, agent episode reward: [-2.4666583497175343, -7.6050122610569835], time: 68.919
steps: 899975, episodes: 36000, mean episode reward: -9.757204529131151, agent episode reward: [-2.469359649908477, -7.287844879222676], time: 69.919
steps: 924975, episodes: 37000, mean episode reward: -9.754044282380047, agent episode reward: [-2.416936307947086, -7.337107974432963], time: 68.997
steps: 949975, episodes: 38000, mean episode reward: -9.651066605019437, agent episode reward: [-2.2162285865034352, -7.434838018516003], time: 69.122
steps: 974975, episodes: 39000, mean episode reward: -9.458469995377461, agent episode reward: [-2.1342124473674637, -7.324257548009997], time: 69.268
steps: 999975, episodes: 40000, mean episode reward: -10.014223301169439, agent episode reward: [-2.3116360371502678, -7.702587264019172], time: 68.911
steps: 1024975, episodes: 41000, mean episode reward: -9.765315338375043, agent episode reward: [-1.980279124168876, -7.785036214206166], time: 68.899
steps: 1049975, episodes: 42000, mean episode reward: -9.63785456706581, agent episode reward: [-1.3032341015561626, -8.334620465509648], time: 69.275
steps: 1074975, episodes: 43000, mean episode reward: -9.984360154958928, agent episode reward: [-1.279469990666747, -8.70489016429218], time: 69.365
steps: 1099975, episodes: 44000, mean episode reward: -9.750355562502634, agent episode reward: [-0.42653323113806196, -9.323822331364571], time: 69.52
steps: 1124975, episodes: 45000, mean episode reward: -9.74979316983034, agent episode reward: [0.08973465598040747, -9.839527825810746], time: 70.554
steps: 1149975, episodes: 46000, mean episode reward: -9.780021142118859, agent episode reward: [-0.8168616231662567, -8.963159518952603], time: 70.059
steps: 1174975, episodes: 47000, mean episode reward: -10.210211080588374, agent episode reward: [-1.6719181480707639, -8.538292932517612], time: 69.634
steps: 1199975, episodes: 48000, mean episode reward: -9.5260049958079, agent episode reward: [-1.1763213194740767, -8.349683676333822], time: 69.326
steps: 1224975, episodes: 49000, mean episode reward: -9.73544546225866, agent episode reward: [-1.243409613359425, -8.492035848899235], time: 70.05
steps: 1249975, episodes: 50000, mean episode reward: -9.968190515915595, agent episode reward: [-1.9200640295826343, -8.04812648633296], time: 69.747
steps: 1274975, episodes: 51000, mean episode reward: -9.592642479883201, agent episode reward: [-1.6445639658637232, -7.948078514019477], time: 69.589
steps: 1299975, episodes: 52000, mean episode reward: -9.900329017143104, agent episode reward: [-1.6533686813920703, -8.246960335751034], time: 69.348
steps: 1324975, episodes: 53000, mean episode reward: -10.035696331022603, agent episode reward: [-2.1794138899530715, -7.856282441069531], time: 69.523
steps: 1349975, episodes: 54000, mean episode reward: -9.669099218824599, agent episode reward: [-1.62265440772422, -8.046444811100377], time: 69.557
steps: 1374975, episodes: 55000, mean episode reward: -9.967962811708857, agent episode reward: [-1.799807298592871, -8.168155513115986], time: 69.234
steps: 1399975, episodes: 56000, mean episode reward: -9.832838104863015, agent episode reward: [-0.8568607741299308, -8.975977330733084], time: 69.824
steps: 1424975, episodes: 57000, mean episode reward: -10.040978896647797, agent episode reward: [-0.45871641599857643, -9.58226248064922], time: 68.866
steps: 1449975, episodes: 58000, mean episode reward: -10.277569095213886, agent episode reward: [-0.34614353526309105, -9.931425559950796], time: 69.071
steps: 1474975, episodes: 59000, mean episode reward: -10.524371355033963, agent episode reward: [-0.9974028536496115, -9.526968501384351], time: 68.834
steps: 1499975, episodes: 60000, mean episode reward: -10.617159918016643, agent episode reward: [-0.7223709517171516, -9.894788966299494], time: 64.602
...Finished total of 60001 episodes.
