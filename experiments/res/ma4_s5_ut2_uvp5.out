Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -1.8659958017616975, agent episode reward: [2.72, 2.72, 2.72, -10.025995801761699], time: 121.404
steps: 49975, episodes: 2000, mean episode reward: 0.9090335494518503, agent episode reward: [4.26, 4.26, 4.26, -11.87096645054815], time: 159.856
steps: 74975, episodes: 3000, mean episode reward: 5.927492376315717, agent episode reward: [3.66, 3.66, 3.66, -5.052507623684281], time: 160.622
steps: 99975, episodes: 4000, mean episode reward: 8.992227947233504, agent episode reward: [4.98, 4.98, 4.98, -5.947772052766498], time: 160.85
steps: 124975, episodes: 5000, mean episode reward: 13.535856600437715, agent episode reward: [6.97, 6.97, 6.97, -7.374143399562285], time: 161.369
steps: 149975, episodes: 6000, mean episode reward: 23.276677027903784, agent episode reward: [11.85, 11.85, 11.85, -12.273322972096215], time: 161.151
steps: 174975, episodes: 7000, mean episode reward: 28.67553439425149, agent episode reward: [14.64, 14.64, 14.64, -15.244465605748509], time: 162.26
steps: 199975, episodes: 8000, mean episode reward: 39.71085970273672, agent episode reward: [20.43, 20.43, 20.43, -21.57914029726328], time: 160.643
steps: 224975, episodes: 9000, mean episode reward: 24.011017511400432, agent episode reward: [13.14, 13.14, 13.14, -15.408982488599568], time: 161.57
steps: 249975, episodes: 10000, mean episode reward: 12.980735434811177, agent episode reward: [8.75, 8.75, 8.75, -13.269264565188823], time: 161.576
steps: 274975, episodes: 11000, mean episode reward: 10.227816538046145, agent episode reward: [7.77, 7.77, 7.77, -13.082183461953855], time: 162.266
steps: 299975, episodes: 12000, mean episode reward: 13.490142733669726, agent episode reward: [8.74, 8.74, 8.74, -12.729857266330272], time: 161.675
steps: 324975, episodes: 13000, mean episode reward: 17.233821569295497, agent episode reward: [10.17, 10.17, 10.17, -13.276178430704501], time: 160.923
steps: 349975, episodes: 14000, mean episode reward: 27.454751001666132, agent episode reward: [15.54, 15.54, 15.54, -19.16524899833387], time: 161.956
steps: 374975, episodes: 15000, mean episode reward: 33.08403403498316, agent episode reward: [18.29, 18.29, 18.29, -21.785965965016832], time: 161.303
steps: 399975, episodes: 16000, mean episode reward: 27.713779998247084, agent episode reward: [16.23, 16.23, 16.23, -20.976220001752917], time: 162.159
steps: 424975, episodes: 17000, mean episode reward: 20.25558808095289, agent episode reward: [12.94, 12.94, 12.94, -18.56441191904711], time: 162.636
steps: 449975, episodes: 18000, mean episode reward: 15.159570720672322, agent episode reward: [10.73, 10.73, 10.73, -17.030429279327677], time: 162.814
steps: 474975, episodes: 19000, mean episode reward: 13.753522976426273, agent episode reward: [9.67, 9.67, 9.67, -15.256477023573726], time: 161.93
steps: 499975, episodes: 20000, mean episode reward: 13.39280313863039, agent episode reward: [8.69, 8.69, 8.69, -12.67719686136961], time: 161.673
steps: 524975, episodes: 21000, mean episode reward: 14.483860183103596, agent episode reward: [9.27, 9.27, 9.27, -13.326139816896404], time: 162.615
steps: 549975, episodes: 22000, mean episode reward: 14.697695264361839, agent episode reward: [9.18, 9.18, 9.18, -12.84230473563816], time: 161.795
steps: 574975, episodes: 23000, mean episode reward: 16.2570161505828, agent episode reward: [9.89, 9.89, 9.89, -13.4129838494172], time: 162.914
steps: 599975, episodes: 24000, mean episode reward: 20.626553882300797, agent episode reward: [12.09, 12.09, 12.09, -15.643446117699204], time: 161.207
steps: 624975, episodes: 25000, mean episode reward: 25.305683519497368, agent episode reward: [14.31, 14.31, 14.31, -17.62431648050263], time: 160.824
steps: 649975, episodes: 26000, mean episode reward: 33.951599746830645, agent episode reward: [18.55, 18.55, 18.55, -21.698400253169353], time: 161.883
steps: 674975, episodes: 27000, mean episode reward: 36.45605913252634, agent episode reward: [19.89, 19.89, 19.89, -23.213940867473656], time: 161.705
steps: 699975, episodes: 28000, mean episode reward: 39.1499389788875, agent episode reward: [21.16, 21.16, 21.16, -24.330061021112506], time: 163.063
steps: 724975, episodes: 29000, mean episode reward: 38.02845009946363, agent episode reward: [21.38, 21.38, 21.38, -26.111549900536367], time: 163.903
steps: 749975, episodes: 30000, mean episode reward: 49.30736553379708, agent episode reward: [27.07, 27.07, 27.07, -31.90263446620291], time: 161.831
steps: 774975, episodes: 31000, mean episode reward: 56.471843604055934, agent episode reward: [30.29, 30.29, 30.29, -34.39815639594407], time: 162.073
steps: 799975, episodes: 32000, mean episode reward: 56.3767557082666, agent episode reward: [29.93, 29.93, 29.93, -33.4132442917334], time: 161.653
steps: 824975, episodes: 33000, mean episode reward: 57.478078359209896, agent episode reward: [30.56, 30.56, 30.56, -34.201921640790104], time: 161.304
steps: 849975, episodes: 34000, mean episode reward: 53.71960007879914, agent episode reward: [28.75, 28.75, 28.75, -32.53039992120086], time: 161.73
steps: 874975, episodes: 35000, mean episode reward: 46.990118584357944, agent episode reward: [25.72, 25.72, 25.72, -30.169881415642056], time: 162.068
steps: 899975, episodes: 36000, mean episode reward: 48.71556263248147, agent episode reward: [26.56, 26.56, 26.56, -30.964437367518528], time: 163.367
steps: 924975, episodes: 37000, mean episode reward: 39.49721206072911, agent episode reward: [22.31, 22.31, 22.31, -27.43278793927089], time: 162.086
steps: 949975, episodes: 38000, mean episode reward: 33.781554409963896, agent episode reward: [20.04, 20.04, 20.04, -26.338445590036102], time: 162.234
steps: 974975, episodes: 39000, mean episode reward: 34.924355676415374, agent episode reward: [20.6, 20.6, 20.6, -26.87564432358463], time: 162.22
steps: 999975, episodes: 40000, mean episode reward: 31.788802956358662, agent episode reward: [19.17, 19.17, 19.17, -25.72119704364134], time: 161.619
steps: 1024975, episodes: 41000, mean episode reward: 26.388093639461452, agent episode reward: [16.53, 16.53, 16.53, -23.201906360538544], time: 161.932
steps: 1049975, episodes: 42000, mean episode reward: 28.59607732803194, agent episode reward: [17.24, 17.24, 17.24, -23.123922671968064], time: 162.246
steps: 1074975, episodes: 43000, mean episode reward: 27.919198035847437, agent episode reward: [17.26, 17.26, 17.26, -23.860801964152564], time: 161.46
steps: 1099975, episodes: 44000, mean episode reward: 27.26704565204344, agent episode reward: [16.85, 16.85, 16.85, -23.28295434795656], time: 160.981
steps: 1124975, episodes: 45000, mean episode reward: 31.5313343081681, agent episode reward: [18.71, 18.71, 18.71, -24.5986656918319], time: 163.432
steps: 1149975, episodes: 46000, mean episode reward: 31.706525122157814, agent episode reward: [18.38, 18.38, 18.38, -23.433474877842187], time: 161.574
steps: 1174975, episodes: 47000, mean episode reward: 24.343621010669516, agent episode reward: [15.33, 15.33, 15.33, -21.646378989330483], time: 161.577
steps: 1199975, episodes: 48000, mean episode reward: 30.587473200658852, agent episode reward: [18.26, 18.26, 18.26, -24.19252679934115], time: 161.856
steps: 1224975, episodes: 49000, mean episode reward: 32.45915429379367, agent episode reward: [18.85, 18.85, 18.85, -24.09084570620633], time: 160.326
steps: 1249975, episodes: 50000, mean episode reward: 26.704475072768805, agent episode reward: [16.06, 16.06, 16.06, -21.475524927231195], time: 161.837
steps: 1274975, episodes: 51000, mean episode reward: 30.855648589624348, agent episode reward: [17.87, 17.87, 17.87, -22.754351410375655], time: 160.42
steps: 1299975, episodes: 52000, mean episode reward: 26.488374433683234, agent episode reward: [15.98, 15.98, 15.98, -21.451625566316764], time: 158.601
steps: 1324975, episodes: 53000, mean episode reward: 27.24805711868276, agent episode reward: [16.21, 16.21, 16.21, -21.381942881317244], time: 158.014
steps: 1349975, episodes: 54000, mean episode reward: 29.670798088090656, agent episode reward: [17.24, 17.24, 17.24, -22.04920191190935], time: 158.028
steps: 1374975, episodes: 55000, mean episode reward: 30.041109072259385, agent episode reward: [17.36, 17.36, 17.36, -22.038890927740617], time: 159.768
steps: 1399975, episodes: 56000, mean episode reward: 23.6662366488017, agent episode reward: [14.5, 14.5, 14.5, -19.8337633511983], time: 158.233
steps: 1424975, episodes: 57000, mean episode reward: 30.231958223734345, agent episode reward: [17.81, 17.81, 17.81, -23.198041776265654], time: 158.122
steps: 1449975, episodes: 58000, mean episode reward: 23.874837694399467, agent episode reward: [14.86, 14.86, 14.86, -20.705162305600535], time: 158.554
steps: 1474975, episodes: 59000, mean episode reward: 27.222210563160516, agent episode reward: [16.7, 16.7, 16.7, -22.87778943683949], time: 158.793
steps: 1499975, episodes: 60000, mean episode reward: 25.476258886937664, agent episode reward: [15.48, 15.48, 15.48, -20.963741113062337], time: 157.573
...Finished total of 60001 episodes.
