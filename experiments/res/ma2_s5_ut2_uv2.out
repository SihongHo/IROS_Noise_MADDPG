Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -2.2855389909161614, agent episode reward: [2.15, 2.15, 2.15, -8.73553899091616], time: 110.676
steps: 49975, episodes: 2000, mean episode reward: 0.016911536389731738, agent episode reward: [4.03, 4.03, 4.03, -12.073088463610267], time: 148.738
steps: 74975, episodes: 3000, mean episode reward: 9.663674366981926, agent episode reward: [5.59, 5.59, 5.59, -7.1063256330180735], time: 152.356
steps: 99975, episodes: 4000, mean episode reward: 8.947815945229767, agent episode reward: [5.15, 5.15, 5.15, -6.502184054770234], time: 152.496
steps: 124975, episodes: 5000, mean episode reward: 11.949666423772646, agent episode reward: [6.29, 6.29, 6.29, -6.920333576227355], time: 152.735
steps: 149975, episodes: 6000, mean episode reward: 21.526310253639227, agent episode reward: [10.93, 10.93, 10.93, -11.26368974636077], time: 152.394
steps: 174975, episodes: 7000, mean episode reward: 52.602860750117536, agent episode reward: [26.52, 26.52, 26.52, -26.957139249882463], time: 152.828
steps: 199975, episodes: 8000, mean episode reward: 67.35245318345162, agent episode reward: [34.34, 34.34, 34.34, -35.66754681654839], time: 151.992
steps: 224975, episodes: 9000, mean episode reward: 26.113132769855305, agent episode reward: [15.5, 15.5, 15.5, -20.386867230144695], time: 152.198
steps: 249975, episodes: 10000, mean episode reward: 6.887482257382908, agent episode reward: [6.71, 6.71, 6.71, -13.242517742617093], time: 152.41
steps: 274975, episodes: 11000, mean episode reward: 1.6960514143783074, agent episode reward: [3.69, 3.69, 3.69, -9.373948585621692], time: 153.106
steps: 299975, episodes: 12000, mean episode reward: 4.750083202225659, agent episode reward: [4.34, 4.34, 4.34, -8.269916797774341], time: 153.293
steps: 324975, episodes: 13000, mean episode reward: 7.617396972118709, agent episode reward: [5.43, 5.43, 5.43, -8.67260302788129], time: 152.836
steps: 349975, episodes: 14000, mean episode reward: 8.083080155907348, agent episode reward: [5.57, 5.57, 5.57, -8.626919844092653], time: 153.181
steps: 374975, episodes: 15000, mean episode reward: 10.640077651342624, agent episode reward: [6.71, 6.71, 6.71, -9.489922348657375], time: 151.924
steps: 399975, episodes: 16000, mean episode reward: 11.959311153271177, agent episode reward: [7.49, 7.49, 7.49, -10.510688846728822], time: 152.696
steps: 424975, episodes: 17000, mean episode reward: 11.4093952029407, agent episode reward: [7.11, 7.11, 7.11, -9.9206047970593], time: 157.298
steps: 449975, episodes: 18000, mean episode reward: 17.724387183313382, agent episode reward: [9.84, 9.84, 9.84, -11.795612816686619], time: 161.503
steps: 474975, episodes: 19000, mean episode reward: 15.786281759917074, agent episode reward: [9.01, 9.01, 9.01, -11.243718240082925], time: 160.9
steps: 499975, episodes: 20000, mean episode reward: 21.359182935444878, agent episode reward: [11.47, 11.47, 11.47, -13.050817064555122], time: 160.591
steps: 524975, episodes: 21000, mean episode reward: 31.60761099293594, agent episode reward: [16.45, 16.45, 16.45, -17.742389007064062], time: 160.695
steps: 549975, episodes: 22000, mean episode reward: 35.072581786175725, agent episode reward: [18.62, 18.62, 18.62, -20.787418213824274], time: 160.157
steps: 574975, episodes: 23000, mean episode reward: 47.84009213687125, agent episode reward: [25.07, 25.07, 25.07, -27.36990786312875], time: 162.937
steps: 599975, episodes: 24000, mean episode reward: 41.07736136852874, agent episode reward: [21.79, 21.79, 21.79, -24.292638631471252], time: 162.714
steps: 624975, episodes: 25000, mean episode reward: 47.56182997878039, agent episode reward: [25.7, 25.7, 25.7, -29.538170021219617], time: 161.826
steps: 649975, episodes: 26000, mean episode reward: 30.580928502963552, agent episode reward: [18.12, 18.12, 18.12, -23.77907149703645], time: 162.072
steps: 674975, episodes: 27000, mean episode reward: 22.43558653959893, agent episode reward: [14.26, 14.26, 14.26, -20.344413460401068], time: 161.062
steps: 699975, episodes: 28000, mean episode reward: 23.522303416985906, agent episode reward: [14.51, 14.51, 14.51, -20.007696583014095], time: 161.977
steps: 724975, episodes: 29000, mean episode reward: 18.697677349522404, agent episode reward: [11.98, 11.98, 11.98, -17.242322650477597], time: 162.982
steps: 749975, episodes: 30000, mean episode reward: 18.1806788398589, agent episode reward: [11.03, 11.03, 11.03, -14.909321160141104], time: 177.799
steps: 774975, episodes: 31000, mean episode reward: 15.451433784590444, agent episode reward: [9.84, 9.84, 9.84, -14.068566215409556], time: 177.259
steps: 799975, episodes: 32000, mean episode reward: 17.627786094354473, agent episode reward: [10.35, 10.35, 10.35, -13.422213905645528], time: 161.169
steps: 824975, episodes: 33000, mean episode reward: 14.90667651521602, agent episode reward: [9.3, 9.3, 9.3, -12.993323484783978], time: 161.722
steps: 849975, episodes: 34000, mean episode reward: 16.63428894607586, agent episode reward: [10.27, 10.27, 10.27, -14.175711053924143], time: 161.486
steps: 874975, episodes: 35000, mean episode reward: 14.311813521367974, agent episode reward: [8.7, 8.7, 8.7, -11.788186478632026], time: 162.053
steps: 899975, episodes: 36000, mean episode reward: 14.998891095778287, agent episode reward: [9.09, 9.09, 9.09, -12.271108904221714], time: 163.121
steps: 924975, episodes: 37000, mean episode reward: 16.76020912655102, agent episode reward: [10.04, 10.04, 10.04, -13.35979087344898], time: 162.201
steps: 949975, episodes: 38000, mean episode reward: 18.577238750688572, agent episode reward: [11.0, 11.0, 11.0, -14.42276124931143], time: 161.763
steps: 974975, episodes: 39000, mean episode reward: 17.68587001228446, agent episode reward: [10.79, 10.79, 10.79, -14.68412998771554], time: 162.196
steps: 999975, episodes: 40000, mean episode reward: 18.524171812639192, agent episode reward: [10.89, 10.89, 10.89, -14.145828187360806], time: 162.386
steps: 1024975, episodes: 41000, mean episode reward: 18.45727232310308, agent episode reward: [10.83, 10.83, 10.83, -14.03272767689692], time: 160.907
steps: 1049975, episodes: 42000, mean episode reward: 19.12685155241166, agent episode reward: [11.32, 11.32, 11.32, -14.833148447588338], time: 160.92
steps: 1074975, episodes: 43000, mean episode reward: 19.40151626179091, agent episode reward: [11.39, 11.39, 11.39, -14.76848373820909], time: 161.927
steps: 1099975, episodes: 44000, mean episode reward: 19.542740494145843, agent episode reward: [11.29, 11.29, 11.29, -14.32725950585416], time: 162.038
steps: 1124975, episodes: 45000, mean episode reward: 21.291715932849872, agent episode reward: [12.73, 12.73, 12.73, -16.89828406715013], time: 163.175
steps: 1149975, episodes: 46000, mean episode reward: 18.96130392021521, agent episode reward: [11.01, 11.01, 11.01, -14.06869607978479], time: 162.195
steps: 1174975, episodes: 47000, mean episode reward: 27.49179799189142, agent episode reward: [15.12, 15.12, 15.12, -17.86820200810858], time: 162.293
steps: 1199975, episodes: 48000, mean episode reward: 29.322164824595266, agent episode reward: [16.56, 16.56, 16.56, -20.357835175404734], time: 161.851
steps: 1224975, episodes: 49000, mean episode reward: 32.485074403116414, agent episode reward: [17.86, 17.86, 17.86, -21.094925596883584], time: 161.622
steps: 1249975, episodes: 50000, mean episode reward: 39.121607740330504, agent episode reward: [21.27, 21.27, 21.27, -24.6883922596695], time: 161.383
steps: 1274975, episodes: 51000, mean episode reward: 39.48615230449652, agent episode reward: [21.8, 21.8, 21.8, -25.913847695503478], time: 162.722
steps: 1299975, episodes: 52000, mean episode reward: 37.69571802621008, agent episode reward: [20.82, 20.82, 20.82, -24.764281973789924], time: 162.095
steps: 1324975, episodes: 53000, mean episode reward: 49.480804723571694, agent episode reward: [26.71, 26.71, 26.71, -30.64919527642831], time: 162.137
steps: 1349975, episodes: 54000, mean episode reward: 45.50808551555104, agent episode reward: [25.18, 25.18, 25.18, -30.031914484448965], time: 162.68
steps: 1374975, episodes: 55000, mean episode reward: 33.497546686580186, agent episode reward: [19.6, 19.6, 19.6, -25.302453313419818], time: 163.955
steps: 1399975, episodes: 56000, mean episode reward: 39.251063012992965, agent episode reward: [22.47, 22.47, 22.47, -28.15893698700704], time: 161.146
steps: 1424975, episodes: 57000, mean episode reward: 40.48324323419022, agent episode reward: [23.14, 23.14, 23.14, -28.936756765809783], time: 162.381
steps: 1449975, episodes: 58000, mean episode reward: 23.491820643121805, agent episode reward: [15.27, 15.27, 15.27, -22.318179356878193], time: 160.747
steps: 1474975, episodes: 59000, mean episode reward: 19.051409292838155, agent episode reward: [13.32, 13.32, 13.32, -20.908590707161846], time: 154.114
steps: 1499975, episodes: 60000, mean episode reward: 20.909624414962767, agent episode reward: [13.95, 13.95, 13.95, -20.940375585037234], time: 147.686
...Finished total of 60001 episodes.
