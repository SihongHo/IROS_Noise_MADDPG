Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -2.2855389909161614, agent episode reward: [2.15, 2.15, 2.15, -8.73553899091616], time: 110.676
steps: 49975, episodes: 2000, mean episode reward: 0.016911536389731738, agent episode reward: [4.03, 4.03, 4.03, -12.073088463610267], time: 148.738
steps: 74975, episodes: 3000, mean episode reward: 9.663674366981926, agent episode reward: [5.59, 5.59, 5.59, -7.1063256330180735], time: 152.356
steps: 99975, episodes: 4000, mean episode reward: 8.947815945229767, agent episode reward: [5.15, 5.15, 5.15, -6.502184054770234], time: 152.496
steps: 124975, episodes: 5000, mean episode reward: 11.949666423772646, agent episode reward: [6.29, 6.29, 6.29, -6.920333576227355], time: 152.735
steps: 149975, episodes: 6000, mean episode reward: 21.526310253639227, agent episode reward: [10.93, 10.93, 10.93, -11.26368974636077], time: 152.394
steps: 174975, episodes: 7000, mean episode reward: 52.602860750117536, agent episode reward: [26.52, 26.52, 26.52, -26.957139249882463], time: 152.828
steps: 199975, episodes: 8000, mean episode reward: 67.35245318345162, agent episode reward: [34.34, 34.34, 34.34, -35.66754681654839], time: 151.992
steps: 224975, episodes: 9000, mean episode reward: 26.113132769855305, agent episode reward: [15.5, 15.5, 15.5, -20.386867230144695], time: 152.198
steps: 249975, episodes: 10000, mean episode reward: 6.887482257382908, agent episode reward: [6.71, 6.71, 6.71, -13.242517742617093], time: 152.41
steps: 274975, episodes: 11000, mean episode reward: 1.6960514143783074, agent episode reward: [3.69, 3.69, 3.69, -9.373948585621692], time: 153.106
steps: 299975, episodes: 12000, mean episode reward: 4.750083202225659, agent episode reward: [4.34, 4.34, 4.34, -8.269916797774341], time: 153.293
steps: 324975, episodes: 13000, mean episode reward: 7.617396972118709, agent episode reward: [5.43, 5.43, 5.43, -8.67260302788129], time: 152.836
steps: 349975, episodes: 14000, mean episode reward: 8.083080155907348, agent episode reward: [5.57, 5.57, 5.57, -8.626919844092653], time: 153.181
steps: 374975, episodes: 15000, mean episode reward: 10.640077651342624, agent episode reward: [6.71, 6.71, 6.71, -9.489922348657375], time: 151.924
steps: 399975, episodes: 16000, mean episode reward: 11.959311153271177, agent episode reward: [7.49, 7.49, 7.49, -10.510688846728822], time: 152.696
steps: 424975, episodes: 17000, mean episode reward: 11.4093952029407, agent episode reward: [7.11, 7.11, 7.11, -9.9206047970593], time: 157.298
steps: 449975, episodes: 18000, mean episode reward: 17.724387183313382, agent episode reward: [9.84, 9.84, 9.84, -11.795612816686619], time: 161.503
steps: 474975, episodes: 19000, mean episode reward: 15.786281759917074, agent episode reward: [9.01, 9.01, 9.01, -11.243718240082925], time: 160.9
steps: 499975, episodes: 20000, mean episode reward: 21.359182935444878, agent episode reward: [11.47, 11.47, 11.47, -13.050817064555122], time: 160.591
steps: 524975, episodes: 21000, mean episode reward: 31.60761099293594, agent episode reward: [16.45, 16.45, 16.45, -17.742389007064062], time: 160.695
steps: 549975, episodes: 22000, mean episode reward: 35.072581786175725, agent episode reward: [18.62, 18.62, 18.62, -20.787418213824274], time: 160.157
steps: 574975, episodes: 23000, mean episode reward: 47.84009213687125, agent episode reward: [25.07, 25.07, 25.07, -27.36990786312875], time: 162.937
steps: 599975, episodes: 24000, mean episode reward: 41.07736136852874, agent episode reward: [21.79, 21.79, 21.79, -24.292638631471252], time: 162.714
steps: 624975, episodes: 25000, mean episode reward: 47.56182997878039, agent episode reward: [25.7, 25.7, 25.7, -29.538170021219617], time: 161.826
steps: 649975, episodes: 26000, mean episode reward: 30.580928502963552, agent episode reward: [18.12, 18.12, 18.12, -23.77907149703645], time: 162.072
steps: 674975, episodes: 27000, mean episode reward: 22.43558653959893, agent episode reward: [14.26, 14.26, 14.26, -20.344413460401068], time: 161.062
steps: 699975, episodes: 28000, mean episode reward: 23.522303416985906, agent episode reward: [14.51, 14.51, 14.51, -20.007696583014095], time: 161.977
steps: 724975, episodes: 29000, mean episode reward: 18.697677349522404, agent episode reward: [11.98, 11.98, 11.98, -17.242322650477597], time: 162.982
steps: 749975, episodes: 30000, mean episode reward: 18.1806788398589, agent episode reward: [11.03, 11.03, 11.03, -14.909321160141104], time: 177.799
steps: 774975, episodes: 31000, mean episode reward: 15.451433784590444, agent episode reward: [9.84, 9.84, 9.84, -14.068566215409556], time: 177.259
steps: 799975, episodes: 32000, mean episode reward: 17.627786094354473, agent episode reward: [10.35, 10.35, 10.35, -13.422213905645528], time: 161.169
steps: 824975, episodes: 33000, mean episode reward: 14.90667651521602, agent episode reward: [9.3, 9.3, 9.3, -12.993323484783978], time: 161.722
