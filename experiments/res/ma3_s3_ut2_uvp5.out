Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -27.021982509387477, agent episode reward: [0.9420906948090915, -27.964073204196573], time: 31.622
steps: 49975, episodes: 2000, mean episode reward: -21.592663083785375, agent episode reward: [-6.00509985535023, -15.587563228435144], time: 50.982
steps: 74975, episodes: 3000, mean episode reward: -13.298469252453211, agent episode reward: [-5.788092565309497, -7.510376687143713], time: 50.597
steps: 99975, episodes: 4000, mean episode reward: -10.79167487107011, agent episode reward: [-3.6133791161306603, -7.178295754939451], time: 54.24
steps: 124975, episodes: 5000, mean episode reward: -10.102979404761376, agent episode reward: [-3.0983393514338307, -7.004640053327544], time: 64.56
steps: 149975, episodes: 6000, mean episode reward: -10.071590003188554, agent episode reward: [-2.815246945716787, -7.256343057471768], time: 63.742
steps: 174975, episodes: 7000, mean episode reward: -9.970717092503843, agent episode reward: [-2.7574214341788585, -7.213295658324985], time: 64.719
steps: 199975, episodes: 8000, mean episode reward: -9.517055938260132, agent episode reward: [-2.2765320881023974, -7.240523850157733], time: 65.032
steps: 224975, episodes: 9000, mean episode reward: -10.037186499627925, agent episode reward: [-2.691239743070505, -7.34594675655742], time: 64.143
steps: 249975, episodes: 10000, mean episode reward: -9.698242615886603, agent episode reward: [-2.410270918424091, -7.28797169746251], time: 64.382
steps: 274975, episodes: 11000, mean episode reward: -9.530065737538312, agent episode reward: [-2.124427082642956, -7.405638654895356], time: 64.415
steps: 299975, episodes: 12000, mean episode reward: -9.345883824280842, agent episode reward: [-1.8213158534099647, -7.524567970870875], time: 65.07
steps: 324975, episodes: 13000, mean episode reward: -9.50731865807344, agent episode reward: [-2.049958298480204, -7.4573603595932365], time: 64.001
steps: 349975, episodes: 14000, mean episode reward: -9.179127846265281, agent episode reward: [-1.66776980048908, -7.5113580457762], time: 64.452
steps: 374975, episodes: 15000, mean episode reward: -9.211632377910886, agent episode reward: [-2.0862325459030306, -7.125399832007855], time: 64.611
steps: 399975, episodes: 16000, mean episode reward: -9.54169540401712, agent episode reward: [-2.363204219108292, -7.178491184908827], time: 64.802
steps: 424975, episodes: 17000, mean episode reward: -9.173415041112785, agent episode reward: [-2.129918469093689, -7.043496572019097], time: 64.786
steps: 449975, episodes: 18000, mean episode reward: -9.716794630821388, agent episode reward: [-2.1648773545940774, -7.551917276227311], time: 64.341
steps: 474975, episodes: 19000, mean episode reward: -9.4743011841561, agent episode reward: [-2.3163687549361245, -7.157932429219977], time: 64.163
steps: 499975, episodes: 20000, mean episode reward: -9.793048765629186, agent episode reward: [-2.341768042635693, -7.451280722993493], time: 64.228
steps: 524975, episodes: 21000, mean episode reward: -9.6996416571145, agent episode reward: [-2.390607005477769, -7.309034651636732], time: 64.987
steps: 549975, episodes: 22000, mean episode reward: -9.723930292012197, agent episode reward: [-2.257329330176244, -7.466600961835953], time: 64.623
steps: 574975, episodes: 23000, mean episode reward: -9.387273496169188, agent episode reward: [-2.1293496864508836, -7.257923809718304], time: 64.989
steps: 599975, episodes: 24000, mean episode reward: -9.285590666654066, agent episode reward: [-1.9923156909984956, -7.293274975655572], time: 67.115
steps: 624975, episodes: 25000, mean episode reward: -9.2288945706205, agent episode reward: [-1.6365355431560624, -7.59235902746444], time: 68.467
steps: 649975, episodes: 26000, mean episode reward: -9.393066650658644, agent episode reward: [-1.5224473720523493, -7.870619278606295], time: 69.265
steps: 674975, episodes: 27000, mean episode reward: -9.490374400884212, agent episode reward: [-1.690510123641621, -7.799864277242592], time: 69.047
steps: 699975, episodes: 28000, mean episode reward: -9.42552656198232, agent episode reward: [-1.7561708110690208, -7.6693557509133], time: 69.287
steps: 724975, episodes: 29000, mean episode reward: -9.481978557158266, agent episode reward: [-1.7682489718438097, -7.713729585314455], time: 68.842
steps: 749975, episodes: 30000, mean episode reward: -9.78874274206985, agent episode reward: [-1.6626304912678331, -8.126112250802016], time: 69.109
steps: 774975, episodes: 31000, mean episode reward: -9.646465146858503, agent episode reward: [-1.7348776586071593, -7.911587488251343], time: 68.852
steps: 799975, episodes: 32000, mean episode reward: -9.658713179878319, agent episode reward: [-1.959707484585929, -7.6990056952923895], time: 68.484
steps: 824975, episodes: 33000, mean episode reward: -9.407374758823048, agent episode reward: [-1.305738508923248, -8.101636249899801], time: 68.523
steps: 849975, episodes: 34000, mean episode reward: -9.484940672589964, agent episode reward: [-1.7215782891364035, -7.76336238345356], time: 68.914
steps: 874975, episodes: 35000, mean episode reward: -9.724753302169193, agent episode reward: [-1.407148463660343, -8.31760483850885], time: 68.906
steps: 899975, episodes: 36000, mean episode reward: -9.655621905586909, agent episode reward: [-1.5872767743961729, -8.068345131190735], time: 70.023
steps: 924975, episodes: 37000, mean episode reward: -9.76137038515829, agent episode reward: [-1.84612742673028, -7.915242958428011], time: 68.906
steps: 949975, episodes: 38000, mean episode reward: -9.70998846254019, agent episode reward: [-2.3322109527065615, -7.377777509833629], time: 68.955
steps: 974975, episodes: 39000, mean episode reward: -9.687340760080856, agent episode reward: [-2.5448521578339585, -7.142488602246896], time: 68.685
steps: 999975, episodes: 40000, mean episode reward: -9.281866436988967, agent episode reward: [-1.9933848098490796, -7.288481627139888], time: 68.942
steps: 1024975, episodes: 41000, mean episode reward: -9.551735197972615, agent episode reward: [-1.9409701478194483, -7.610765050153168], time: 68.874
steps: 1049975, episodes: 42000, mean episode reward: -9.421745017642417, agent episode reward: [-0.44356044474457645, -8.978184572897842], time: 68.693
steps: 1074975, episodes: 43000, mean episode reward: -9.515010544039255, agent episode reward: [-0.006324806999447674, -9.50868573703981], time: 68.665
steps: 1099975, episodes: 44000, mean episode reward: -10.141876561629765, agent episode reward: [-0.22832081937010953, -9.913555742259655], time: 68.45
steps: 1124975, episodes: 45000, mean episode reward: -10.005444123388546, agent episode reward: [-0.6038030332914741, -9.401641090097073], time: 69.719
steps: 1149975, episodes: 46000, mean episode reward: -10.103557801470844, agent episode reward: [0.005344891041167301, -10.108902692512013], time: 68.833
steps: 1174975, episodes: 47000, mean episode reward: -9.615950569714975, agent episode reward: [-0.26087528675298133, -9.355075282961996], time: 68.657
steps: 1199975, episodes: 48000, mean episode reward: -9.689318927765731, agent episode reward: [-0.54677661792263, -9.1425423098431], time: 68.492
steps: 1224975, episodes: 49000, mean episode reward: -9.794236078351373, agent episode reward: [-0.18707875171814448, -9.607157326633226], time: 69.074
steps: 1249975, episodes: 50000, mean episode reward: -9.481955885380627, agent episode reward: [-0.9027605290120531, -8.579195356368574], time: 68.67
steps: 1274975, episodes: 51000, mean episode reward: -9.323070886642107, agent episode reward: [-1.404630455876034, -7.918440430766072], time: 68.794
steps: 1299975, episodes: 52000, mean episode reward: -9.127915478797275, agent episode reward: [-1.2443508372333674, -7.883564641563908], time: 69.378
steps: 1324975, episodes: 53000, mean episode reward: -9.470396377796668, agent episode reward: [-1.624683617532542, -7.845712760264126], time: 68.85
steps: 1349975, episodes: 54000, mean episode reward: -9.671836631872605, agent episode reward: [-1.7788338218738453, -7.893002809998758], time: 69.029
steps: 1374975, episodes: 55000, mean episode reward: -9.116583036954085, agent episode reward: [-1.0671822978710548, -8.04940073908303], time: 68.794
steps: 1399975, episodes: 56000, mean episode reward: -9.181746491329493, agent episode reward: [-1.4009421232705457, -7.780804368058947], time: 69.588
steps: 1424975, episodes: 57000, mean episode reward: -9.278674270506627, agent episode reward: [-1.4879677121814998, -7.790706558325127], time: 68.147
steps: 1449975, episodes: 58000, mean episode reward: -8.76670359283844, agent episode reward: [-0.886819068610718, -7.8798845242277205], time: 68.193
steps: 1474975, episodes: 59000, mean episode reward: -9.235994405170265, agent episode reward: [-0.8770450065512599, -8.358949398619007], time: 68.095
steps: 1499975, episodes: 60000, mean episode reward: -9.413663863007129, agent episode reward: [-0.6916025254149697, -8.722061337592157], time: 68.361
...Finished total of 60001 episodes.
