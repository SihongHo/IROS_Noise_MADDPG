Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -2.5987878319406983, agent episode reward: [2.66, 2.66, 2.66, -10.5787878319407], time: 97.987
steps: 49975, episodes: 2000, mean episode reward: -3.9624021251080213, agent episode reward: [4.27, 4.27, 4.27, -16.77240212510802], time: 184.498
steps: 74975, episodes: 3000, mean episode reward: 9.97662826880318, agent episode reward: [5.29, 5.29, 5.29, -5.89337173119682], time: 187.42
steps: 99975, episodes: 4000, mean episode reward: 9.710526745684147, agent episode reward: [5.15, 5.15, 5.15, -5.739473254315855], time: 187.991
steps: 124975, episodes: 5000, mean episode reward: 13.536016755468289, agent episode reward: [7.07, 7.07, 7.07, -7.67398324453171], time: 187.827
steps: 149975, episodes: 6000, mean episode reward: 24.909870763186262, agent episode reward: [12.68, 12.68, 12.68, -13.130129236813737], time: 187.637
steps: 174975, episodes: 7000, mean episode reward: 52.82580146234607, agent episode reward: [27.36, 27.36, 27.36, -29.25419853765393], time: 188.895
steps: 199975, episodes: 8000, mean episode reward: 20.29539277801721, agent episode reward: [14.62, 14.62, 14.62, -23.564607221982794], time: 187.176
steps: 224975, episodes: 9000, mean episode reward: 4.370276681423669, agent episode reward: [7.26, 7.26, 7.26, -17.40972331857633], time: 188.427
steps: 249975, episodes: 10000, mean episode reward: 9.352469932729852, agent episode reward: [7.95, 7.95, 7.95, -14.497530067270148], time: 187.054
steps: 274975, episodes: 11000, mean episode reward: 11.700797820611243, agent episode reward: [7.6, 7.6, 7.6, -11.099202179388758], time: 189.294
steps: 299975, episodes: 12000, mean episode reward: 15.544420447442258, agent episode reward: [9.0, 9.0, 9.0, -11.455579552557742], time: 188.053
steps: 324975, episodes: 13000, mean episode reward: 22.197791488944254, agent episode reward: [12.11, 12.11, 12.11, -14.132208511055744], time: 188.554
steps: 349975, episodes: 14000, mean episode reward: 29.65246242356872, agent episode reward: [16.01, 16.01, 16.01, -18.377537576431276], time: 187.921
steps: 374975, episodes: 15000, mean episode reward: 27.369040856711653, agent episode reward: [15.36, 15.36, 15.36, -18.710959143288346], time: 187.756
steps: 399975, episodes: 16000, mean episode reward: 24.837611140926192, agent episode reward: [14.56, 14.56, 14.56, -18.842388859073807], time: 187.984
steps: 424975, episodes: 17000, mean episode reward: 30.353700406016735, agent episode reward: [16.9, 16.9, 16.9, -20.346299593983264], time: 189.062
steps: 449975, episodes: 18000, mean episode reward: 31.256596670622464, agent episode reward: [17.66, 17.66, 17.66, -21.723403329377533], time: 189.189
steps: 474975, episodes: 19000, mean episode reward: 33.17641195035718, agent episode reward: [18.61, 18.61, 18.61, -22.65358804964282], time: 187.015
steps: 499975, episodes: 20000, mean episode reward: 28.934811272185442, agent episode reward: [16.61, 16.61, 16.61, -20.89518872781456], time: 188.088
steps: 524975, episodes: 21000, mean episode reward: 32.63484993438273, agent episode reward: [19.0, 19.0, 19.0, -24.36515006561727], time: 187.548
steps: 549975, episodes: 22000, mean episode reward: 26.316174740011828, agent episode reward: [16.53, 16.53, 16.53, -23.27382525998817], time: 188.466
steps: 574975, episodes: 23000, mean episode reward: 22.241905131845787, agent episode reward: [14.49, 14.49, 14.49, -21.228094868154216], time: 188.932
steps: 599975, episodes: 24000, mean episode reward: 21.119486489164373, agent episode reward: [14.28, 14.28, 14.28, -21.720513510835627], time: 187.006
steps: 624975, episodes: 25000, mean episode reward: 20.160823954079284, agent episode reward: [13.06, 13.06, 13.06, -19.01917604592072], time: 188.461
steps: 649975, episodes: 26000, mean episode reward: 20.676235179703852, agent episode reward: [13.49, 13.49, 13.49, -19.793764820296147], time: 187.475
steps: 674975, episodes: 27000, mean episode reward: 26.383814779108427, agent episode reward: [15.91, 15.91, 15.91, -21.346185220891574], time: 187.809
steps: 699975, episodes: 28000, mean episode reward: 29.551884546934037, agent episode reward: [16.85, 16.85, 16.85, -20.998115453065964], time: 188.522
steps: 724975, episodes: 29000, mean episode reward: 30.04374464906724, agent episode reward: [16.69, 16.69, 16.69, -20.026255350932757], time: 189.525
steps: 749975, episodes: 30000, mean episode reward: 33.5928816177529, agent episode reward: [18.64, 18.64, 18.64, -22.327118382247097], time: 187.243
steps: 774975, episodes: 31000, mean episode reward: 36.32003071058827, agent episode reward: [19.87, 19.87, 19.87, -23.289969289411733], time: 187.902
steps: 799975, episodes: 32000, mean episode reward: 37.64832563925142, agent episode reward: [20.45, 20.45, 20.45, -23.70167436074859], time: 187.802
steps: 824975, episodes: 33000, mean episode reward: 36.9503604125662, agent episode reward: [20.46, 20.46, 20.46, -24.429639587433805], time: 188.055
steps: 849975, episodes: 34000, mean episode reward: 37.91272574076819, agent episode reward: [20.71, 20.71, 20.71, -24.21727425923181], time: 188.067
steps: 874975, episodes: 35000, mean episode reward: 39.91534364787135, agent episode reward: [21.68, 21.68, 21.68, -25.124656352128646], time: 189.208
steps: 899975, episodes: 36000, mean episode reward: 34.4941464547935, agent episode reward: [19.15, 19.15, 19.15, -22.955853545206498], time: 190.281
steps: 924975, episodes: 37000, mean episode reward: 31.45307632790638, agent episode reward: [17.82, 17.82, 17.82, -22.006923672093624], time: 187.419
steps: 949975, episodes: 38000, mean episode reward: 34.751829135695885, agent episode reward: [19.23, 19.23, 19.23, -22.938170864304112], time: 186.208
steps: 974975, episodes: 39000, mean episode reward: 34.25790665158124, agent episode reward: [19.0, 19.0, 19.0, -22.742093348418763], time: 188.348
steps: 999975, episodes: 40000, mean episode reward: 31.51122971607279, agent episode reward: [17.93, 17.93, 17.93, -22.27877028392721], time: 187.49
steps: 1024975, episodes: 41000, mean episode reward: 38.48073204421562, agent episode reward: [21.56, 21.56, 21.56, -26.19926795578439], time: 188.434
steps: 1049975, episodes: 42000, mean episode reward: 35.73824379687298, agent episode reward: [20.27, 20.27, 20.27, -25.07175620312702], time: 188.471
steps: 1074975, episodes: 43000, mean episode reward: 34.42523954371052, agent episode reward: [19.49, 19.49, 19.49, -24.04476045628948], time: 189.644
steps: 1099975, episodes: 44000, mean episode reward: 33.89566364286125, agent episode reward: [19.03, 19.03, 19.03, -23.194336357138745], time: 187.289
steps: 1124975, episodes: 45000, mean episode reward: 34.11211666608661, agent episode reward: [18.99, 18.99, 18.99, -22.857883333913392], time: 187.167
steps: 1149975, episodes: 46000, mean episode reward: 37.40080887545911, agent episode reward: [20.71, 20.71, 20.71, -24.72919112454088], time: 184.777
steps: 1174975, episodes: 47000, mean episode reward: 30.068589228045873, agent episode reward: [17.19, 17.19, 17.19, -21.501410771954127], time: 184.374
steps: 1199975, episodes: 48000, mean episode reward: 31.01679412529106, agent episode reward: [17.41, 17.41, 17.41, -21.21320587470894], time: 185.374
steps: 1224975, episodes: 49000, mean episode reward: 28.191617087319752, agent episode reward: [16.5, 16.5, 16.5, -21.308382912680248], time: 185.299
steps: 1249975, episodes: 50000, mean episode reward: 27.589061568791177, agent episode reward: [15.91, 15.91, 15.91, -20.14093843120882], time: 184.717
steps: 1274975, episodes: 51000, mean episode reward: 29.901017505596407, agent episode reward: [16.9, 16.9, 16.9, -20.798982494403592], time: 185.202
steps: 1299975, episodes: 52000, mean episode reward: 25.74256486481888, agent episode reward: [15.2, 15.2, 15.2, -19.85743513518112], time: 184.925
steps: 1324975, episodes: 53000, mean episode reward: 25.564894992216033, agent episode reward: [14.86, 14.86, 14.86, -19.01510500778397], time: 183.583
steps: 1349975, episodes: 54000, mean episode reward: 22.93791168227405, agent episode reward: [13.61, 13.61, 13.61, -17.892088317725953], time: 184.676
steps: 1374975, episodes: 55000, mean episode reward: 23.25233311327063, agent episode reward: [13.93, 13.93, 13.93, -18.53766688672937], time: 185.115
steps: 1399975, episodes: 56000, mean episode reward: 24.79261761873035, agent episode reward: [14.67, 14.67, 14.67, -19.21738238126965], time: 184.882
steps: 1424975, episodes: 57000, mean episode reward: 20.986806337828895, agent episode reward: [12.81, 12.81, 12.81, -17.44319366217111], time: 184.452
steps: 1449975, episodes: 58000, mean episode reward: 22.012171143834852, agent episode reward: [13.51, 13.51, 13.51, -18.51782885616515], time: 184.014
steps: 1474975, episodes: 59000, mean episode reward: 25.08741375236775, agent episode reward: [14.85, 14.85, 14.85, -19.462586247632252], time: 184.866
steps: 1499975, episodes: 60000, mean episode reward: 25.793794325228916, agent episode reward: [15.36, 15.36, 15.36, -20.286205674771086], time: 183.396
...Finished total of 60001 episodes.
