Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -25.32939003271536, agent episode reward: [-24.454870857473356, -0.4372595876210016, -0.4372595876210016], time: 72.02
steps: 49975, episodes: 2000, mean episode reward: -13.536355801007701, agent episode reward: [-9.71430543274674, -1.911025184130481, -1.911025184130481], time: 110.502
steps: 74975, episodes: 3000, mean episode reward: -2.419580506170818, agent episode reward: [-8.744549705123289, 3.1624845994762354, 3.1624845994762354], time: 109.123
steps: 99975, episodes: 4000, mean episode reward: 5.848228233334438, agent episode reward: [-16.913037732027856, 11.380632982681146, 11.380632982681146], time: 109.289
steps: 124975, episodes: 5000, mean episode reward: 2.878205983734343, agent episode reward: [-13.771246150019149, 8.324726066876744, 8.324726066876744], time: 108.763
steps: 149975, episodes: 6000, mean episode reward: 8.601400405972912, agent episode reward: [-12.389919677682501, 10.495660041827705, 10.495660041827705], time: 110.183
steps: 174975, episodes: 7000, mean episode reward: 14.407601309733916, agent episode reward: [-19.502195405069784, 16.95489835740185, 16.95489835740185], time: 110.212
steps: 199975, episodes: 8000, mean episode reward: 7.481964318374335, agent episode reward: [-11.366536764010267, 9.4242505411923, 9.4242505411923], time: 110.272
steps: 224975, episodes: 9000, mean episode reward: 14.123751406965233, agent episode reward: [-18.162570608961055, 16.14316100796314, 16.14316100796314], time: 110.356
steps: 249975, episodes: 10000, mean episode reward: 15.029947631381132, agent episode reward: [-26.032824410253845, 20.53138602081749, 20.53138602081749], time: 110.236
steps: 274975, episodes: 11000, mean episode reward: 18.722828138089273, agent episode reward: [-21.821989121460362, 20.27240862977482, 20.27240862977482], time: 110.333
steps: 299975, episodes: 12000, mean episode reward: 20.446565207459052, agent episode reward: [-23.28546309344229, 21.866014150450667, 21.866014150450667], time: 110.431
steps: 324975, episodes: 13000, mean episode reward: 19.145906939370057, agent episode reward: [-22.157609548686647, 20.651758244028354, 20.651758244028354], time: 109.366
steps: 349975, episodes: 14000, mean episode reward: 18.04513956513151, agent episode reward: [-21.327770335057316, 19.68645495009441, 19.68645495009441], time: 109.893
steps: 374975, episodes: 15000, mean episode reward: 17.57224565100084, agent episode reward: [-21.111914492198483, 19.342080071599664, 19.342080071599664], time: 111.065
steps: 399975, episodes: 16000, mean episode reward: 19.27969776408175, agent episode reward: [-22.60946986287689, 20.944583813479316, 20.944583813479316], time: 110.909
steps: 424975, episodes: 17000, mean episode reward: 17.995980355071715, agent episode reward: [-22.945843241277295, 20.47091179817451, 20.47091179817451], time: 110.595
steps: 449975, episodes: 18000, mean episode reward: 17.097650495670692, agent episode reward: [-26.95613758491427, 22.026894040292483, 22.026894040292483], time: 109.639
steps: 474975, episodes: 19000, mean episode reward: 23.417010756659387, agent episode reward: [-29.37178537680864, 26.394398066734016, 26.394398066734016], time: 109.822
steps: 499975, episodes: 20000, mean episode reward: 25.24176922323287, agent episode reward: [-28.29899378146026, 26.770381502346567, 26.770381502346567], time: 109.848
steps: 524975, episodes: 21000, mean episode reward: 25.22951016998655, agent episode reward: [-27.822042747462756, 26.525776458724653, 26.525776458724653], time: 110.703
steps: 549975, episodes: 22000, mean episode reward: 21.825896061759256, agent episode reward: [-24.524071335385607, 23.174983698572433, 23.174983698572433], time: 110.364
steps: 574975, episodes: 23000, mean episode reward: 18.353476198671974, agent episode reward: [-21.179803575030665, 19.76663988685132, 19.76663988685132], time: 110.161
