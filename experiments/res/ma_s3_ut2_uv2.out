Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -26.385412454014133, agent episode reward: [0.4369024661598681, -26.82231492017401], time: 37.664
steps: 49975, episodes: 2000, mean episode reward: -22.6217090701773, agent episode reward: [-5.406005963623269, -17.21570310655403], time: 47.054
steps: 74975, episodes: 3000, mean episode reward: -14.110076360803006, agent episode reward: [-6.042709272453294, -8.067367088349712], time: 45.794
steps: 99975, episodes: 4000, mean episode reward: -11.05302876705232, agent episode reward: [-4.115884205560011, -6.937144561492307], time: 45.395
steps: 124975, episodes: 5000, mean episode reward: -10.602984814313615, agent episode reward: [-3.336446406791919, -7.266538407521698], time: 45.354
steps: 149975, episodes: 6000, mean episode reward: -10.652917325723017, agent episode reward: [-3.2281142322991982, -7.42480309342382], time: 47.301
steps: 174975, episodes: 7000, mean episode reward: -10.34185070602087, agent episode reward: [-2.2844657758344686, -8.0573849301864], time: 51.534
steps: 199975, episodes: 8000, mean episode reward: -10.300196136754156, agent episode reward: [-2.547948817242759, -7.7522473195113974], time: 52.317
steps: 224975, episodes: 9000, mean episode reward: -10.307082275346094, agent episode reward: [-2.3642689847377976, -7.942813290608298], time: 52.328
steps: 249975, episodes: 10000, mean episode reward: -10.302608547635424, agent episode reward: [-2.3021596497714305, -8.000448897863995], time: 51.982
steps: 274975, episodes: 11000, mean episode reward: -10.492928515640033, agent episode reward: [-1.9147741943519645, -8.578154321288068], time: 51.584
steps: 299975, episodes: 12000, mean episode reward: -10.210085272092938, agent episode reward: [-1.6577043884871203, -8.552380883605817], time: 52.306
steps: 324975, episodes: 13000, mean episode reward: -9.966631925388644, agent episode reward: [-1.5173890099199503, -8.449242915468691], time: 50.969
steps: 349975, episodes: 14000, mean episode reward: -9.88643569069838, agent episode reward: [-2.03367013889281, -7.852765551805571], time: 52.242
steps: 374975, episodes: 15000, mean episode reward: -9.653507882628492, agent episode reward: [-1.847230088180043, -7.806277794448448], time: 52.837
steps: 399975, episodes: 16000, mean episode reward: -9.924292818856731, agent episode reward: [-2.2086952593036973, -7.715597559553034], time: 52.573
steps: 424975, episodes: 17000, mean episode reward: -9.624935630047093, agent episode reward: [-2.208601039489712, -7.416334590557379], time: 51.105
steps: 449975, episodes: 18000, mean episode reward: -9.700095738915198, agent episode reward: [-1.6432032134808152, -8.056892525434384], time: 52.471
steps: 474975, episodes: 19000, mean episode reward: -9.919262091470028, agent episode reward: [-2.2663489220580675, -7.652913169411961], time: 51.661
steps: 499975, episodes: 20000, mean episode reward: -9.978010344158198, agent episode reward: [-2.3472804461283334, -7.6307298980298635], time: 52.602
steps: 524975, episodes: 21000, mean episode reward: -9.856167483926798, agent episode reward: [-2.3094062417757724, -7.5467612421510255], time: 51.506
steps: 549975, episodes: 22000, mean episode reward: -10.057865728658843, agent episode reward: [-2.0761953072839323, -7.981670421374909], time: 53.126
steps: 574975, episodes: 23000, mean episode reward: -10.135633381795117, agent episode reward: [-2.371660235787666, -7.76397314600745], time: 52.986
steps: 599975, episodes: 24000, mean episode reward: -10.022367363320093, agent episode reward: [-2.3815420609018445, -7.640825302418248], time: 51.786
steps: 624975, episodes: 25000, mean episode reward: -10.117624409532228, agent episode reward: [-2.1021250165256475, -8.015499393006579], time: 52.411
steps: 649975, episodes: 26000, mean episode reward: -10.038976468609809, agent episode reward: [-2.437628081619351, -7.601348386990457], time: 53.19
steps: 674975, episodes: 27000, mean episode reward: -10.198598514844841, agent episode reward: [-2.414225462594689, -7.784373052250154], time: 52.6
steps: 699975, episodes: 28000, mean episode reward: -9.906588364935383, agent episode reward: [-1.444773319597916, -8.461815045337465], time: 52.258
steps: 724975, episodes: 29000, mean episode reward: -10.096047087373547, agent episode reward: [-1.9396600679447211, -8.156387019428827], time: 51.341
steps: 749975, episodes: 30000, mean episode reward: -10.208138733672865, agent episode reward: [-2.266840422943397, -7.941298310729468], time: 52.918
steps: 774975, episodes: 31000, mean episode reward: -9.810465443257794, agent episode reward: [-1.529831848543187, -8.280633594714606], time: 50.755
steps: 799975, episodes: 32000, mean episode reward: -9.756515174660008, agent episode reward: [-1.6222893393439333, -8.134225835316075], time: 52.46
steps: 824975, episodes: 33000, mean episode reward: -10.273154123956436, agent episode reward: [-1.914041158644197, -8.359112965312239], time: 52.015
steps: 849975, episodes: 34000, mean episode reward: -9.818520307447317, agent episode reward: [-1.3698249398089992, -8.448695367638317], time: 51.748
steps: 874975, episodes: 35000, mean episode reward: -9.976872084237966, agent episode reward: [-1.2956897310259599, -8.681182353212007], time: 52.081
steps: 899975, episodes: 36000, mean episode reward: -9.851653966164285, agent episode reward: [-1.6488990385529398, -8.202754927611347], time: 52.385
steps: 924975, episodes: 37000, mean episode reward: -9.775534375778555, agent episode reward: [-1.8765232362759565, -7.899011139502598], time: 51.959
steps: 949975, episodes: 38000, mean episode reward: -10.026355779810245, agent episode reward: [-2.001805063560306, -8.024550716249939], time: 53.09
steps: 974975, episodes: 39000, mean episode reward: -9.39681782159676, agent episode reward: [-2.0493094148292674, -7.347508406767495], time: 50.514
steps: 999975, episodes: 40000, mean episode reward: -9.560706114948449, agent episode reward: [-2.1833164976807904, -7.377389617267657], time: 52.059
steps: 1024975, episodes: 41000, mean episode reward: -9.83462267529118, agent episode reward: [-2.0812237800322198, -7.753398895258961], time: 51.786
steps: 1049975, episodes: 42000, mean episode reward: -9.792851115525412, agent episode reward: [-2.407199811496181, -7.385651304029231], time: 52.105
steps: 1074975, episodes: 43000, mean episode reward: -9.855870047556115, agent episode reward: [-2.3213985585837396, -7.5344714889723745], time: 52.258
steps: 1099975, episodes: 44000, mean episode reward: -9.616440363691854, agent episode reward: [-1.9342287105168174, -7.682211653175036], time: 52.928
steps: 1124975, episodes: 45000, mean episode reward: -10.019736475966784, agent episode reward: [-2.3247864617453153, -7.694950014221469], time: 53.267
steps: 1149975, episodes: 46000, mean episode reward: -10.106526497652698, agent episode reward: [-2.2285012965231443, -7.878025201129552], time: 52.328
steps: 1174975, episodes: 47000, mean episode reward: -9.845574384107723, agent episode reward: [-1.8450890259717343, -8.000485358135988], time: 52.147
steps: 1199975, episodes: 48000, mean episode reward: -9.745496922478742, agent episode reward: [-2.1875939058395235, -7.5579030166392185], time: 52.471
steps: 1224975, episodes: 49000, mean episode reward: -9.875666491063308, agent episode reward: [-1.9158576011972523, -7.959808889866056], time: 52.403
steps: 1249975, episodes: 50000, mean episode reward: -9.777991941296984, agent episode reward: [-1.6475727817354104, -8.130419159561574], time: 52.267
steps: 1274975, episodes: 51000, mean episode reward: -10.093137087360887, agent episode reward: [-1.6568263672505597, -8.436310720110326], time: 52.43
steps: 1299975, episodes: 52000, mean episode reward: -10.009236963445177, agent episode reward: [-1.0082466820799478, -9.00099028136523], time: 52.203
steps: 1324975, episodes: 53000, mean episode reward: -10.066288365345667, agent episode reward: [-0.7537871584978992, -9.312501206847767], time: 52.468
steps: 1349975, episodes: 54000, mean episode reward: -10.340847951442928, agent episode reward: [-0.9471533143957227, -9.393694637047203], time: 51.549
steps: 1374975, episodes: 55000, mean episode reward: -10.173821038302181, agent episode reward: [-0.5389126157073905, -9.634908422594792], time: 51.151
steps: 1399975, episodes: 56000, mean episode reward: -10.171032506703519, agent episode reward: [-1.3613280416465339, -8.809704465056988], time: 52.875
steps: 1424975, episodes: 57000, mean episode reward: -10.337348705857503, agent episode reward: [-1.57675354676899, -8.760595159088512], time: 53.275
steps: 1449975, episodes: 58000, mean episode reward: -9.60796124359728, agent episode reward: [-1.0259961839445966, -8.581965059652685], time: 52.139
steps: 1474975, episodes: 59000, mean episode reward: -9.804250367354085, agent episode reward: [-1.4202984869047066, -8.383951880449379], time: 53.288
steps: 1499975, episodes: 60000, mean episode reward: -9.433975176567316, agent episode reward: [-1.406759153328654, -8.027216023238662], time: 51.93
