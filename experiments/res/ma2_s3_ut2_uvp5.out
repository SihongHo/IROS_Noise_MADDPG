Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -26.40175827595423, agent episode reward: [3.2111500997588434, -29.612908375713072], time: 34.467
steps: 49975, episodes: 2000, mean episode reward: -21.170636853254525, agent episode reward: [-2.000336914302263, -19.170299938952258], time: 55.494
steps: 74975, episodes: 3000, mean episode reward: -13.46288499867095, agent episode reward: [-6.0049901779259, -7.457894820745049], time: 54.49
steps: 99975, episodes: 4000, mean episode reward: -10.6113054193555, agent episode reward: [-3.5802460744023366, -7.031059344953164], time: 47.913
steps: 124975, episodes: 5000, mean episode reward: -10.50860555572412, agent episode reward: [-3.186605183290033, -7.322000372434086], time: 47.146
steps: 149975, episodes: 6000, mean episode reward: -9.602629794211222, agent episode reward: [-2.6072824090631213, -6.995347385148102], time: 50.853
steps: 174975, episodes: 7000, mean episode reward: -9.832370993828407, agent episode reward: [-2.618688593553438, -7.213682400274967], time: 50.052
steps: 199975, episodes: 8000, mean episode reward: -9.835148582172009, agent episode reward: [-2.4188303161456717, -7.416318266026337], time: 58.698
steps: 224975, episodes: 9000, mean episode reward: -9.675599378749924, agent episode reward: [-2.360487643709771, -7.315111735040155], time: 62.94
steps: 249975, episodes: 10000, mean episode reward: -9.74264393957264, agent episode reward: [-2.014296040753374, -7.7283478988192655], time: 64.596
steps: 274975, episodes: 11000, mean episode reward: -9.61608850003699, agent episode reward: [-1.7219238997209532, -7.894164600316037], time: 64.624
steps: 299975, episodes: 12000, mean episode reward: -9.695907059928146, agent episode reward: [-2.076970732388784, -7.6189363275393625], time: 62.482
steps: 324975, episodes: 13000, mean episode reward: -9.433920225220403, agent episode reward: [-2.1509467468783487, -7.282973478342053], time: 63.431
steps: 349975, episodes: 14000, mean episode reward: -9.124008507904191, agent episode reward: [-2.0649124864560573, -7.059096021448134], time: 64.832
steps: 374975, episodes: 15000, mean episode reward: -9.030621438359818, agent episode reward: [-1.8374375533223606, -7.193183885037457], time: 63.742
steps: 399975, episodes: 16000, mean episode reward: -9.278017189392434, agent episode reward: [-1.8828666708372703, -7.395150518555163], time: 63.449
steps: 424975, episodes: 17000, mean episode reward: -9.212570005804176, agent episode reward: [-1.3059400079931118, -7.9066299978110655], time: 63.455
steps: 449975, episodes: 18000, mean episode reward: -9.041881904859908, agent episode reward: [-1.0900960245351978, -7.951785880324709], time: 63.457
steps: 474975, episodes: 19000, mean episode reward: -9.342541478710837, agent episode reward: [-1.270166820898839, -8.072374657811997], time: 64.172
steps: 499975, episodes: 20000, mean episode reward: -9.263917658896727, agent episode reward: [-1.4644422751016937, -7.799475383795034], time: 64.545
steps: 524975, episodes: 21000, mean episode reward: -9.519189565101229, agent episode reward: [-1.3734637613383975, -8.145725803762831], time: 64.346
steps: 549975, episodes: 22000, mean episode reward: -9.378643630596999, agent episode reward: [-1.5228729663994311, -7.855770664197568], time: 64.251
steps: 574975, episodes: 23000, mean episode reward: -9.283139240552787, agent episode reward: [-1.5337837798531662, -7.749355460699621], time: 64.124
steps: 599975, episodes: 24000, mean episode reward: -9.411005133768667, agent episode reward: [-1.543182491337132, -7.867822642431536], time: 64.55
steps: 624975, episodes: 25000, mean episode reward: -9.483151355473236, agent episode reward: [-1.2390221812444582, -8.24412917422878], time: 63.486
steps: 649975, episodes: 26000, mean episode reward: -9.817050022404295, agent episode reward: [-1.5428226137574206, -8.274227408646873], time: 64.415
steps: 674975, episodes: 27000, mean episode reward: -9.39322190054469, agent episode reward: [-1.3650031361193131, -8.028218764425377], time: 64.05
steps: 699975, episodes: 28000, mean episode reward: -9.103399491071071, agent episode reward: [-1.4443178944895874, -7.659081596581485], time: 66.575
steps: 724975, episodes: 29000, mean episode reward: -9.341264471950293, agent episode reward: [-1.2745795111990252, -8.066684960751267], time: 66.719
steps: 749975, episodes: 30000, mean episode reward: -9.559216115792719, agent episode reward: [-1.2697726833484708, -8.289443432444248], time: 67.724
steps: 774975, episodes: 31000, mean episode reward: -9.345155709538151, agent episode reward: [-1.548425793900903, -7.796729915637249], time: 68.643
steps: 799975, episodes: 32000, mean episode reward: -9.509985185086324, agent episode reward: [-1.5195563751448697, -7.990428809941455], time: 67.928
steps: 824975, episodes: 33000, mean episode reward: -9.694398229998555, agent episode reward: [-1.6694205999505016, -8.024977630048054], time: 68.49
steps: 849975, episodes: 34000, mean episode reward: -9.38178032325989, agent episode reward: [-1.67280852824191, -7.708971795017982], time: 68.11
steps: 874975, episodes: 35000, mean episode reward: -9.194131166707017, agent episode reward: [-1.291780398259258, -7.902350768447758], time: 67.958
steps: 899975, episodes: 36000, mean episode reward: -9.349992268802401, agent episode reward: [-1.1418067687345328, -8.208185500067868], time: 68.674
steps: 924975, episodes: 37000, mean episode reward: -9.687053794027818, agent episode reward: [-1.1620374438248597, -8.52501635020296], time: 68.195
steps: 949975, episodes: 38000, mean episode reward: -9.73332461544642, agent episode reward: [-0.847697681879235, -8.885626933567186], time: 68.253
steps: 974975, episodes: 39000, mean episode reward: -9.927005738713909, agent episode reward: [-0.9284402701530392, -8.998565468560871], time: 68.388
steps: 999975, episodes: 40000, mean episode reward: -10.061096562410869, agent episode reward: [-1.5003414782261204, -8.560755084184748], time: 68.441
steps: 1024975, episodes: 41000, mean episode reward: -10.471043064154586, agent episode reward: [-1.5752622559500842, -8.895780808204503], time: 68.008
steps: 1049975, episodes: 42000, mean episode reward: -10.454859684196228, agent episode reward: [-0.6096782748794172, -9.84518140931681], time: 68.179
steps: 1074975, episodes: 43000, mean episode reward: -10.754098817992144, agent episode reward: [-0.5628971086901631, -10.19120170930198], time: 67.893
steps: 1099975, episodes: 44000, mean episode reward: -10.559952142905011, agent episode reward: [-0.7324920543551041, -9.827460088549905], time: 68.025
steps: 1124975, episodes: 45000, mean episode reward: -10.71748168604462, agent episode reward: [-0.761498640677335, -9.955983045367281], time: 68.674
steps: 1149975, episodes: 46000, mean episode reward: -11.2643111623076, agent episode reward: [-1.779677985028878, -9.484633177278722], time: 68.082
steps: 1174975, episodes: 47000, mean episode reward: -11.026621425255119, agent episode reward: [-2.8198108902003116, -8.206810535054808], time: 67.992
steps: 1199975, episodes: 48000, mean episode reward: -10.874310347738618, agent episode reward: [-2.747775921298989, -8.12653442643963], time: 67.841
steps: 1224975, episodes: 49000, mean episode reward: -10.719278090447562, agent episode reward: [-2.9650770038514374, -7.754201086596124], time: 68.085
steps: 1249975, episodes: 50000, mean episode reward: -9.91544472635827, agent episode reward: [-1.8276943095999807, -8.08775041675829], time: 68.092
steps: 1274975, episodes: 51000, mean episode reward: -10.15747951478882, agent episode reward: [-1.674154041969959, -8.483325472818862], time: 67.853
steps: 1299975, episodes: 52000, mean episode reward: -10.284238979912926, agent episode reward: [-2.5609252205797777, -7.723313759333151], time: 68.307
steps: 1324975, episodes: 53000, mean episode reward: -10.240789367592162, agent episode reward: [-2.7748971161735088, -7.465892251418653], time: 68.39
steps: 1349975, episodes: 54000, mean episode reward: -10.297044475329953, agent episode reward: [-2.69526020468456, -7.601784270645393], time: 67.765
steps: 1374975, episodes: 55000, mean episode reward: -10.445396111624465, agent episode reward: [-2.5329918018197306, -7.912404309804733], time: 68.234
steps: 1399975, episodes: 56000, mean episode reward: -10.600953616284253, agent episode reward: [-2.9491632943518735, -7.651790321932378], time: 69.101
steps: 1424975, episodes: 57000, mean episode reward: -10.75069087357876, agent episode reward: [-3.1828853176833856, -7.567805555895375], time: 67.972
steps: 1449975, episodes: 58000, mean episode reward: -10.842533678823349, agent episode reward: [-3.179842558348874, -7.6626911204744745], time: 68.098
steps: 1474975, episodes: 59000, mean episode reward: -10.92146460244766, agent episode reward: [-3.1699660888740793, -7.751498513573581], time: 67.709
steps: 1499975, episodes: 60000, mean episode reward: -10.970722801043603, agent episode reward: [-3.6019630370990523, -7.368759763944549], time: 67.945
...Finished total of 60001 episodes.
