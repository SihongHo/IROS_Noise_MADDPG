Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  2.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -1.8778798133993728, agent episode reward: [2.45, 2.45, 2.45, -9.227879813399372], time: 69.962
steps: 49975, episodes: 2000, mean episode reward: -14.322927291450538, agent episode reward: [3.15, 3.15, 3.15, -23.772927291450543], time: 99.222
steps: 74975, episodes: 3000, mean episode reward: 5.858085297686359, agent episode reward: [4.96, 4.96, 4.96, -9.021914702313643], time: 103.733
steps: 99975, episodes: 4000, mean episode reward: 10.743287818243743, agent episode reward: [5.89, 5.89, 5.89, -6.9267121817562565], time: 117.881
steps: 124975, episodes: 5000, mean episode reward: 19.002092455227686, agent episode reward: [9.68, 9.68, 9.68, -10.037907544772315], time: 117.008
steps: 149975, episodes: 6000, mean episode reward: 42.10146995801312, agent episode reward: [21.29, 21.29, 21.29, -21.76853004198688], time: 118.793
steps: 174975, episodes: 7000, mean episode reward: 66.6443239625569, agent episode reward: [33.81, 33.81, 33.81, -34.78567603744311], time: 117.013
steps: 199975, episodes: 8000, mean episode reward: 44.33371892630884, agent episode reward: [23.66, 23.66, 23.66, -26.646281073691153], time: 119.825
steps: 224975, episodes: 9000, mean episode reward: 17.62473833432056, agent episode reward: [11.61, 11.61, 11.61, -17.205261665679437], time: 120.486
steps: 249975, episodes: 10000, mean episode reward: 12.877284932792703, agent episode reward: [8.68, 8.68, 8.68, -13.162715067207296], time: 117.715
steps: 274975, episodes: 11000, mean episode reward: 12.236908780517597, agent episode reward: [8.26, 8.26, 8.26, -12.5430912194824], time: 119.335
steps: 299975, episodes: 12000, mean episode reward: 12.522042841074294, agent episode reward: [7.1, 7.1, 7.1, -8.777957158925707], time: 117.222
steps: 324975, episodes: 13000, mean episode reward: 8.152634914421002, agent episode reward: [5.27, 5.27, 5.27, -7.6573650855789985], time: 115.408
steps: 349975, episodes: 14000, mean episode reward: 9.142887625689676, agent episode reward: [6.1, 6.1, 6.1, -9.157112374310323], time: 117.554
steps: 374975, episodes: 15000, mean episode reward: 7.899948788510344, agent episode reward: [5.4, 5.4, 5.4, -8.300051211489656], time: 120.131
steps: 399975, episodes: 16000, mean episode reward: 8.728431581062987, agent episode reward: [6.05, 6.05, 6.05, -9.421568418937014], time: 121.262
steps: 424975, episodes: 17000, mean episode reward: 8.716090764274973, agent episode reward: [5.61, 5.61, 5.61, -8.113909235725025], time: 118.316
steps: 449975, episodes: 18000, mean episode reward: 13.36824771754821, agent episode reward: [7.83, 7.83, 7.83, -10.121752282451792], time: 121.364
steps: 474975, episodes: 19000, mean episode reward: 19.963477355251868, agent episode reward: [11.24, 11.24, 11.24, -13.75652264474813], time: 119.192
steps: 499975, episodes: 20000, mean episode reward: 23.72516614348833, agent episode reward: [13.63, 13.63, 13.63, -17.16483385651167], time: 119.651
steps: 524975, episodes: 21000, mean episode reward: 25.365245299148373, agent episode reward: [14.43, 14.43, 14.43, -17.924754700851626], time: 119.122
steps: 549975, episodes: 22000, mean episode reward: 36.198804183813216, agent episode reward: [19.6, 19.6, 19.6, -22.60119581618678], time: 117.286
steps: 574975, episodes: 23000, mean episode reward: 37.198643092025584, agent episode reward: [20.11, 20.11, 20.11, -23.13135690797442], time: 118.508
steps: 599975, episodes: 24000, mean episode reward: 45.86677420566106, agent episode reward: [24.59, 24.59, 24.59, -27.903225794338937], time: 119.441
steps: 624975, episodes: 25000, mean episode reward: 48.06863811166006, agent episode reward: [25.3, 25.3, 25.3, -27.83136188833995], time: 118.608
steps: 649975, episodes: 26000, mean episode reward: 49.11709800155693, agent episode reward: [25.94, 25.94, 25.94, -28.702901998443075], time: 120.834
steps: 674975, episodes: 27000, mean episode reward: 36.63536619589962, agent episode reward: [20.31, 20.31, 20.31, -24.29463380410037], time: 119.963
steps: 699975, episodes: 28000, mean episode reward: 32.225884979841865, agent episode reward: [18.13, 18.13, 18.13, -22.164115020158132], time: 119.557
steps: 724975, episodes: 29000, mean episode reward: 31.285762195757176, agent episode reward: [17.62, 17.62, 17.62, -21.574237804242824], time: 119.082
steps: 749975, episodes: 30000, mean episode reward: 25.61080539895078, agent episode reward: [15.05, 15.05, 15.05, -19.539194601049218], time: 116.987
steps: 774975, episodes: 31000, mean episode reward: 18.497732195519685, agent episode reward: [12.65, 12.65, 12.65, -19.452267804480314], time: 117.905
steps: 799975, episodes: 32000, mean episode reward: 15.87893326404689, agent episode reward: [11.07, 11.07, 11.07, -17.33106673595311], time: 115.833
steps: 824975, episodes: 33000, mean episode reward: 14.655341320657433, agent episode reward: [11.23, 11.23, 11.23, -19.034658679342563], time: 118.857
steps: 849975, episodes: 34000, mean episode reward: 13.254309334125406, agent episode reward: [10.69, 10.69, 10.69, -18.815690665874595], time: 117.239
steps: 874975, episodes: 35000, mean episode reward: 12.092789843267612, agent episode reward: [10.02, 10.02, 10.02, -17.96721015673239], time: 116.72
steps: 899975, episodes: 36000, mean episode reward: 8.692085638602453, agent episode reward: [8.5, 8.5, 8.5, -16.807914361397547], time: 120.562
steps: 924975, episodes: 37000, mean episode reward: 14.562870993363493, agent episode reward: [9.96, 9.96, 9.96, -15.317129006636511], time: 116.557
steps: 949975, episodes: 38000, mean episode reward: 11.197298770214413, agent episode reward: [8.1, 8.1, 8.1, -13.102701229785588], time: 113.986
steps: 974975, episodes: 39000, mean episode reward: 16.248877209558646, agent episode reward: [11.09, 11.09, 11.09, -17.021122790441353], time: 110.97
steps: 999975, episodes: 40000, mean episode reward: 15.91248067748228, agent episode reward: [10.1, 10.1, 10.1, -14.38751932251772], time: 112.671
steps: 1024975, episodes: 41000, mean episode reward: 15.20275990836662, agent episode reward: [9.94, 9.94, 9.94, -14.617240091633382], time: 113.741
steps: 1049975, episodes: 42000, mean episode reward: 19.20967271039956, agent episode reward: [11.62, 11.62, 11.62, -15.65032728960044], time: 112.967
steps: 1074975, episodes: 43000, mean episode reward: 17.088418141655396, agent episode reward: [10.65, 10.65, 10.65, -14.861581858344605], time: 109.813
steps: 1099975, episodes: 44000, mean episode reward: 20.68152809820789, agent episode reward: [12.16, 12.16, 12.16, -15.798471901792112], time: 110.616
steps: 1124975, episodes: 45000, mean episode reward: 18.63357702390755, agent episode reward: [11.18, 11.18, 11.18, -14.90642297609245], time: 113.221
steps: 1149975, episodes: 46000, mean episode reward: 16.12472121369062, agent episode reward: [10.32, 10.32, 10.32, -14.835278786309384], time: 113.458
steps: 1174975, episodes: 47000, mean episode reward: 19.95678311330803, agent episode reward: [11.96, 11.96, 11.96, -15.923216886691968], time: 112.741
steps: 1199975, episodes: 48000, mean episode reward: 20.427393834302006, agent episode reward: [11.88, 11.88, 11.88, -15.212606165697993], time: 112.324
steps: 1224975, episodes: 49000, mean episode reward: 21.954735824679265, agent episode reward: [13.06, 13.06, 13.06, -17.22526417532074], time: 111.641
steps: 1249975, episodes: 50000, mean episode reward: 23.109721728160608, agent episode reward: [13.2, 13.2, 13.2, -16.490278271839394], time: 112.59
steps: 1274975, episodes: 51000, mean episode reward: 19.605572602456853, agent episode reward: [11.54, 11.54, 11.54, -15.014427397543146], time: 111.893
steps: 1299975, episodes: 52000, mean episode reward: 23.59436969764479, agent episode reward: [13.68, 13.68, 13.68, -17.44563030235521], time: 112.11
steps: 1324975, episodes: 53000, mean episode reward: 23.92448511895032, agent episode reward: [13.4, 13.4, 13.4, -16.275514881049684], time: 111.16
steps: 1349975, episodes: 54000, mean episode reward: 25.879616117161333, agent episode reward: [14.41, 14.41, 14.41, -17.350383882838663], time: 109.659
steps: 1374975, episodes: 55000, mean episode reward: 24.533124343549513, agent episode reward: [13.93, 13.93, 13.93, -17.256875656450486], time: 111.334
steps: 1399975, episodes: 56000, mean episode reward: 24.734071304423857, agent episode reward: [13.97, 13.97, 13.97, -17.175928695576143], time: 110.895
steps: 1424975, episodes: 57000, mean episode reward: 18.55255776200861, agent episode reward: [11.37, 11.37, 11.37, -15.557442237991388], time: 109.819
steps: 1449975, episodes: 58000, mean episode reward: 14.03539037934881, agent episode reward: [9.16, 9.16, 9.16, -13.44460962065119], time: 110.056
steps: 1474975, episodes: 59000, mean episode reward: 15.109494974443951, agent episode reward: [9.82, 9.82, 9.82, -14.35050502555605], time: 110.198
steps: 1499975, episodes: 60000, mean episode reward: 14.013178734142217, agent episode reward: [9.52, 9.52, 9.52, -14.546821265857783], time: 110.06
