Using good policy maddpg and adv policy maddpg
Uncertainty type is:  None ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -0.3106368713203436, agent episode reward: [2.03, 2.03, 2.03, -6.4006368713203425], time: 142.214
steps: 49975, episodes: 2000, mean episode reward: 4.936345373907041, agent episode reward: [4.62, 4.62, 4.62, -8.92365462609296], time: 171.849
steps: 74975, episodes: 3000, mean episode reward: 10.773467078083012, agent episode reward: [5.85, 5.85, 5.85, -6.7765329219169885], time: 169.565
steps: 99975, episodes: 4000, mean episode reward: 15.681189968529631, agent episode reward: [8.3, 8.3, 8.3, -9.218810031470367], time: 170.23
steps: 124975, episodes: 5000, mean episode reward: 37.86535770754311, agent episode reward: [19.67, 19.67, 19.67, -21.14464229245689], time: 169.551
steps: 149975, episodes: 6000, mean episode reward: 48.74178715082079, agent episode reward: [27.42, 27.42, 27.42, -33.51821284917921], time: 169.574
steps: 174975, episodes: 7000, mean episode reward: 10.147194362812272, agent episode reward: [11.07, 11.07, 11.07, -23.062805637187726], time: 170.384
steps: 199975, episodes: 8000, mean episode reward: 4.453013587233723, agent episode reward: [7.14, 7.14, 7.14, -16.966986412766275], time: 169.398
steps: 224975, episodes: 9000, mean episode reward: 12.402646531455895, agent episode reward: [8.59, 8.59, 8.59, -13.367353468544104], time: 169.469
steps: 249975, episodes: 10000, mean episode reward: 15.309492661821684, agent episode reward: [9.55, 9.55, 9.55, -13.340507338178314], time: 170.541
steps: 274975, episodes: 11000, mean episode reward: 14.286254217675333, agent episode reward: [8.76, 8.76, 8.76, -11.993745782324666], time: 169.119
steps: 299975, episodes: 12000, mean episode reward: 15.545824025600604, agent episode reward: [9.32, 9.32, 9.32, -12.414175974399397], time: 170.548
steps: 324975, episodes: 13000, mean episode reward: 15.229971762866876, agent episode reward: [8.93, 8.93, 8.93, -11.560028237133123], time: 170.383
steps: 349975, episodes: 14000, mean episode reward: 14.9649081554291, agent episode reward: [8.97, 8.97, 8.97, -11.9450918445709], time: 169.827
steps: 374975, episodes: 15000, mean episode reward: 17.57763727561212, agent episode reward: [10.22, 10.22, 10.22, -13.08236272438788], time: 169.899
steps: 399975, episodes: 16000, mean episode reward: 16.289666072247172, agent episode reward: [9.48, 9.48, 9.48, -12.150333927752826], time: 169.658
steps: 424975, episodes: 17000, mean episode reward: 16.609827735606622, agent episode reward: [10.09, 10.09, 10.09, -13.660172264393381], time: 169.365
steps: 449975, episodes: 18000, mean episode reward: 18.803979308799075, agent episode reward: [11.05, 11.05, 11.05, -14.346020691200923], time: 169.747
steps: 474975, episodes: 19000, mean episode reward: 21.20018086081217, agent episode reward: [12.43, 12.43, 12.43, -16.089819139187828], time: 169.239
steps: 499975, episodes: 20000, mean episode reward: 24.230477728270685, agent episode reward: [13.97, 13.97, 13.97, -17.679522271729315], time: 170.214
steps: 524975, episodes: 21000, mean episode reward: 25.223453693725975, agent episode reward: [14.84, 14.84, 14.84, -19.296546306274024], time: 169.889
steps: 549975, episodes: 22000, mean episode reward: 25.03672870052415, agent episode reward: [14.79, 14.79, 14.79, -19.333271299475847], time: 169.485
steps: 574975, episodes: 23000, mean episode reward: 16.196927999215347, agent episode reward: [11.12, 11.12, 11.12, -17.163072000784656], time: 169.921
steps: 599975, episodes: 24000, mean episode reward: 14.140279025872154, agent episode reward: [10.12, 10.12, 10.12, -16.219720974127846], time: 169.287
steps: 624975, episodes: 25000, mean episode reward: 14.588806124860037, agent episode reward: [10.43, 10.43, 10.43, -16.701193875139964], time: 169.277
steps: 649975, episodes: 26000, mean episode reward: 12.593301002436096, agent episode reward: [8.88, 8.88, 8.88, -14.046698997563904], time: 169.843
steps: 674975, episodes: 27000, mean episode reward: 17.660223800383537, agent episode reward: [11.08, 11.08, 11.08, -15.57977619961646], time: 169.221
steps: 699975, episodes: 28000, mean episode reward: 13.609281092704498, agent episode reward: [9.06, 9.06, 9.06, -13.570718907295502], time: 170.113
steps: 724975, episodes: 29000, mean episode reward: 14.70783723494705, agent episode reward: [9.46, 9.46, 9.46, -13.67216276505295], time: 170.045
steps: 749975, episodes: 30000, mean episode reward: 15.157806305114674, agent episode reward: [10.0, 10.0, 10.0, -14.842193694885326], time: 169.701
steps: 774975, episodes: 31000, mean episode reward: 18.04120997686942, agent episode reward: [11.17, 11.17, 11.17, -15.468790023130579], time: 170.135
steps: 799975, episodes: 32000, mean episode reward: 17.831809717846973, agent episode reward: [11.35, 11.35, 11.35, -16.218190282153024], time: 169.886
steps: 824975, episodes: 33000, mean episode reward: 16.457769364833915, agent episode reward: [10.63, 10.63, 10.63, -15.432230635166084], time: 168.981
steps: 849975, episodes: 34000, mean episode reward: 16.494547084525717, agent episode reward: [10.84, 10.84, 10.84, -16.025452915474283], time: 169.893
steps: 874975, episodes: 35000, mean episode reward: 14.001162152434343, agent episode reward: [9.7, 9.7, 9.7, -15.098837847565658], time: 168.964
steps: 899975, episodes: 36000, mean episode reward: 15.454541458279357, agent episode reward: [10.32, 10.32, 10.32, -15.505458541720644], time: 170.09
steps: 924975, episodes: 37000, mean episode reward: 15.470600914666914, agent episode reward: [10.43, 10.43, 10.43, -15.819399085333085], time: 169.875
steps: 949975, episodes: 38000, mean episode reward: 15.868231001652454, agent episode reward: [10.74, 10.74, 10.74, -16.35176899834755], time: 168.371
steps: 974975, episodes: 39000, mean episode reward: 17.61491082111137, agent episode reward: [11.46, 11.46, 11.46, -16.76508917888863], time: 169.308
steps: 999975, episodes: 40000, mean episode reward: 20.239392009676628, agent episode reward: [12.79, 12.79, 12.79, -18.130607990323373], time: 169.2
steps: 1024975, episodes: 41000, mean episode reward: 17.24470230985433, agent episode reward: [11.26, 11.26, 11.26, -16.535297690145672], time: 169.189
steps: 1049975, episodes: 42000, mean episode reward: 18.15589696234971, agent episode reward: [11.33, 11.33, 11.33, -15.834103037650289], time: 170.229
steps: 1074975, episodes: 43000, mean episode reward: 15.98694619061548, agent episode reward: [10.54, 10.54, 10.54, -15.633053809384519], time: 168.656
steps: 1099975, episodes: 44000, mean episode reward: 17.09134878223951, agent episode reward: [11.09, 11.09, 11.09, -16.178651217760493], time: 169.56
steps: 1124975, episodes: 45000, mean episode reward: 17.965168876591434, agent episode reward: [11.42, 11.42, 11.42, -16.294831123408567], time: 169.332
steps: 1149975, episodes: 46000, mean episode reward: 17.502153176104866, agent episode reward: [10.86, 10.86, 10.86, -15.077846823895136], time: 169.709
steps: 1174975, episodes: 47000, mean episode reward: 17.446644263079282, agent episode reward: [11.11, 11.11, 11.11, -15.883355736920723], time: 169.95
steps: 1199975, episodes: 48000, mean episode reward: 18.558100924153557, agent episode reward: [11.89, 11.89, 11.89, -17.11189907584644], time: 169.478
steps: 1224975, episodes: 49000, mean episode reward: 16.282116699994642, agent episode reward: [10.62, 10.62, 10.62, -15.577883300005352], time: 169.938
steps: 1249975, episodes: 50000, mean episode reward: 16.10150002406448, agent episode reward: [10.56, 10.56, 10.56, -15.57849997593552], time: 169.793
steps: 1274975, episodes: 51000, mean episode reward: 22.24411267185547, agent episode reward: [13.67, 13.67, 13.67, -18.76588732814453], time: 169.215
steps: 1299975, episodes: 52000, mean episode reward: 19.847777389396047, agent episode reward: [12.5, 12.5, 12.5, -17.652222610603953], time: 126.82
steps: 1324975, episodes: 53000, mean episode reward: 17.578406403280603, agent episode reward: [11.47, 11.47, 11.47, -16.831593596719394], time: 118.987
steps: 1349975, episodes: 54000, mean episode reward: 17.4860559892656, agent episode reward: [11.21, 11.21, 11.21, -16.143944010734398], time: 118.53
steps: 1374975, episodes: 55000, mean episode reward: 20.197897565074317, agent episode reward: [12.51, 12.51, 12.51, -17.332102434925684], time: 119.978
steps: 1399975, episodes: 56000, mean episode reward: 19.202568403006087, agent episode reward: [12.12, 12.12, 12.12, -17.157431596993913], time: 119.816
steps: 1424975, episodes: 57000, mean episode reward: 20.79928473504728, agent episode reward: [12.45, 12.45, 12.45, -16.55071526495272], time: 119.39
steps: 1449975, episodes: 58000, mean episode reward: 21.073858084311688, agent episode reward: [12.47, 12.47, 12.47, -16.336141915688312], time: 119.975
steps: 1474975, episodes: 59000, mean episode reward: 17.98483236963031, agent episode reward: [11.04, 11.04, 11.04, -15.13516763036969], time: 117.918
steps: 1499975, episodes: 60000, mean episode reward: 19.094399208653517, agent episode reward: [11.79, 11.79, 11.79, -16.27560079134648], time: 113.664
...Finished total of 60001 episodes.
