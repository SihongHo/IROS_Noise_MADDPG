Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  0.5
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -24.32749223384019, agent episode reward: [-24.465877640624967, 0.06919270339239117, 0.06919270339239117], time: 84.18
steps: 49975, episodes: 2000, mean episode reward: -24.063337563177374, agent episode reward: [-19.463972220585966, -2.299682671295702, -2.299682671295702], time: 109.526
steps: 74975, episodes: 3000, mean episode reward: -21.17777344671344, agent episode reward: [-18.207662094096012, -1.4850556763087155, -1.4850556763087155], time: 108.012
steps: 99975, episodes: 4000, mean episode reward: -23.90207686791714, agent episode reward: [-29.120384003496497, 2.6091535677896824, 2.6091535677896824], time: 110.471
steps: 124975, episodes: 5000, mean episode reward: -1.3722142092199183, agent episode reward: [-15.730949487649847, 7.179367639214965, 7.179367639214965], time: 110.338
steps: 149975, episodes: 6000, mean episode reward: 12.757792780611176, agent episode reward: [-21.25102105326578, 17.00440691693848, 17.00440691693848], time: 110.514
steps: 174975, episodes: 7000, mean episode reward: 20.836602610230493, agent episode reward: [-23.774831532852353, 22.305717071541427, 22.305717071541427], time: 110.317
steps: 199975, episodes: 8000, mean episode reward: 14.802018668166841, agent episode reward: [-24.9044450401256, 19.853231854146223, 19.853231854146223], time: 110.38
steps: 224975, episodes: 9000, mean episode reward: 20.501461656305416, agent episode reward: [-23.513739439284304, 22.00760054779486, 22.00760054779486], time: 109.634
steps: 249975, episodes: 10000, mean episode reward: 19.794441675655627, agent episode reward: [-23.07856939250581, 21.436505534080716, 21.436505534080716], time: 109.589
steps: 274975, episodes: 11000, mean episode reward: 18.62594762119807, agent episode reward: [-22.29164168105806, 20.458794651128063, 20.458794651128063], time: 110.29
steps: 299975, episodes: 12000, mean episode reward: 18.07749526553489, agent episode reward: [-21.87665288451022, 19.977074075022554, 19.977074075022554], time: 110.847
steps: 324975, episodes: 13000, mean episode reward: 6.584592448103538, agent episode reward: [-20.84081052943917, 13.712701488771355, 13.712701488771355], time: 111.479
steps: 349975, episodes: 14000, mean episode reward: 16.448322032504162, agent episode reward: [-19.865311700300577, 18.156816866402366, 18.156816866402366], time: 110.643
steps: 374975, episodes: 15000, mean episode reward: 17.994436466526647, agent episode reward: [-21.6799154284331, 19.837175947479874, 19.837175947479874], time: 110.549
steps: 399975, episodes: 16000, mean episode reward: 19.906416262602583, agent episode reward: [-23.477982339444424, 21.692199301023503, 21.692199301023503], time: 111.034
steps: 424975, episodes: 17000, mean episode reward: 19.320540472462067, agent episode reward: [-22.436879561859385, 20.878710017160728, 20.878710017160728], time: 110.47
steps: 449975, episodes: 18000, mean episode reward: 19.612344967531403, agent episode reward: [-22.565734727630232, 21.089039847580818, 21.089039847580818], time: 109.899
steps: 474975, episodes: 19000, mean episode reward: 18.50433264931377, agent episode reward: [-21.469314536729176, 19.986823593021473, 19.986823593021473], time: 111.147
steps: 499975, episodes: 20000, mean episode reward: 17.843330024491465, agent episode reward: [-21.196812306381585, 19.520071165436523, 19.520071165436523], time: 110.362
steps: 524975, episodes: 21000, mean episode reward: 17.12900976218193, agent episode reward: [-20.69036041602758, 18.909685089104755, 18.909685089104755], time: 109.997
steps: 549975, episodes: 22000, mean episode reward: 17.32936200255961, agent episode reward: [-20.637486597021866, 18.98342429979074, 18.98342429979074], time: 110.107
steps: 574975, episodes: 23000, mean episode reward: 17.809735086325695, agent episode reward: [-20.92049582179629, 19.365115454060994, 19.365115454060994], time: 110.61
