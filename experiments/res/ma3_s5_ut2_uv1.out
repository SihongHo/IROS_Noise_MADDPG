Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -2.8573177318942835, agent episode reward: [2.39, 2.39, 2.39, -10.027317731894284], time: 115.76
steps: 49975, episodes: 2000, mean episode reward: -12.476607167075706, agent episode reward: [3.26, 3.26, 3.26, -22.256607167075707], time: 150.794
steps: 74975, episodes: 3000, mean episode reward: 5.951527878494657, agent episode reward: [4.19, 4.19, 4.19, -6.618472121505344], time: 153.753
steps: 99975, episodes: 4000, mean episode reward: 9.674141752136915, agent episode reward: [5.27, 5.27, 5.27, -6.135858247863085], time: 153.281
steps: 124975, episodes: 5000, mean episode reward: 8.53406078726974, agent episode reward: [4.68, 4.68, 4.68, -5.5059392127302615], time: 154.074
steps: 149975, episodes: 6000, mean episode reward: 11.739043570989725, agent episode reward: [6.09, 6.09, 6.09, -6.530956429010276], time: 153.275
steps: 174975, episodes: 7000, mean episode reward: 12.894585690203192, agent episode reward: [6.71, 6.71, 6.71, -7.235414309796809], time: 153.134
steps: 199975, episodes: 8000, mean episode reward: 14.997841882496035, agent episode reward: [7.76, 7.76, 7.76, -8.282158117503965], time: 153.803
steps: 224975, episodes: 9000, mean episode reward: 18.737798802798505, agent episode reward: [9.67, 9.67, 9.67, -10.272201197201499], time: 153.368
steps: 249975, episodes: 10000, mean episode reward: 19.518160151242164, agent episode reward: [10.1, 10.1, 10.1, -10.781839848757835], time: 154.261
steps: 274975, episodes: 11000, mean episode reward: 21.721641565507554, agent episode reward: [11.52, 11.52, 11.52, -12.838358434492447], time: 154.074
steps: 299975, episodes: 12000, mean episode reward: 32.60555637652489, agent episode reward: [17.38, 17.38, 17.38, -19.534443623475113], time: 154.077
steps: 324975, episodes: 13000, mean episode reward: 25.733022206359056, agent episode reward: [13.98, 13.98, 13.98, -16.206977793640945], time: 153.073
steps: 349975, episodes: 14000, mean episode reward: 29.491324172094508, agent episode reward: [15.27, 15.27, 15.27, -16.318675827905494], time: 154.222
steps: 374975, episodes: 15000, mean episode reward: 36.66898346599915, agent episode reward: [18.82, 18.82, 18.82, -19.79101653400085], time: 153.381
steps: 399975, episodes: 16000, mean episode reward: 43.741834546551445, agent episode reward: [22.48, 22.48, 22.48, -23.69816545344855], time: 153.708
steps: 424975, episodes: 17000, mean episode reward: 37.81541952765464, agent episode reward: [19.91, 19.91, 19.91, -21.914580472345357], time: 159.235
steps: 449975, episodes: 18000, mean episode reward: 38.97401269629737, agent episode reward: [20.78, 20.78, 20.78, -23.365987303702624], time: 163.55
steps: 474975, episodes: 19000, mean episode reward: 33.16671218037535, agent episode reward: [18.13, 18.13, 18.13, -21.22328781962465], time: 162.832
steps: 499975, episodes: 20000, mean episode reward: 46.31341662904945, agent episode reward: [25.63, 25.63, 25.63, -30.57658337095056], time: 162.677
steps: 524975, episodes: 21000, mean episode reward: 46.22116793160728, agent episode reward: [25.77, 25.77, 25.77, -31.088832068392716], time: 162.915
steps: 549975, episodes: 22000, mean episode reward: 52.27952940861357, agent episode reward: [28.71, 28.71, 28.71, -33.85047059138643], time: 162.679
steps: 574975, episodes: 23000, mean episode reward: 39.37197626266993, agent episode reward: [23.26, 23.26, 23.26, -30.40802373733007], time: 164.152
steps: 599975, episodes: 24000, mean episode reward: 38.805342257792525, agent episode reward: [23.52, 23.52, 23.52, -31.754657742207478], time: 163.415
steps: 624975, episodes: 25000, mean episode reward: 44.2267823298756, agent episode reward: [26.39, 26.39, 26.39, -34.943217670124405], time: 162.843
steps: 649975, episodes: 26000, mean episode reward: 41.36813954538267, agent episode reward: [24.68, 24.68, 24.68, -32.671860454617324], time: 161.731
steps: 674975, episodes: 27000, mean episode reward: 40.92559002932196, agent episode reward: [24.66, 24.66, 24.66, -33.05440997067804], time: 162.173
steps: 699975, episodes: 28000, mean episode reward: 36.101528967176044, agent episode reward: [22.17, 22.17, 22.17, -30.408471032823947], time: 162.344
steps: 724975, episodes: 29000, mean episode reward: 31.347760962625017, agent episode reward: [19.4, 19.4, 19.4, -26.852239037374986], time: 163.609
steps: 749975, episodes: 30000, mean episode reward: 30.916818490770517, agent episode reward: [18.94, 18.94, 18.94, -25.90318150922948], time: 176.996
steps: 774975, episodes: 31000, mean episode reward: 34.43204992201816, agent episode reward: [20.08, 20.08, 20.08, -25.80795007798184], time: 177.116
steps: 799975, episodes: 32000, mean episode reward: 33.77983806887675, agent episode reward: [19.63, 19.63, 19.63, -25.110161931123248], time: 162.092
steps: 824975, episodes: 33000, mean episode reward: 38.437841213784225, agent episode reward: [21.84, 21.84, 21.84, -27.082158786215768], time: 162.689
steps: 849975, episodes: 34000, mean episode reward: 40.17265417682082, agent episode reward: [22.84, 22.84, 22.84, -28.347345823179182], time: 162.931
steps: 874975, episodes: 35000, mean episode reward: 39.91745559653511, agent episode reward: [22.84, 22.84, 22.84, -28.602544403464897], time: 161.629
steps: 899975, episodes: 36000, mean episode reward: 42.29693598578625, agent episode reward: [23.84, 23.84, 23.84, -29.223064014213747], time: 164.92
steps: 924975, episodes: 37000, mean episode reward: 41.69081756805184, agent episode reward: [23.55, 23.55, 23.55, -28.959182431948165], time: 163.052
steps: 949975, episodes: 38000, mean episode reward: 38.78356266797718, agent episode reward: [22.63, 22.63, 22.63, -29.10643733202281], time: 162.856
steps: 974975, episodes: 39000, mean episode reward: 42.606229301571226, agent episode reward: [24.47, 24.47, 24.47, -30.803770698428778], time: 162.624
steps: 999975, episodes: 40000, mean episode reward: 38.2719240584877, agent episode reward: [22.82, 22.82, 22.82, -30.188075941512295], time: 162.211
steps: 1024975, episodes: 41000, mean episode reward: 40.03301872930987, agent episode reward: [23.2, 23.2, 23.2, -29.566981270690125], time: 162.065
steps: 1049975, episodes: 42000, mean episode reward: 36.50001338298181, agent episode reward: [21.27, 21.27, 21.27, -27.309986617018186], time: 162.16
steps: 1074975, episodes: 43000, mean episode reward: 34.11145136782271, agent episode reward: [19.69, 19.69, 19.69, -24.95854863217729], time: 163.864
steps: 1099975, episodes: 44000, mean episode reward: 36.03675775405693, agent episode reward: [20.25, 20.25, 20.25, -24.713242245943075], time: 163.453
steps: 1124975, episodes: 45000, mean episode reward: 38.00017804986806, agent episode reward: [21.38, 21.38, 21.38, -26.13982195013194], time: 165.297
steps: 1149975, episodes: 46000, mean episode reward: 42.00684959439038, agent episode reward: [22.95, 22.95, 22.95, -26.843150405609613], time: 164.492
steps: 1174975, episodes: 47000, mean episode reward: 36.146613000789934, agent episode reward: [20.15, 20.15, 20.15, -24.303386999210066], time: 163.07
steps: 1199975, episodes: 48000, mean episode reward: 32.48653537867703, agent episode reward: [18.81, 18.81, 18.81, -23.943464621322963], time: 163.239
steps: 1224975, episodes: 49000, mean episode reward: 36.27802491186841, agent episode reward: [20.42, 20.42, 20.42, -24.98197508813159], time: 161.512
steps: 1249975, episodes: 50000, mean episode reward: 30.733918273054357, agent episode reward: [18.2, 18.2, 18.2, -23.866081726945637], time: 162.962
steps: 1274975, episodes: 51000, mean episode reward: 31.70331353821384, agent episode reward: [18.77, 18.77, 18.77, -24.606686461786158], time: 163.276
steps: 1299975, episodes: 52000, mean episode reward: 31.364598420725283, agent episode reward: [17.92, 17.92, 17.92, -22.395401579274715], time: 163.237
steps: 1324975, episodes: 53000, mean episode reward: 25.01412710293621, agent episode reward: [15.38, 15.38, 15.38, -21.12587289706379], time: 161.16
steps: 1349975, episodes: 54000, mean episode reward: 29.694861389643147, agent episode reward: [17.38, 17.38, 17.38, -22.44513861035685], time: 162.644
steps: 1374975, episodes: 55000, mean episode reward: 28.270168695992062, agent episode reward: [16.97, 16.97, 16.97, -22.639831304007938], time: 163.924
steps: 1399975, episodes: 56000, mean episode reward: 26.090090934897162, agent episode reward: [15.92, 15.92, 15.92, -21.669909065102843], time: 163.746
steps: 1424975, episodes: 57000, mean episode reward: 28.628854501340186, agent episode reward: [16.97, 16.97, 16.97, -22.281145498659814], time: 163.527
steps: 1449975, episodes: 58000, mean episode reward: 27.068599828419156, agent episode reward: [15.96, 15.96, 15.96, -20.81140017158085], time: 157.802
steps: 1474975, episodes: 59000, mean episode reward: 26.645864393544308, agent episode reward: [15.64, 15.64, 15.64, -20.274135606455694], time: 154.505
steps: 1499975, episodes: 60000, mean episode reward: 26.401065120334344, agent episode reward: [15.52, 15.52, 15.52, -20.158934879665654], time: 124.861
...Finished total of 60001 episodes.
