Using good policy maddpg and adv policy maddpg
Uncertainty type is:  Action ; Uncertainty level is:  1.0
Starting iterations...
steps: 24975, episodes: 1000, mean episode reward: -26.91739618799727, agent episode reward: [-0.19542225587482126, -26.721973932122452], time: 43.737
steps: 49975, episodes: 2000, mean episode reward: -23.07720908753784, agent episode reward: [-5.407603761356583, -17.669605326181262], time: 56.277
steps: 74975, episodes: 3000, mean episode reward: -13.785711390919484, agent episode reward: [-5.807598238480699, -7.978113152438787], time: 54.803
steps: 99975, episodes: 4000, mean episode reward: -11.233217730016538, agent episode reward: [-3.782066179066654, -7.4511515509498825], time: 55.765
steps: 124975, episodes: 5000, mean episode reward: -10.773465892203323, agent episode reward: [-3.334746874792101, -7.438719017411221], time: 55.844
steps: 149975, episodes: 6000, mean episode reward: -10.203466667308646, agent episode reward: [-2.4677945183933487, -7.735672148915297], time: 56.223
steps: 174975, episodes: 7000, mean episode reward: -10.519186200903412, agent episode reward: [-3.227437270680246, -7.291748930223166], time: 55.073
steps: 199975, episodes: 8000, mean episode reward: -10.380285205144224, agent episode reward: [-3.037786971816905, -7.34249823332732], time: 55.917
steps: 224975, episodes: 9000, mean episode reward: -9.948015086924473, agent episode reward: [-2.48806426410277, -7.459950822821703], time: 56.718
steps: 249975, episodes: 10000, mean episode reward: -10.160745481157214, agent episode reward: [-2.5434522223556524, -7.617293258801562], time: 55.882
steps: 274975, episodes: 11000, mean episode reward: -9.48542975906411, agent episode reward: [-2.18427073123491, -7.301159027829199], time: 55.06
steps: 299975, episodes: 12000, mean episode reward: -9.622955048717161, agent episode reward: [-2.104561119331741, -7.518393929385422], time: 56.815
steps: 324975, episodes: 13000, mean episode reward: -9.664100718315234, agent episode reward: [-1.9835562404279419, -7.680544477887293], time: 55.476
steps: 349975, episodes: 14000, mean episode reward: -9.652037861598284, agent episode reward: [-1.673357438502077, -7.978680423096208], time: 56.41
steps: 374975, episodes: 15000, mean episode reward: -9.810290970507706, agent episode reward: [-2.151791429913392, -7.658499540594313], time: 54.607
steps: 399975, episodes: 16000, mean episode reward: -9.340205678501706, agent episode reward: [-1.62689991144465, -7.713305767057054], time: 55.302
steps: 424975, episodes: 17000, mean episode reward: -9.407485845739709, agent episode reward: [-1.5666590225618096, -7.840826823177901], time: 55.243
steps: 449975, episodes: 18000, mean episode reward: -9.491604658161087, agent episode reward: [-1.6950953723265358, -7.796509285834551], time: 56.527
steps: 474975, episodes: 19000, mean episode reward: -9.443674510786783, agent episode reward: [-1.6362178558930107, -7.807456654893774], time: 55.233
steps: 499975, episodes: 20000, mean episode reward: -9.534711065973596, agent episode reward: [-1.73625594593922, -7.798455120034376], time: 55.898
steps: 524975, episodes: 21000, mean episode reward: -9.593027835097178, agent episode reward: [-2.1956148485540417, -7.397412986543138], time: 55.612
steps: 549975, episodes: 22000, mean episode reward: -10.036892450949964, agent episode reward: [-2.413881092763371, -7.62301135818659], time: 56.636
steps: 574975, episodes: 23000, mean episode reward: -10.068650335428122, agent episode reward: [-2.3072576681023143, -7.7613926673258105], time: 56.769
steps: 599975, episodes: 24000, mean episode reward: -10.31086578471956, agent episode reward: [-2.7726131145546695, -7.53825267016489], time: 54.981
steps: 624975, episodes: 25000, mean episode reward: -10.034946789537358, agent episode reward: [-2.4935510330545725, -7.541395756482785], time: 55.741
steps: 649975, episodes: 26000, mean episode reward: -9.955179054307731, agent episode reward: [-1.98164612867385, -7.973532925633882], time: 55.311
steps: 674975, episodes: 27000, mean episode reward: -9.915718736162399, agent episode reward: [-2.2943670031640293, -7.621351732998369], time: 56.223
steps: 699975, episodes: 28000, mean episode reward: -9.96370279957159, agent episode reward: [-2.4171044578961483, -7.546598341675442], time: 56.691
steps: 724975, episodes: 29000, mean episode reward: -9.914452256352101, agent episode reward: [-2.385862370467957, -7.528589885884145], time: 57.259
steps: 749975, episodes: 30000, mean episode reward: -10.419310955299396, agent episode reward: [-3.035189640194094, -7.384121315105301], time: 56.539
steps: 774975, episodes: 31000, mean episode reward: -9.877104847876636, agent episode reward: [-2.3227583920690296, -7.554346455807605], time: 56.519
steps: 799975, episodes: 32000, mean episode reward: -9.68382501487415, agent episode reward: [-1.907690926916141, -7.776134087958009], time: 55.312
steps: 824975, episodes: 33000, mean episode reward: -9.85724236107107, agent episode reward: [-2.277149745708446, -7.580092615362624], time: 55.292
steps: 849975, episodes: 34000, mean episode reward: -10.025869831817618, agent episode reward: [-2.487279364530913, -7.538590467286703], time: 56.238
steps: 874975, episodes: 35000, mean episode reward: -10.216950758230862, agent episode reward: [-2.7553184752476647, -7.461632282983197], time: 56.023
steps: 899975, episodes: 36000, mean episode reward: -10.343778530764702, agent episode reward: [-2.731603035107658, -7.612175495657044], time: 55.316
steps: 924975, episodes: 37000, mean episode reward: -10.316564968949562, agent episode reward: [-2.8835045418947174, -7.433060427054846], time: 56.758
steps: 949975, episodes: 38000, mean episode reward: -10.264406641876622, agent episode reward: [-2.7906402532977896, -7.473766388578832], time: 55.573
steps: 974975, episodes: 39000, mean episode reward: -10.302829049530697, agent episode reward: [-2.895336743564696, -7.407492305966], time: 55.362
steps: 999975, episodes: 40000, mean episode reward: -9.878333964314274, agent episode reward: [-2.2670646780876944, -7.6112692862265785], time: 55.496
steps: 1024975, episodes: 41000, mean episode reward: -10.139810635324295, agent episode reward: [-2.2603779313707446, -7.87943270395355], time: 55.796
steps: 1049975, episodes: 42000, mean episode reward: -9.942936422653103, agent episode reward: [-1.5698642971421968, -8.373072125510905], time: 55.97
steps: 1074975, episodes: 43000, mean episode reward: -9.988526037311592, agent episode reward: [-1.551760949993909, -8.436765087317683], time: 55.319
steps: 1099975, episodes: 44000, mean episode reward: -10.135837262218876, agent episode reward: [-1.3729242244335793, -8.762913037785296], time: 55.227
steps: 1124975, episodes: 45000, mean episode reward: -10.32368450615698, agent episode reward: [-1.5615491592849722, -8.762135346872007], time: 57.407
steps: 1149975, episodes: 46000, mean episode reward: -9.903191691754396, agent episode reward: [-1.3730957896323206, -8.530095902122074], time: 56.479
steps: 1174975, episodes: 47000, mean episode reward: -10.434587411132933, agent episode reward: [-1.8461772650068156, -8.588410146126115], time: 57.165
steps: 1199975, episodes: 48000, mean episode reward: -10.267859063983913, agent episode reward: [-1.2315083712098163, -9.036350692774096], time: 55.622
steps: 1224975, episodes: 49000, mean episode reward: -10.258432429662863, agent episode reward: [-1.4509829964998688, -8.807449433162994], time: 55.763
steps: 1249975, episodes: 50000, mean episode reward: -10.655594769487225, agent episode reward: [-2.1882773680927583, -8.467317401394466], time: 55.865
steps: 1274975, episodes: 51000, mean episode reward: -10.629803133631565, agent episode reward: [-2.758543184629031, -7.871259949002534], time: 56.751
steps: 1299975, episodes: 52000, mean episode reward: -10.358834245661525, agent episode reward: [-2.5287763718425014, -7.830057873819024], time: 55.739
steps: 1324975, episodes: 53000, mean episode reward: -11.028419807687428, agent episode reward: [-2.396229890546361, -8.632189917141066], time: 56.085
steps: 1349975, episodes: 54000, mean episode reward: -10.655377169926076, agent episode reward: [-1.8438627975690867, -8.811514372356992], time: 56.399
steps: 1374975, episodes: 55000, mean episode reward: -11.194282182371891, agent episode reward: [-2.1637451015045173, -9.030537080867372], time: 55.431
steps: 1399975, episodes: 56000, mean episode reward: -11.082608555368381, agent episode reward: [-1.9228626626183836, -9.159745892749998], time: 57.072
steps: 1424975, episodes: 57000, mean episode reward: -10.908019065241682, agent episode reward: [-1.6056353059159383, -9.302383759325744], time: 57.402
steps: 1449975, episodes: 58000, mean episode reward: -11.43847556663126, agent episode reward: [-1.6817559194566964, -9.756719647174563], time: 56.523
steps: 1474975, episodes: 59000, mean episode reward: -10.747894292514783, agent episode reward: [-1.4026971973101459, -9.345197095204636], time: 56.183
steps: 1499975, episodes: 60000, mean episode reward: -9.961743400911924, agent episode reward: [-1.9488676018520246, -8.012875799059898], time: 56.098
